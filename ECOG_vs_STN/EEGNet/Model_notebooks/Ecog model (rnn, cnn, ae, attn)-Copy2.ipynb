{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import mne\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import *\n",
    "from keras.losses import *\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, History\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model \n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\code\\\\icn\\\\ECOG_vs_STN\\\\EEGNet\\\\Model_notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub-000_coordsystem.json',\n",
       " 'sub-000_electrodes.tsv',\n",
       " 'sub-000_ses-right_task-force_run-0_channels.tsv',\n",
       " 'sub-000_ses-right_task-force_run-0_channels_M1.tsv',\n",
       " 'sub-000_ses-right_task-force_run-0_ieeg.eeg',\n",
       " 'sub-000_ses-right_task-force_run-0_ieeg.json',\n",
       " 'sub-000_ses-right_task-force_run-0_ieeg.vhdr',\n",
       " 'sub-000_ses-right_task-force_run-0_ieeg.vmrk',\n",
       " 'sub-000_ses-right_task-force_run-1_channels.tsv',\n",
       " 'sub-000_ses-right_task-force_run-1_channels_M1.tsv',\n",
       " 'sub-000_ses-right_task-force_run-1_ieeg.eeg',\n",
       " 'sub-000_ses-right_task-force_run-1_ieeg.json',\n",
       " 'sub-000_ses-right_task-force_run-1_ieeg.vhdr',\n",
       " 'sub-000_ses-right_task-force_run-1_ieeg.vmrk',\n",
       " 'sub-000_ses-right_task-force_run-2_channels.tsv',\n",
       " 'sub-000_ses-right_task-force_run-2_channels_M1.tsv',\n",
       " 'sub-000_ses-right_task-force_run-2_ieeg.eeg',\n",
       " 'sub-000_ses-right_task-force_run-2_ieeg.json',\n",
       " 'sub-000_ses-right_task-force_run-2_ieeg.vhdr',\n",
       " 'sub-000_ses-right_task-force_run-2_ieeg.vmrk',\n",
       " 'sub-000_ses-right_task-force_run-3_channels.tsv',\n",
       " 'sub-000_ses-right_task-force_run-3_channels_M1.tsv',\n",
       " 'sub-000_ses-right_task-force_run-3_ieeg.eeg',\n",
       " 'sub-000_ses-right_task-force_run-3_ieeg.json',\n",
       " 'sub-000_ses-right_task-force_run-3_ieeg.vhdr',\n",
       " 'sub-000_ses-right_task-force_run-3_ieeg.vmrk']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('D:\\Dropbox (Brain Modulation Lab)\\Shared Lab Folders\\CRCNS\\MOVEMENT DATA\\sub-000\\ses-right\\ieeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirName = 'D:\\Dropbox (Brain Modulation Lab)\\Shared Lab Folders\\CRCNS\\MOVEMENT DATA\\sub-000\\ses-right\\ieeg'\n",
    "fileName = 'sub-000_ses-right_task-force_run-0_ieeg.vhdr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(dirName,fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from D:\\Dropbox (Brain Modulation Lab)\\Shared Lab Folders\\CRCNS\\MOVEMENT DATA\\sub-000\\ses-right\\ieeg\\sub-000_ses-right_task-force_run-0_ieeg.vhdr...\n",
      "Setting channel info structure...\n",
      "(10, 130001)\n",
      "(6, 130001)\n",
      "(2, 130001)\n"
     ]
    }
   ],
   "source": [
    "raw = mne.io.read_raw_brainvision(fname)\n",
    "data = raw.get_data()\n",
    "print(data.shape)\n",
    "X = data[0:6,:]\n",
    "y = data[-2:,:]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104000, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.T, y.T, test_size = 0.2, shuffle = False )\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, shuffle = False )\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_regression(features, labels, batch_size, epoch_size, rebalance=False, rebalanced_thr=0): \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Given the \"features\" array, this function  returns \"batch_size\" epochs of size \"epoch_size\" balanced or unbalanced.\n",
    "    Batches can be equally sampled by two distributions defined by \"rebalanced_thr\" if \"rebalance\" is True. \n",
    "    In the rebalance=True case returned batches are shuffled. Corresponding \"labels\" are returned in the same manner.\n",
    "    \n",
    "    params: \n",
    "    features (np array) : data array in shape (time, num_channels), \n",
    "    labels (np array) : in shape (time),\n",
    "    batch_size (int) : number of samples per epoch, equivallent to keras model.fit batch_size, \n",
    "    epoch_size (int) : time dimension of data vector defined as one training sample\n",
    "                       dependent on the sampling frequency!,\n",
    "    rebalance (boolean) : if True data is sampled evenly from the greater and smaller rebalanced_thr distributions, \n",
    "    rebalanced_thr (float) : defines rebalancing thresholds for two distributions \n",
    "    \n",
    "    returns:\n",
    "    batch_features (np array) : batch array in shape (batch_size, 1, num_channels, epoch_size), \n",
    "    batch_labels (np array) : label array of size batch_size\n",
    "    \n",
    "    The batch_features shape is necessary for tensorflow train\n",
    "    \"\"\"\n",
    "    chans = features.shape[1]\n",
    "    batch_features = np.zeros([batch_size, 1, chans, epoch_size])  # definition of returned arrays\n",
    "    batch_labels = np.zeros([batch_size]) \n",
    "    \n",
    "    # the True statement is neccessary due to the \"endless\" model.fit iterations\n",
    "    # it is hence terminated by the number of epoch being specified \n",
    "    \n",
    "    while True:\n",
    "        for idx_label in np.arange(epoch_size, features.shape[0]-batch_size, batch_size): \n",
    "            for i in range(batch_size):\n",
    "\n",
    "                for ch in range(chans):\n",
    "                    batch_features[i,0,ch,:] = features[(idx_label+i-epoch_size):i+idx_label,ch]\n",
    "                batch_labels[i] = labels[idx_label+i]\n",
    "                \n",
    "            if rebalance == True:    \n",
    "                \n",
    "                # resample balancing: \n",
    "                # the indices of respective above and below threshold values are concatenated \"num_pos\" times\n",
    "                # respectively the distribution with less samples is repeated \n",
    "                # thus random sampling is avoided, but data samples are only concatenated \n",
    "                \n",
    "                dat_ = (batch_features, batch_labels)\n",
    "                ind_below = np.where(batch_labels<=rebalanced_thr)[0]\n",
    "                ind_above = np.where(batch_labels>rebalanced_thr)[0]\n",
    "                \n",
    "                # unformly sample from both distributions\n",
    "                print(ind_above.shape[0])\n",
    "                ind_sample_below = np.random.randint(0, ind_below.shape[0], size=int(batch_size/2))\n",
    "                ind_sample_above = np.random.randint(0, ind_above.shape[0], size=int(batch_size/2))                \n",
    "                \n",
    "                batch_features_ = np.concatenate((batch_features[ind_below[ind_sample_below],:,:,:], \\\n",
    "                                batch_features[ind_above[ind_sample_above],:,:,:]), axis=0)\n",
    "                batch_labels_ = np.concatenate((batch_labels[ind_below[ind_sample_below]], \\\n",
    "                                batch_labels[ind_above[ind_sample_above]]), axis=0)\n",
    "                yield batch_features_, batch_labels_, [None]\n",
    "            else:\n",
    "                yield batch_features, batch_labels, [None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "REBALANCE_THR = 0.5\n",
    "epoch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tr = generator_regression(X_train, y_train[:,0], batch_size, \\\n",
    "                    epoch_size, rebalance=True, rebalanced_thr=REBALANCE_THR)\n",
    "gen_test = utils.generator_regression(X_test, y_test, batch_size, \\\n",
    "                    epoch_size, rebalance=True, rebalanced_thr=REBALANCE_THR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "low >= high",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-c68fdec47f23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-c1b095db83fd>\u001b[0m in \u001b[0;36mgenerator_regression\u001b[1;34m(features, labels, batch_size, epoch_size, rebalance, rebalanced_thr)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind_above\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mind_sample_below\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind_below\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0mind_sample_above\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind_above\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 batch_features_ = np.concatenate((batch_features[ind_below[ind_sample_below],:,:,:], \\\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int32\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: low >= high"
     ]
    }
   ],
   "source": [
    "next(gen_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_keras_loss(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (( 1 - (SS_res/SS_tot) )*-1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cnnModel(input_shape = (500,6,1), summary = True\n",
    "                 ,n_conv = 3 ,conv_units = [64,64,64] \n",
    "                 ,kernel_sizes = [3,3,3] ,conv_activation = ReLU, conv = Conv2D\n",
    "                 ,pool_func = MaxPool2D ,pool = [0,1,1]\n",
    "                 ,n_dense = 2, dense_units = [64,120], dense_activation = ReLU\n",
    "                 ,output_layer = Dense, output_units = 2, output_activation = None\n",
    "                                                     ):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(input_shape))\n",
    "#     model.add(Permute((2,1,3) ))\n",
    "\n",
    "    for i in range(n_conv):\n",
    "        model.add(conv(conv_units[i], kernel_sizes[i], padding= 'same'))\n",
    "        model.add(conv_activation())\n",
    "        if pool[i] == 1:\n",
    "            model.add(pool_func(pool_size = (2), strides = (2), padding = 'same'))\n",
    "            \n",
    "    model.add(Flatten())        \n",
    "    \n",
    "    \n",
    "    for i in range(n_dense):\n",
    "        model.add(Dense(dense_units[i]))\n",
    "        model.add(dense_activation())\n",
    "        \n",
    "              \n",
    "    model.add(output_layer(output_units, activation = 'sigmoid') )\n",
    "    \n",
    "    \n",
    "    if summary:\n",
    "        model.summary()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103000, 1000, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 1000, 6, 64)       1664      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 1000, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1000, 6, 64)       102464    \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 1000, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 500, 3, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 500, 3, 64)        102464    \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 500, 3, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 250, 2, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                2048064   \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 2,262,577\n",
      "Trainable params: 2,262,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_right = make_cnnModel(input_shape= (1000,6,1), pool = [0,1,1], kernel_sizes=[5,5,5], output_units=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_right.compile(optimizer= Adam(), loss= 'mse', metrics= ['mae',r2_keras])\n",
    "\n",
    "cnn_callbacks = [\n",
    "    ModelCheckpoint('models/v2/best_cnnR_model.h5', monitor='val_loss', verbose=1, save_best_only= True),\n",
    "    ReduceLROnPlateau(patience= 2, monitor = 'val_loss'),\n",
    "    History()\n",
    "    \n",
    "]\n",
    "\n",
    "history_cnn = cnn_right.fit(X_train_ep[:,:,:, np.newaxis],y_train_ep[:,0],batch_size= 128, epochs= 5,\n",
    "                  callbacks= cnn_callbacks,\n",
    "                 validation_data= (X_test_ep[:,:,:,np.newaxis], y_test_ep[:,0])\n",
    "         )\n",
    "\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x27d7bf1c888>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_cnn.history['loss'], label = 'mse')\n",
    "plt.plot(history_cnn.history['r2_keras'], label = 'r2_keras')\n",
    "plt.plot(history_cnn.history['mae'], label = 'mae')\n",
    "plt.title('CNN Model (2D)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 1s 1ms/step - loss: 0.0705 - mae: 0.0941 - r2_keras: -3307153.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07049892842769623, 0.09413135051727295, -3307153.5]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_right.evaluate(X_test[:,np.newaxis,:, np.newaxis], y_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 267s 341ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = cnn_right.predict(X_test_ep[:,:,:, np.newaxis], verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25001, 1000, 2)\n",
      "(25001, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_test_ep.shape)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x27d7bd27748>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pred = cnn_right.predict(X_val_ep[:,:,:, np.newaxis])\n",
    "plt.plot(pred, label = 'prediction')\n",
    "#plt.plot(y_test_ep[:,0,0], label = 'ground truth', alpha = 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with 500 sample window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103500, 500, 6)\n",
      "(103500, 500, 2)\n",
      "(25501, 500, 6)\n",
      "(25501, 500, 2)\n"
     ]
    }
   ],
   "source": [
    "window_ms = 0.5\n",
    "window = int(window_ms* raw.info['sfreq'])\n",
    "train_gen = Generator(X_train, y_train, window, 1)\n",
    "test_gen = Generator(X_test, y_test, window, 1)\n",
    "\n",
    "\n",
    "X_train_ep = np.array(next(train_gen.generator())[0])\n",
    "y_train_ep = np.array(next(train_gen.generator())[1])\n",
    "X_test_ep = np.array(next(test_gen.generator())[0])\n",
    "y_test_ep = np.array(next(test_gen.generator())[1])\n",
    "\n",
    "print(X_train_ep.shape)\n",
    "print(y_train_ep.shape)\n",
    "\n",
    "print(X_test_ep.shape)\n",
    "print(y_test_ep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 500, 6, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 500, 6, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 500, 6, 64)        12352     \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 500, 6, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 250, 3, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 250, 3, 64)        12352     \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 250, 3, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 125, 2, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                1024064   \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 1,056,945\n",
      "Trainable params: 1,056,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_500 = make_cnnModel(input_shape= (500,6,1), pool = [0,1,1], kernel_sizes=[[3,1],[3,1],[3,1]], output_units=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "405/405 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1133\n",
      "Epoch 00001: val_loss improved from inf to 0.06351, saving model to models/v2\\best_cnn500_model.h5\n",
      "405/405 [==============================] - 618s 2s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1133 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5638\n",
      "Epoch 2/5\n",
      "405/405 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1133\n",
      "Epoch 00002: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "405/405 [==============================] - 608s 2s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1133 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5638\n",
      "Epoch 3/5\n",
      "405/405 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1133\n",
      "Epoch 00003: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "405/405 [==============================] - 610s 2s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1133 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5638\n",
      "Epoch 4/5\n",
      "405/405 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1133\n",
      "Epoch 00004: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "405/405 [==============================] - 611s 2s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1133 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5638\n",
      "Epoch 5/5\n",
      "405/405 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1134\n",
      "Epoch 00005: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "405/405 [==============================] - 615s 2s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1134 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5638\n"
     ]
    }
   ],
   "source": [
    "cnn_500.compile(optimizer= Adam(), loss= 'mse', metrics= ['mae',r2_keras])\n",
    "\n",
    "cnn_500_callbacks = [\n",
    "    ModelCheckpoint('models/v2/best_cnn500_model.h5', monitor='val_loss', verbose=1, save_best_only= True),\n",
    "    ReduceLROnPlateau(patience= 1, monitor = 'val_loss', verbose = 1),\n",
    "    History()\n",
    "    \n",
    "]\n",
    "\n",
    "history_cnn = cnn_500.fit(X_train_ep[:,:,:, np.newaxis],y_train_ep[:,0],batch_size= 256, epochs= 5,\n",
    "                  callbacks= cnn_500_callbacks,\n",
    "                 validation_data= (X_test_ep[:,:,:,np.newaxis], y_test_ep[:,0])\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797/797 [==============================] - 47s 59ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x27de0c11808>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAATcklEQVR4nO3df5BcZZ3v8ff3JpDsRTbCEFjIDye7pPJDIhC7ECq4WouEoGiUGwtYKVPqNVW66N27tWC2rFqQ3T/C1l1cLSNbuWIlaymoXCmztbViIKFuoYBMMCtJyI8JBpiA/EgiGlnUwPf+0Se5zdCTzKQ70zPzvF9VU33Oc54+/X3O6ZlPn3O6pyMzkSSV6790ugBJUmcZBJJUOINAkgpnEEhS4QwCSSrc+E4XcCxOO+207O7u7nQZkjSqbNy48cXMnNy/fVQGQXd3Nz09PZ0uQ5JGlYh4slm7p4YkqXAGgSQVziCQpMKNymsEksaG3//+9/T19fHKK690upQxZeLEiUydOpUTTjhhUP0NAkkd09fXx8knn0x3dzcR0elyxoTMZO/evfT19TFjxoxB3cdTQ5I65pVXXqGrq8sQaKOIoKura0hHWQaBpI4yBNpvqNvUIJCkwhkEktQm999/P1dccQUAa9euZcWKFQP2/eUvf8lXv/rVw/PPPPMMS5YsOe41NmMQSNJRvPrqq0O+zwc+8AGWL18+4PL+QXDWWWdx1113HVN9rTIIJBVt9+7dzJ49m4985CPMmTOHJUuW8PLLL9Pd3c3nPvc55s+fz3e/+11++MMfctFFFzF//nw+/OEPc+DAAQB+8IMfMHv2bObPn8/3vve9w+tdvXo11113HQDPPfccH/rQhzj33HM599xz+fGPf8zy5cvZtWsX5513Htdffz27d+/mnHPOAeoX0T/2sY8xb948zj//fDZs2HB4nVdeeSWLFi1i5syZ3HDDDW3ZBr59VNKI8IV/3cLWZ37V1nXOPesPufH9bz1qv+3bt3P77bezYMECPv7xjx9+pd7V1cWjjz7Kiy++yJVXXsm9997LSSedxC233MKtt97KDTfcwCc/+UnWr1/P2WefzVVXXdV0/Z/97Gd517vexd13382rr77KgQMHWLFiBZs3b2bTpk1APZAOWblyJRHBY489xrZt21i4cCE7duwAYNOmTfz0pz9lwoQJzJo1i8985jNMmzatpe3kEYGk4k2bNo0FCxYAcO211/LAAw8AHP7D/tBDD7F161YWLFjAeeedx5o1a3jyySfZtm0bM2bMYObMmUQE1157bdP1r1+/nk996lMAjBs3jkmTJh2xngceeODwumbPns1b3vKWw0FwySWXMGnSJCZOnMjcuXN58smm/0duSDwikDQiDOaV+/HS/+2Wh+ZPOukkoP4hrUsvvZQ77rjjdf0OvZofThMmTDg8PW7cOA4ePNjyOj0ikFS8p556igcffBCAb33rW1x88cWvW37hhRfyox/9iN7eXgB+85vfsGPHDmbPns3u3bvZtWsXwBuC4pBLLrmE2267DahfeH7ppZc4+eST+fWvf920/zvf+U6++c1vArBjxw6eeuopZs2a1fpAB2AQSCrerFmzWLlyJXPmzGH//v2HT+McMnnyZFavXs0111zD2972Ni666CK2bdvGxIkTWbVqFe973/uYP38+p59+etP1f+lLX2LDhg3MmzePt7/97WzdupWuri4WLFjAOeecw/XXX/+6/p/+9Kd57bXXmDdvHldddRWrV69+3ZFAu0VmHreVHy+1Wi39Yhpp9Hv88ceZM2dOR2vYvXs3V1xxBZs3b+5oHe3WbNtGxMbMrPXv6xGBJBXOIJBUtO7u7jF3NDBUBoGkjhqNp6dHuqFuU4NAUsdMnDiRvXv3GgZtdOj7CCZOnDjo+/g5AkkdM3XqVPr6+njhhRc6XcqYcugbygbLIJDUMSeccMKgv0VLx4+nhiSpcAaBJBWuLUEQEYsiYntE9EbEG/4Bd0RMiIhvV8sfjojufsunR8SBiPjrdtQjSRq8loMgIsYBK4HLgbnANRExt1+3TwD7M/Ns4IvALf2W3wr8e6u1SJKGrh1HBBcAvZn5RGb+DrgTWNyvz2JgTTV9F3BJVP/eLyI+CPwc2NKGWiRJQ9SOIJgCPN0w31e1Ne2TmQeBl4CuiHgT8DngC0d7kIhYFhE9EdHjW80kqX06fbH4JuCLmXngaB0zc1Vm1jKzNnny5ONfmSQVoh2fI9gDNH5P2tSqrVmfvogYD0wC9gLvAJZExD8AbwZei4hXMvMrbahLkjQI7QiCR4CZETGD+h/8q4E/79dnLbAUeBBYAqzP+mfK33moQ0TcBBwwBCRpeLUcBJl5MCKuA+4BxgFfz8wtEXEz0JOZa4HbgW9ERC+wj3pYSJJGAL+YRpIK4RfTSJKaMggkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgrXliCIiEURsT0ieiNieZPlEyLi29XyhyOiu2q/NCI2RsRj1e2ftaMeSdLgtRwEETEOWAlcDswFromIuf26fQLYn5lnA18EbqnaXwTen5nzgKXAN1qtR5I0NO04IrgA6M3MJzLzd8CdwOJ+fRYDa6rpu4BLIiIy86eZ+UzVvgX4g4iY0IaaJEmD1I4gmAI83TDfV7U17ZOZB4GXgK5+ff4b8Ghm/rYNNUmSBml8pwsAiIi3Uj9dtPAIfZYBywCmT58+TJVJ0tjXjiOCPcC0hvmpVVvTPhExHpgE7K3mpwJ3Ax/NzF0DPUhmrsrMWmbWJk+e3IayJUnQniB4BJgZETMi4kTgamBtvz5rqV8MBlgCrM/MjIg3A/8GLM/MH7WhFknSELUcBNU5/+uAe4DHge9k5paIuDkiPlB1ux3oiohe4K+AQ28xvQ44G/jbiNhU/Zzeak2SpMGLzOx0DUNWq9Wyp6en02VI0qgSERszs9a/3U8WS1LhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUuLYEQUQsiojtEdEbEcubLJ8QEd+ulj8cEd0Ny/6mat8eEZe1ox5J0uC1HAQRMQ5YCVwOzAWuiYi5/bp9AtifmWcDXwRuqe47F7gaeCuwCPhqtT5J0jAZ34Z1XAD0ZuYTABFxJ7AY2NrQZzFwUzV9F/CViIiq/c7M/C3w84jordb3YBvqeoMv/OsWfvHSK8dj1ZI0LL509fmcOL69Z/XbEQRTgKcb5vuAdwzUJzMPRsRLQFfV/lC/+05p9iARsQxYBjB9+vRjKvTpff/JU/t+c0z3laSRIMm2r7MdQTAsMnMVsAqgVqsd05b42tJaW2uSpLGgHccXe4BpDfNTq7amfSJiPDAJ2DvI+0qSjqN2BMEjwMyImBERJ1K/+Lu2X5+1wNJqegmwPjOzar+6elfRDGAm8JM21CRJGqSWTw1V5/yvA+4BxgFfz8wtEXEz0JOZa4HbgW9UF4P3UQ8Lqn7foX5h+SDwF5n5aqs1SZIGL+ovzEeXWq2WPT09nS5DkkaViNiYmW+4WOoniyWpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhWgqCiDg1ItZFxM7q9pQB+i2t+uyMiKVV23+NiH+LiG0RsSUiVrRSiyTp2LR6RLAcuC8zZwL3VfOvExGnAjcC7wAuAG5sCIz/lZmzgfOBBRFxeYv1SJKGqNUgWAysqabXAB9s0ucyYF1m7svM/cA6YFFmvpyZGwAy83fAo8DUFuuRJA1Rq0FwRmY+W03/AjijSZ8pwNMN831V22ER8Wbg/dSPKiRJw2j80TpExL3AHzVZ9PnGmczMiMihFhAR44E7gC9n5hNH6LcMWAYwffr0oT6MJGkARw2CzHzPQMsi4rmIODMzn42IM4Hnm3TbA7y7YX4qcH/D/CpgZ2b+01HqWFX1pVarDTlwJEnNtXpqaC2wtJpeCny/SZ97gIURcUp1kXhh1UZE/D0wCfjLFuuQJB2jVoNgBXBpROwE3lPNExG1iPgaQGbuA/4OeKT6uTkz90XEVOqnl+YCj0bEpoj47y3WI0kaosgcfWdZarVa9vT0dLoMSRpVImJjZtb6t/vJYkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCtdSEETEqRGxLiJ2VrenDNBvadVnZ0QsbbJ8bURsbqUWSdKxafWIYDlwX2bOBO6r5l8nIk4FbgTeAVwA3NgYGBFxJXCgxTokSceo1SBYDKypptcAH2zS5zJgXWbuy8z9wDpgEUBEvAn4K+DvW6xDknSMWg2CMzLz2Wr6F8AZTfpMAZ5umO+r2gD+DvhH4OWjPVBELIuInojoeeGFF1ooWZLUaPzROkTEvcAfNVn0+caZzMyIyME+cEScB/xJZv7PiOg+Wv/MXAWsAqjVaoN+HEnSkR01CDLzPQMti4jnIuLMzHw2Is4Enm/SbQ/w7ob5qcD9wEVALSJ2V3WcHhH3Z+a7kSQNm1ZPDa0FDr0LaCnw/SZ97gEWRsQp1UXihcA9mXlbZp6Vmd3AxcAOQ0CShl+rQbACuDQidgLvqeaJiFpEfA0gM/dRvxbwSPVzc9UmSRoBInP0nW6v1WrZ09PT6TIkaVSJiI2ZWevf7ieLJalwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhYvM7HQNQxYRLwBPHuPdTwNebGM5I11J4y1prOB4x7LjNda3ZObk/o2jMghaERE9mVnrdB3DpaTxljRWcLxj2XCP1VNDklQ4g0CSCldiEKzqdAHDrKTxljRWcLxj2bCOtbhrBJKk1yvxiECS1MAgkKTCFRMEEbEoIrZHRG9ELO90Pa2IiN0R8VhEbIqInqrt1IhYFxE7q9tTqvaIiC9X4/5ZRMxvWM/Sqv/OiFjaqfH0FxFfj4jnI2JzQ1vbxhcRb6+2X2913xjeEf5/A4z1pojYU+3fTRHx3oZlf1PVvT0iLmtob/r8jogZEfFw1f7tiDhx+Eb3RhExLSI2RMTWiNgSEf+jah9z+/cIYx15+zczx/wPMA7YBfwxcCLwH8DcTtfVwnh2A6f1a/sHYHk1vRy4pZp+L/DvQAAXAg9X7acCT1S3p1TTp3R6bFVtfwrMBzYfj/EBP6n6RnXfy0fYWG8C/rpJ37nVc3cCMKN6To870vMb+A5wdTX9z8CnOrxvzwTmV9MnAzuqcY25/XuEsY64/VvKEcEFQG9mPpGZvwPuBBZ3uKZ2WwysqabXAB9saP+XrHsIeHNEnAlcBqzLzH2ZuR9YBywa5pqbysz/C+zr19yW8VXL/jAzH8r6b8+/NKxr2A0w1oEsBu7MzN9m5s+BXurP7abP7+qV8J8Bd1X3b9xuHZGZz2bmo9X0r4HHgSmMwf17hLEOpGP7t5QgmAI83TDfx5F3yEiXwA8jYmNELKvazsjMZ6vpXwBnVNMDjX20bZN2jW9KNd2/faS5rjoV8vVDp0kY+li7gF9m5sF+7SNCRHQD5wMPM8b3b7+xwgjbv6UEwVhzcWbOBy4H/iIi/rRxYfVKaMy+L3isjw+4DfgT4DzgWeAfO1rNcRARbwL+D/CXmfmrxmVjbf82GeuI27+lBMEeYFrD/NSqbVTKzD3V7fPA3dQPHZ+rDoupbp+vug809tG2Tdo1vj3VdP/2ESMzn8vMVzPzNeB/U9+/MPSx7qV+KmV8v/aOiogTqP9h/GZmfq9qHpP7t9lYR+L+LSUIHgFmVlfYTwSuBtZ2uKZjEhEnRcTJh6aBhcBm6uM59M6JpcD3q+m1wEerd19cCLxUHYLfAyyMiFOqQ9OFVdtI1ZbxVct+FREXVudYP9qwrhHh0B/Eyoeo71+oj/XqiJgQETOAmdQvjDZ9flevrDcAS6r7N263jqi2+e3A45l5a8OiMbd/BxrriNy/w30lvVM/1N99sIP61ffPd7qeFsbxx9TfNfAfwJZDY6F+vvA+YCdwL3Bq1R7AymrcjwG1hnV9nPoFqV7gY50eW0Ndd1A/ZP499fOen2jn+IBa9cu3C/gK1SfsR9BYv1GN5WfU/zic2dD/81Xd22l4N8xAz+/q+fKTaht8F5jQ4X17MfXTPj8DNlU/7x2L+/cIYx1x+9d/MSFJhSvl1JAkaQAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSrc/wNlZ5kjFPs2MgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = cnn_500.predict(X_test_ep[:,:,:, np.newaxis], verbose= 1)\n",
    "# pred = cnn_right.predict(X_val_ep[:,:,:, np.newaxis])\n",
    "plt.plot(pred, label = 'prediction')\n",
    "#plt.plot(y_test_ep[:,0,0], label = 'ground truth', alpha = 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 500 with 25 sized kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 500, 6, 64)        1664      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 500, 6, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 500, 6, 64)        102464    \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 500, 6, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 250, 3, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 250, 3, 64)        102464    \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 250, 3, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 125, 2, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1024064   \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 1,238,577\n",
      "Trainable params: 1,238,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_500_k25 = make_cnnModel(input_shape= (500,6,1), pool = [0,1,1], kernel_sizes=[[25,1],[25,1],[25,1]], output_units=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1138\n",
      "Epoch 00001: val_loss improved from inf to 0.06351, saving model to models/v2\\best_cnn500k25_model.h5\n",
      "809/809 [==============================] - 2854s 4s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1138 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5804\n",
      "Epoch 2/5\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1137\n",
      "Epoch 00002: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "809/809 [==============================] - 2685s 3s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1137 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5804\n",
      "Epoch 3/5\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1138\n",
      "Epoch 00003: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "809/809 [==============================] - 2684s 3s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1138 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5804\n",
      "Epoch 4/5\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1137\n",
      "Epoch 00004: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "809/809 [==============================] - 2684s 3s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1137 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5804\n",
      "Epoch 5/5\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1137\n",
      "Epoch 00005: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "809/809 [==============================] - 2711s 3s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1137 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5804\n"
     ]
    }
   ],
   "source": [
    "cnn_500_k25.compile(optimizer= Adam(), loss= 'mse', metrics= ['mae',r2_keras])\n",
    "\n",
    "cnn_500k25_callbacks = [\n",
    "    ModelCheckpoint('models/v2/best_cnn500k25_model.h5', monitor='val_loss', verbose=1, save_best_only= True),\n",
    "    ReduceLROnPlateau(patience= 1, monitor = 'val_loss', verbose = 1),\n",
    "    History()\n",
    "    \n",
    "]\n",
    "\n",
    "history_cnn_500k25 = cnn_500_k25.fit(X_train_ep[:,:,:, np.newaxis],y_train_ep[:,0],batch_size= 128, epochs= 5,\n",
    "                  callbacks= cnn_500k25_callbacks,\n",
    "                 validation_data= (X_test_ep[:,:,:,np.newaxis], y_test_ep[:,0])\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797/797 [==============================] - 143s 179ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x224409b34c8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = cnn_500_k25.predict(X_test_ep[:,:,:, np.newaxis], verbose= 1)\n",
    "# pred = cnn_right.predict(X_val_ep[:,:,:, np.newaxis])\n",
    "plt.plot(pred, label = 'prediction')\n",
    "#plt.plot(y_test_ep[:,0,0], label = 'ground truth', alpha = 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn 500_k3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 500, 6, 6)         60        \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 500, 6, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 500, 6, 6)         330       \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 500, 6, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 250, 3, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 250, 3, 6)         330       \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 250, 3, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 125, 2, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 125, 2, 6)         330       \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 125, 2, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 63, 1, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 63, 1, 6)          330       \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 63, 1, 6)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 32, 1, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 1, 6)          330       \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 32, 1, 6)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 1, 6)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "re_lu_16 (ReLU)              (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "re_lu_17 (ReLU)              (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 15,839\n",
      "Trainable params: 15,839\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_500_k3 = make_cnnModel(input_shape= (500,6,1), n_conv= 6\n",
    "                           , pool = [0,1,1,1,1,1], kernel_sizes=[3,3,3,3,3,3]\n",
    "                           , conv_units = [6,6,6,6,6,6] ,  output_units=1  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0350 - mae: 0.0880 - r2_keras: 0.3399\n",
      "Epoch 00001: val_loss improved from inf to 0.07258, saving model to models/v2\\best_cnn500k3_model.h5\n",
      "809/809 [==============================] - 163s 202ms/step - loss: 0.0350 - mae: 0.0880 - r2_keras: 0.3399 - val_loss: 0.0726 - val_mae: 0.1213 - val_r2_keras: -35530380.0000\n",
      "Epoch 2/2\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0274 - mae: 0.0728 - r2_keras: 0.4835\n",
      "Epoch 00002: val_loss improved from 0.07258 to 0.07074, saving model to models/v2\\best_cnn500k3_model.h5\n",
      "809/809 [==============================] - 164s 203ms/step - loss: 0.0274 - mae: 0.0728 - r2_keras: 0.4835 - val_loss: 0.0707 - val_mae: 0.1161 - val_r2_keras: -31001432.0000\n"
     ]
    }
   ],
   "source": [
    "cnn_500_k3.compile(optimizer= Adam(), loss= 'mse', metrics= ['mae',r2_keras])\n",
    "\n",
    "cnn_500k3_callbacks = [\n",
    "    ModelCheckpoint('models/v2/best_cnn500k3_model.h5', monitor='val_loss', verbose=1, save_best_only= True),\n",
    "    ReduceLROnPlateau(patience= 1, monitor = 'val_loss', verbose = 1),\n",
    "    History()\n",
    "    \n",
    "]\n",
    "\n",
    "history_cnn_500k3 = cnn_500_k3.fit(X_train_ep[:,:,:, np.newaxis],y_train_ep[:,0],batch_size= 128, epochs= 2,\n",
    "                  callbacks= cnn_500k3_callbacks,\n",
    "                 validation_data= (X_test_ep[:,:,:,np.newaxis], y_test_ep[:,0])\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797/797 [==============================] - 21s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x224dc8afd08>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = cnn_500_k3.predict(X_test_ep[:,:,:, np.newaxis], verbose= 1)\n",
    "# pred = cnn_right.predict(X_val_ep[:,:,:, np.newaxis])\n",
    "plt.plot(pred, label = 'prediction')\n",
    "#plt.plot(y_test_ep[:,0,0], label = 'ground truth', alpha = 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1140\n",
      "Epoch 00001: val_loss improved from inf to 0.06351, saving model to models/v2\\best_cnn_500_k100_model.h5\n",
      "809/809 [==============================] - 2331s 3s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1140 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5804\n",
      "Epoch 2/2\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1137\n",
      "Epoch 00002: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "809/809 [==============================] - 2325s 3s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1137 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5804\n"
     ]
    }
   ],
   "source": [
    "cnn_500_k100 = make_cnnModel(input_shape= (500,6,1), pool = [0,1,1], summary = False,\n",
    "                            kernel_sizes=[[100,6],[50,6],[25,6]], output_units=1,\n",
    "                            conv_units = [ 6, 6, 6], dense_units= [100,100])\n",
    "\n",
    "cnn_500_k100.compile(optimizer= Adam(), loss= 'mse', metrics= ['mae',r2_keras])\n",
    "\n",
    "cnn_500_k100_callbacks = [\n",
    "    ModelCheckpoint('models/v2/best_cnn_500_k100_model.h5', monitor='val_loss', verbose=1, save_best_only= True),\n",
    "    ReduceLROnPlateau(patience= 1, monitor = 'val_loss', verbose = 1),\n",
    "    History()\n",
    "    \n",
    "]\n",
    "\n",
    "history_cnn_500_k100 = cnn_500_k100.fit(X_train_ep[:,:,:, np.newaxis],y_train_ep[:,0],batch_size= 128, epochs= 2,\n",
    "                  callbacks= cnn_500_k100_callbacks,\n",
    "                 validation_data= (X_test_ep[:,:,:,np.newaxis], y_test_ep[:,0])\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797/797 [==============================] - 105s 132ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x224dcb3ef88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = cnn_500_k100.predict(X_test_ep[:,:,:, np.newaxis], verbose= 1)\n",
    "# pred = cnn_right.predict(X_val_ep[:,:,:, np.newaxis])\n",
    "plt.plot(pred, label = 'prediction')\n",
    "#plt.plot(y_test_ep[:,0,0], label = 'ground truth', alpha = 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0594 - mae: 0.0779 - r2_keras: -0.1185\n",
      "Epoch 00001: val_loss improved from inf to 0.06351, saving model to models/v2\\best_cnn_500_k500_model.h5\n",
      "809/809 [==============================] - 7741s 10s/step - loss: 0.0594 - mae: 0.0779 - r2_keras: -0.1185 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5804\n",
      "Epoch 2/2\n",
      "809/809 [==============================] - ETA: 0s - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1136\n",
      "Epoch 00002: val_loss did not improve from 0.06351\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "809/809 [==============================] - 7773s 10s/step - loss: 0.0592 - mae: 0.0776 - r2_keras: -0.1136 - val_loss: 0.0635 - val_mae: 0.0779 - val_r2_keras: 0.5804\n"
     ]
    }
   ],
   "source": [
    "cnn_500_k500 = make_cnnModel(input_shape= (500,6,1), pool = [0,1,1], summary = False,\n",
    "                            kernel_sizes=[[500,2],[500,2],[500,2]], output_units=1,\n",
    "                            conv_units = [ 6, 6, 6], dense_units= [100,100])\n",
    "\n",
    "cnn_500_k500.compile(optimizer= Adam(), loss= 'mse', metrics= ['mae',r2_keras])\n",
    "\n",
    "cnn_500_k500_callbacks = [\n",
    "    ModelCheckpoint('models/v2/best_cnn_500_k500_model.h5', monitor='val_loss', verbose=1, save_best_only= True),\n",
    "    ReduceLROnPlateau(patience= 1, monitor = 'val_loss', verbose = 1),\n",
    "    History()\n",
    "    \n",
    "]\n",
    "\n",
    "history_cnn_500_k500 = cnn_500_k500.fit(X_train_ep[:,:,:, np.newaxis],y_train_ep[:,0],batch_size= 128, epochs= 2,\n",
    "                  callbacks= cnn_500_k500_callbacks,\n",
    "                 validation_data= (X_test_ep[:,:,:,np.newaxis], y_test_ep[:,0])\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_backup = rnn_right\n",
    "cnn_backup = cnn_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_aeModel(input_shape = (1,6), summary = True\n",
    "                 ,rnn_model = rnn_backup, cnn_model = cnn_backup,\n",
    "                 output_n = 2\n",
    "                                                        ):\n",
    "    \n",
    "    input_layer = Input(input_shape)\n",
    "    input_layer_extended = tf.keras.backend.expand_dims(input_layer)\n",
    "    \n",
    "    rnn_model = Model(rnn_model.input, rnn_model.layers[-2].output, name = 'RNN_Model')\n",
    "    cnn_model = Model(cnn_model.input, cnn_model.layers[-2].output, name = 'CNN_Model')\n",
    "\n",
    "    for layer in rnn_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    for layer in cnn_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    rnn_features = rnn_model(input_layer)\n",
    "    cnn_features = cnn_model(input_layer_extended)\n",
    "    \n",
    "    concat = Concatenate(axis = -1, name = 'Concat')([rnn_features,cnn_features])\n",
    "    \n",
    "    x = Dense(200, name = 'Dense_1')(concat)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(100, name = 'Dense_2')(x)\n",
    "    x = ReLU()(x)\n",
    "    output = Dense(output_n, activation = 'sigmoid' , name = 'output')(x)\n",
    "    \n",
    "    ae = Model(input_layer, output)\n",
    "    \n",
    "    \n",
    "    ae.summary()\n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 1, 6)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_3 (Tenso [(None, 1, 6, 1)]    0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "RNN_Model (Functional)          (None, 64)           74816       input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "CNN_Model (Functional)          (None, 120)          90552       tf_op_layer_ExpandDims_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Concat (Concatenate)            (None, 184)          0           RNN_Model[0][0]                  \n",
      "                                                                 CNN_Model[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dense_1 (Dense)                 (None, 200)          37000       Concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_55 (ReLU)                 (None, 200)          0           Dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense_2 (Dense)                 (None, 100)          20100       re_lu_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_56 (ReLU)                 (None, 100)          0           Dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         re_lu_56[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 222,569\n",
      "Trainable params: 57,201\n",
      "Non-trainable params: 165,368\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae_right = make_aeModel(output_n= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_right.compile(optimizer=Adam(), loss = r2_keras_loss, metrics=['mae', 'mse', r2_keras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_callbacks = [\n",
    "    ModelCheckpoint('best_aeR_model.h5', monitor='val_loss', verbose=1, save_best_only= True),\n",
    "    ReduceLROnPlateau(patience= 10, monitor = 'val_loss'),\n",
    "    History()\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "639/650 [============================>.] - ETA: 0s - loss: 0.6732 - mae: 0.0916 - mse: 0.0348 - r2_keras: 0.3268\n",
      "Epoch 00001: val_loss did not improve from inf\n",
      "650/650 [==============================] - 3s 5ms/step - loss: 0.6732 - mae: 0.0916 - mse: 0.0348 - r2_keras: 0.3268 - val_loss: inf - val_mae: 0.1174 - val_mse: 0.0714 - val_r2_keras: -13799993.0000\n",
      "Epoch 2/500\n",
      "643/650 [============================>.] - ETA: 0s - loss: 0.6726 - mae: 0.0915 - mse: 0.0348 - r2_keras: 0.3274\n",
      "Epoch 00002: val_loss did not improve from inf\n",
      "650/650 [==============================] - 3s 5ms/step - loss: 0.6727 - mae: 0.0915 - mse: 0.0348 - r2_keras: 0.3273 - val_loss: inf - val_mae: 0.1201 - val_mse: 0.0714 - val_r2_keras: -14771726.0000\n",
      "Epoch 3/500\n",
      "644/650 [============================>.] - ETA: 0s - loss: 0.6719 - mae: 0.0913 - mse: 0.0347 - r2_keras: 0.3281\n",
      "Epoch 00003: val_loss did not improve from inf\n",
      "650/650 [==============================] - 3s 5ms/step - loss: 0.6716 - mae: 0.0914 - mse: 0.0347 - r2_keras: 0.3284 - val_loss: inf - val_mae: 0.1217 - val_mse: 0.0713 - val_r2_keras: -15298302.0000\n",
      "Epoch 4/500\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.6702 - mae: 0.0915 - mse: 0.0347 - r2_keras: 0.3298\n",
      "Epoch 00004: val_loss did not improve from inf\n",
      "650/650 [==============================] - 3s 5ms/step - loss: 0.6704 - mae: 0.0915 - mse: 0.0347 - r2_keras: 0.3296 - val_loss: inf - val_mae: 0.1192 - val_mse: 0.0715 - val_r2_keras: -14581215.0000\n",
      "Epoch 5/500\n",
      "640/650 [============================>.] - ETA: 0s - loss: 0.6701 - mae: 0.0913 - mse: 0.0347 - r2_keras: 0.3299\n",
      "Epoch 00005: val_loss did not improve from inf\n",
      "650/650 [==============================] - 3s 5ms/step - loss: 0.6695 - mae: 0.0913 - mse: 0.0346 - r2_keras: 0.3305 - val_loss: inf - val_mae: 0.1201 - val_mse: 0.0711 - val_r2_keras: -14673543.0000\n",
      "Epoch 6/500\n",
      "646/650 [============================>.] - ETA: 0s - loss: 0.6688 - mae: 0.0911 - mse: 0.0346 - r2_keras: 0.3312\n",
      "Epoch 00006: val_loss did not improve from inf\n",
      "650/650 [==============================] - 4s 6ms/step - loss: 0.6688 - mae: 0.0910 - mse: 0.0346 - r2_keras: 0.3312 - val_loss: inf - val_mae: 0.1188 - val_mse: 0.0713 - val_r2_keras: -14385249.0000\n",
      "Epoch 7/500\n",
      "641/650 [============================>.] - ETA: 0s - loss: 0.6684 - mae: 0.0912 - mse: 0.0345 - r2_keras: 0.3316\n",
      "Epoch 00007: val_loss did not improve from inf\n",
      "650/650 [==============================] - 4s 6ms/step - loss: 0.6679 - mae: 0.0913 - mse: 0.0345 - r2_keras: 0.3321 - val_loss: inf - val_mae: 0.1221 - val_mse: 0.0714 - val_r2_keras: -15800940.0000\n",
      "Epoch 8/500\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.6661 - mae: 0.0913 - mse: 0.0344 - r2_keras: 0.3339\n",
      "Epoch 00008: val_loss did not improve from inf\n",
      "650/650 [==============================] - 5s 7ms/step - loss: 0.6658 - mae: 0.0913 - mse: 0.0344 - r2_keras: 0.3342 - val_loss: inf - val_mae: 0.1156 - val_mse: 0.0711 - val_r2_keras: -13245418.0000\n",
      "Epoch 9/500\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.6641 - mae: 0.0913 - mse: 0.0344 - r2_keras: 0.3359\n",
      "Epoch 00009: val_loss did not improve from inf\n",
      "650/650 [==============================] - 4s 6ms/step - loss: 0.6640 - mae: 0.0913 - mse: 0.0343 - r2_keras: 0.3360 - val_loss: inf - val_mae: 0.1212 - val_mse: 0.0710 - val_r2_keras: -15358519.0000\n",
      "Epoch 10/500\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.6638 - mae: 0.0913 - mse: 0.0343 - r2_keras: 0.3362\n",
      "Epoch 00010: val_loss did not improve from inf\n",
      "650/650 [==============================] - 4s 6ms/step - loss: 0.6634 - mae: 0.0912 - mse: 0.0343 - r2_keras: 0.3366 - val_loss: inf - val_mae: 0.1167 - val_mse: 0.0712 - val_r2_keras: -13850150.0000\n",
      "Epoch 11/500\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.6612 - mae: 0.0912 - mse: 0.0342 - r2_keras: 0.3388\n",
      "Epoch 00011: val_loss did not improve from inf\n",
      "650/650 [==============================] - 4s 6ms/step - loss: 0.6610 - mae: 0.0912 - mse: 0.0342 - r2_keras: 0.3390 - val_loss: inf - val_mae: 0.1208 - val_mse: 0.0710 - val_r2_keras: -15327943.0000\n",
      "Epoch 12/500\n",
      "642/650 [============================>.] - ETA: 0s - loss: 0.6594 - mae: 0.0916 - mse: 0.0342 - r2_keras: 0.3406\n",
      "Epoch 00012: val_loss did not improve from inf\n",
      "650/650 [==============================] - 4s 6ms/step - loss: 0.6597 - mae: 0.0916 - mse: 0.0342 - r2_keras: 0.3403 - val_loss: inf - val_mae: 0.1201 - val_mse: 0.0710 - val_r2_keras: -15002104.0000\n",
      "Epoch 13/500\n",
      "595/650 [==========================>...] - ETA: 0s - loss: 0.6620 - mae: 0.0917 - mse: 0.0343 - r2_keras: 0.3380"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-dc39b9272ff7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history_ae = ae_right.fit(X_train[:,np.newaxis,:],y_train,batch_size= 128, epochs= 500,\n\u001b[0m\u001b[0;32m      2\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mae_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m          )\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_ae = ae_right.fit(X_train[:,np.newaxis,:],y_train,batch_size= 128, epochs= 500,\n",
    "                  callbacks= ae_callbacks,\n",
    "                  validation_data= (X_val[:, np.newaxis, :], y_val[:,0])\n",
    "         )\n",
    "\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 2s 2ms/step - loss: 0.0661 - mae: 0.1154 - r2_keras: -5246552.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06606696546077728, 0.11539081484079361, -5246552.5]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_right.evaluate(X_test[:,np.newaxis,:], y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f8fadad580>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_ae.history['loss'], label = 'mse')\n",
    "plt.plot(history_ae.history['r2_keras'], label = 'r2_keras')\n",
    "plt.plot(history_ae.history['mae'], label = 'mae')\n",
    "plt.title('ae Model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "ae_predictions = ae_right.predict(X_test[:,np.newaxis,:], verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f8fddfd2b0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ae_predictions = ae_predictions.predict(X_test[:,np.newaxis,:], verbose= 1)\n",
    "plt.plot(ae_predictions[:,0], alpha = 0.7, label = 'preds')\n",
    "plt.plot(y_test[:,0], alpha = 0.7, label = 'groud_truth')\n",
    "# plt.ylim(0,7)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep forward NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_50 (Conv2D)           (None, 200, 6, 16)        592       \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 200, 6, 16)        9232      \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 195, 1, 16)        9232      \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 3120)              0         \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 10)                31210     \n",
      "_________________________________________________________________\n",
      "dense_206 (Dense)            (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 50,867\n",
      "Trainable params: 50,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fnn = Sequential()\n",
    "fnn.add(Conv2D(16, kernel_size = 6, input_shape = (200,6,1), activation = 'selu', padding = 'same'))\n",
    "fnn.add(Conv2D(16, kernel_size = 6, activation = 'selu', padding = 'same'))\n",
    "fnn.add(Conv2D(16, kernel_size = 6, activation = 'selu'))\n",
    "# fnn.add(Dense(50, input_shape = (6,1,), activation = \"selu\"))\n",
    "fnn.add(Flatten())\n",
    "fnn.add(Dense(10, activation = 'selu'))\n",
    "\n",
    "# fnn.add(LSTM(50, return_sequences =True))\n",
    "# fnn.add(LSTM(50))\n",
    "\n",
    "fnn.add(Dense(50, activation = 'selu'))\n",
    "# fnn.add(Dense(100, activation = 'selu'))\n",
    "# fnn.add(Dense(50, activation = 'selu'))\n",
    "# fnn.add(Dense(24, activation = 'selu'))\n",
    "# fnn.add(Dense(12, activation = 'selu'))\n",
    "fnn.add(Dense(1, activation = 'sigmoid'))\n",
    "fnn.summary()\n",
    "\n",
    "fnn.compile(optimizer = Adam(), loss = \"mae\", metrics = ['mse',r2_keras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fnn.fit(X_train[:,:,np.newaxis], y_train[:,0], epochs  = 500,\n",
    "#        validation_data= (X_val[:,:,np.newaxis], y_val[:,0])\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 200\n",
    "gen = ecog_gen(X_train[:,:, np.newaxis], y_train[:,0], window= window, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-538-7dd7a75c7bfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mwindow\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "fnn.fit( gen, steps_per_epoch= (len(X_train)-window ), epochs= 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(x, conv= None, k_s = 6, atten_func = Dense, d_k = 6, activation = ReLU):\n",
    "    \n",
    "    # input shape should (ch,1), ch is number of ecog channels\n",
    "    \n",
    "    if conv != None:\n",
    "        input_x = conv(d_k, kernel_size = k_s, padding = 'same')(x)\n",
    "    else:\n",
    "        input_x = x \n",
    "        \n",
    "        \n",
    "    query = atten_func(d_k)(input_x)\n",
    "    query = activation()(query)\n",
    "    \n",
    "    key   = atten_func(d_k)(input_x)\n",
    "    key = activation()(key)\n",
    "    \n",
    "    value = atten_func(d_k, activation = 'relu')(input_x)\n",
    "    value = activation()(value)\n",
    "    \n",
    "    matmul = tf.matmul(query, key, transpose_a=True)\n",
    "\n",
    "    matmul_sqrt = matmul/np.sqrt(query.shape[1])\n",
    "    softmax = Softmax()(matmul_sqrt)\n",
    "    matmul_2 = tf.matmul(value, softmax, transpose_b=False)\n",
    "\n",
    "    d = Dense(d_k)(matmul_2)\n",
    "    d = activation()(d)\n",
    "\n",
    "    d2 = Dense(d_k)(d)\n",
    "    addition = Add()([x, d2])\n",
    "    norm = BatchNormalization()(addition)\n",
    "    \n",
    "    return norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_attenModel(input_shape = (1,6), summary = True\n",
    "                , n_heads = 3, atten_func = Dense\n",
    "                , n_outputs = 2\n",
    "                    \n",
    "                                                   \n",
    "                                                   ):\n",
    "    input_layer = Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    atten_heads = []\n",
    "    for i in range(n_heads):\n",
    "        atten_heads.append(self_attention(input_layer,atten_func= atten_func ))\n",
    "        \n",
    "    concat = atten_heads[0]\n",
    "    for i in range(len(atten_heads)-1):\n",
    "        concat = Concatenate()([concat,atten_heads[i+1]])\n",
    "        \n",
    "    f = Flatten()(concat)\n",
    "    d = Dense(50)(f)\n",
    "    d = ReLU()(d)\n",
    "    d = Dense(50)(d)\n",
    "    d = ReLU()(d)\n",
    "    \n",
    "    output = Dense(n_outputs, 'sigmoid')(d)\n",
    "    \n",
    "    \n",
    "    model = Model(input_layer, output)\n",
    "    \n",
    "    if summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 1, 6)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1, 6)         42          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1, 6)         42          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 1, 6)         42          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 1, 6)         42          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, 1, 6)         0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_40 (ReLU)                 (None, 1, 6)         0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_43 (ReLU)                 (None, 1, 6)         0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_44 (ReLU)                 (None, 1, 6)         0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1, 6)         42          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 1, 6)         42          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2 (Tens [(None, 6, 6)]       0           re_lu_39[0][0]                   \n",
      "                                                                 re_lu_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_2 (Te [(None, 6, 6)]       0           re_lu_43[0][0]                   \n",
      "                                                                 re_lu_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_47 (ReLU)                 (None, 1, 6)         0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_48 (ReLU)                 (None, 1, 6)         0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1, 6)         42          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv (TensorFlow [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1, 6)         42          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_1 (TensorFl [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_4 (Te [(None, 6, 6)]       0           re_lu_47[0][0]                   \n",
      "                                                                 re_lu_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, 1, 6)         0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 6, 6)         0           tf_op_layer_RealDiv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_45 (ReLU)                 (None, 1, 6)         0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 6, 6)         0           tf_op_layer_RealDiv_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1, 6)         42          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_2 (TensorFl [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_1 (Te [(None, 1, 6)]       0           re_lu_41[0][0]                   \n",
      "                                                                 softmax[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_3 (Te [(None, 1, 6)]       0           re_lu_45[0][0]                   \n",
      "                                                                 softmax_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_49 (ReLU)                 (None, 1, 6)         0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax_2 (Softmax)             (None, 6, 6)         0           tf_op_layer_RealDiv_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_5 (Te [(None, 1, 6)]       0           re_lu_49[0][0]                   \n",
      "                                                                 softmax_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_42 (ReLU)                 (None, 1, 6)         0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_46 (ReLU)                 (None, 1, 6)         0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1, 6)         42          re_lu_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 1, 6)         42          re_lu_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_50 (ReLU)                 (None, 1, 6)         0           dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1, 6)         0           input_10[0][0]                   \n",
      "                                                                 dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1, 6)         0           input_10[0][0]                   \n",
      "                                                                 dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 1, 6)         42          re_lu_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1, 6)         24          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 6)         24          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1, 6)         0           input_10[0][0]                   \n",
      "                                                                 dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 12)        0           batch_normalization[0][0]        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1, 6)         24          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 18)        0           concatenate[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 18)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 50)           950         flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_51 (ReLU)                 (None, 50)           0           dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 50)           2550        re_lu_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_52 (ReLU)                 (None, 50)           0           dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 1)            51          re_lu_52[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,253\n",
      "Trainable params: 4,217\n",
      "Non-trainable params: 36\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "atten = make_attenModel(n_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten.compile(optimizer = Adam(), loss = 'mse', metrics= ['mae', r2_keras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten_callbacks = [\n",
    "    ModelCheckpoint('best_atten_model.h5', monitor='loss', verbose=1, save_best_only= True),\n",
    "    ReduceLROnPlateau(patience= 10, monitor = 'loss'),\n",
    "    History()\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "631/650 [============================>.] - ETA: 0s - loss: 0.0345 - mae: 0.0948 - r2_keras: 0.3327\n",
      "Epoch 00001: loss improved from 0.03452 to 0.03440, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 4ms/step - loss: 0.0344 - mae: 0.0947 - r2_keras: 0.3337 - val_loss: 0.0708 - val_mae: 0.1307 - val_r2_keras: -17600684.0000\n",
      "Epoch 2/500\n",
      "643/650 [============================>.] - ETA: 0s - loss: 0.0340 - mae: 0.0937 - r2_keras: 0.3410\n",
      "Epoch 00002: loss improved from 0.03440 to 0.03401, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0340 - mae: 0.0937 - r2_keras: 0.3405 - val_loss: 0.0708 - val_mae: 0.1318 - val_r2_keras: -18832324.0000\n",
      "Epoch 3/500\n",
      "639/650 [============================>.] - ETA: 0s - loss: 0.0339 - mae: 0.0933 - r2_keras: 0.3425\n",
      "Epoch 00003: loss improved from 0.03401 to 0.03390, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0339 - mae: 0.0933 - r2_keras: 0.3423 - val_loss: 0.0709 - val_mae: 0.1320 - val_r2_keras: -18706814.0000\n",
      "Epoch 4/500\n",
      "635/650 [============================>.] - ETA: 0s - loss: 0.0340 - mae: 0.0930 - r2_keras: 0.3431\n",
      "Epoch 00004: loss did not improve from 0.03390\n",
      "650/650 [==============================] - 2s 2ms/step - loss: 0.0339 - mae: 0.0930 - r2_keras: 0.3432 - val_loss: 0.0711 - val_mae: 0.1346 - val_r2_keras: -20340000.0000\n",
      "Epoch 5/500\n",
      "630/650 [============================>.] - ETA: 0s - loss: 0.0336 - mae: 0.0927 - r2_keras: 0.3480\n",
      "Epoch 00005: loss improved from 0.03390 to 0.03360, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0336 - mae: 0.0927 - r2_keras: 0.3480 - val_loss: 0.0703 - val_mae: 0.1304 - val_r2_keras: -17491232.0000\n",
      "Epoch 6/500\n",
      "645/650 [============================>.] - ETA: 0s - loss: 0.0335 - mae: 0.0916 - r2_keras: 0.3513\n",
      "Epoch 00006: loss improved from 0.03360 to 0.03348, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0335 - mae: 0.0916 - r2_keras: 0.3512 - val_loss: 0.0708 - val_mae: 0.1330 - val_r2_keras: -19515390.0000\n",
      "Epoch 7/500\n",
      "632/650 [============================>.] - ETA: 0s - loss: 0.0335 - mae: 0.0921 - r2_keras: 0.3492\n",
      "Epoch 00007: loss did not improve from 0.03348\n",
      "650/650 [==============================] - 2s 2ms/step - loss: 0.0335 - mae: 0.0920 - r2_keras: 0.3495 - val_loss: 0.0710 - val_mae: 0.1314 - val_r2_keras: -19194494.0000\n",
      "Epoch 8/500\n",
      "635/650 [============================>.] - ETA: 0s - loss: 0.0335 - mae: 0.0922 - r2_keras: 0.3512\n",
      "Epoch 00008: loss improved from 0.03348 to 0.03347, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0335 - mae: 0.0922 - r2_keras: 0.3512 - val_loss: 0.0705 - val_mae: 0.1323 - val_r2_keras: -19180724.0000\n",
      "Epoch 9/500\n",
      "636/650 [============================>.] - ETA: 0s - loss: 0.0333 - mae: 0.0917 - r2_keras: 0.3530\n",
      "Epoch 00009: loss improved from 0.03347 to 0.03331, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0333 - mae: 0.0917 - r2_keras: 0.3532 - val_loss: 0.0705 - val_mae: 0.1299 - val_r2_keras: -17793532.0000\n",
      "Epoch 10/500\n",
      "645/650 [============================>.] - ETA: 0s - loss: 0.0333 - mae: 0.0918 - r2_keras: 0.3543\n",
      "Epoch 00010: loss did not improve from 0.03331\n",
      "650/650 [==============================] - 2s 2ms/step - loss: 0.0333 - mae: 0.0917 - r2_keras: 0.3544 - val_loss: 0.0710 - val_mae: 0.1284 - val_r2_keras: -16577641.0000\n",
      "Epoch 11/500\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0333 - mae: 0.0916 - r2_keras: 0.3542\n",
      "Epoch 00011: loss improved from 0.03331 to 0.03329, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0333 - mae: 0.0915 - r2_keras: 0.3542 - val_loss: 0.0709 - val_mae: 0.1304 - val_r2_keras: -19364612.0000\n",
      "Epoch 12/500\n",
      "636/650 [============================>.] - ETA: 0s - loss: 0.0330 - mae: 0.0910 - r2_keras: 0.3600\n",
      "Epoch 00012: loss improved from 0.03329 to 0.03306, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0331 - mae: 0.0911 - r2_keras: 0.3597 - val_loss: 0.0717 - val_mae: 0.1322 - val_r2_keras: -20667452.0000\n",
      "Epoch 13/500\n",
      "630/650 [============================>.] - ETA: 0s - loss: 0.0328 - mae: 0.0902 - r2_keras: 0.3651\n",
      "Epoch 00013: loss improved from 0.03306 to 0.03278, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0328 - mae: 0.0903 - r2_keras: 0.3651 - val_loss: 0.0706 - val_mae: 0.1265 - val_r2_keras: -16753190.0000\n",
      "Epoch 14/500\n",
      "644/650 [============================>.] - ETA: 0s - loss: 0.0328 - mae: 0.0902 - r2_keras: 0.3641\n",
      "Epoch 00014: loss improved from 0.03278 to 0.03277, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0328 - mae: 0.0902 - r2_keras: 0.3643 - val_loss: 0.0705 - val_mae: 0.1259 - val_r2_keras: -16634330.0000\n",
      "Epoch 15/500\n",
      "639/650 [============================>.] - ETA: 0s - loss: 0.0329 - mae: 0.0900 - r2_keras: 0.3626\n",
      "Epoch 00015: loss did not improve from 0.03277\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0329 - mae: 0.0899 - r2_keras: 0.3620 - val_loss: 0.0706 - val_mae: 0.1264 - val_r2_keras: -16684725.0000\n",
      "Epoch 16/500\n",
      "630/650 [============================>.] - ETA: 0s - loss: 0.0327 - mae: 0.0897 - r2_keras: 0.3671\n",
      "Epoch 00016: loss improved from 0.03277 to 0.03266, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0327 - mae: 0.0895 - r2_keras: 0.3674 - val_loss: 0.0719 - val_mae: 0.1279 - val_r2_keras: -18898370.0000\n",
      "Epoch 17/500\n",
      "634/650 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0893 - r2_keras: 0.3680\n",
      "Epoch 00017: loss improved from 0.03266 to 0.03261, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0326 - mae: 0.0893 - r2_keras: 0.3678 - val_loss: 0.0706 - val_mae: 0.1265 - val_r2_keras: -16175677.0000\n",
      "Epoch 18/500\n",
      "643/650 [============================>.] - ETA: 0s - loss: 0.0327 - mae: 0.0893 - r2_keras: 0.3673\n",
      "Epoch 00018: loss did not improve from 0.03261\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0326 - mae: 0.0892 - r2_keras: 0.3675 - val_loss: 0.0709 - val_mae: 0.1266 - val_r2_keras: -16498605.0000\n",
      "Epoch 19/500\n",
      "643/650 [============================>.] - ETA: 0s - loss: 0.0325 - mae: 0.0891 - r2_keras: 0.3701\n",
      "Epoch 00019: loss improved from 0.03261 to 0.03248, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0325 - mae: 0.0892 - r2_keras: 0.3703 - val_loss: 0.0730 - val_mae: 0.1373 - val_r2_keras: -24865244.0000\n",
      "Epoch 20/500\n",
      "632/650 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0893 - r2_keras: 0.3686\n",
      "Epoch 00020: loss did not improve from 0.03248\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0326 - mae: 0.0892 - r2_keras: 0.3687 - val_loss: 0.0719 - val_mae: 0.1293 - val_r2_keras: -18917786.0000\n",
      "Epoch 21/500\n",
      "635/650 [============================>.] - ETA: 0s - loss: 0.0324 - mae: 0.0892 - r2_keras: 0.3723\n",
      "Epoch 00021: loss improved from 0.03248 to 0.03241, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0324 - mae: 0.0892 - r2_keras: 0.3719 - val_loss: 0.0708 - val_mae: 0.1243 - val_r2_keras: -15523195.0000\n",
      "Epoch 22/500\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0325 - mae: 0.0890 - r2_keras: 0.3704\n",
      "Epoch 00022: loss did not improve from 0.03241\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0325 - mae: 0.0890 - r2_keras: 0.3705 - val_loss: 0.0711 - val_mae: 0.1283 - val_r2_keras: -18685550.0000\n",
      "Epoch 23/500\n",
      "641/650 [============================>.] - ETA: 0s - loss: 0.0324 - mae: 0.0888 - r2_keras: 0.3725\n",
      "Epoch 00023: loss improved from 0.03241 to 0.03241, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0324 - mae: 0.0888 - r2_keras: 0.3718 - val_loss: 0.0706 - val_mae: 0.1234 - val_r2_keras: -14760015.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "627/650 [===========================>..] - ETA: 0s - loss: 0.0325 - mae: 0.0889 - r2_keras: 0.3711\n",
      "Epoch 00024: loss did not improve from 0.03241\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0325 - mae: 0.0889 - r2_keras: 0.3709 - val_loss: 0.0714 - val_mae: 0.1246 - val_r2_keras: -15894467.0000\n",
      "Epoch 25/500\n",
      "646/650 [============================>.] - ETA: 0s - loss: 0.0324 - mae: 0.0888 - r2_keras: 0.3726\n",
      "Epoch 00025: loss improved from 0.03241 to 0.03237, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0324 - mae: 0.0887 - r2_keras: 0.3728 - val_loss: 0.0717 - val_mae: 0.1276 - val_r2_keras: -19158272.0000\n",
      "Epoch 26/500\n",
      "644/650 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0886 - r2_keras: 0.3739\n",
      "Epoch 00026: loss improved from 0.03237 to 0.03232, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0323 - mae: 0.0886 - r2_keras: 0.3738 - val_loss: 0.0710 - val_mae: 0.1233 - val_r2_keras: -16815188.0000\n",
      "Epoch 27/500\n",
      "643/650 [============================>.] - ETA: 0s - loss: 0.0322 - mae: 0.0883 - r2_keras: 0.3749\n",
      "Epoch 00027: loss improved from 0.03232 to 0.03225, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0322 - mae: 0.0883 - r2_keras: 0.3749 - val_loss: 0.0716 - val_mae: 0.1303 - val_r2_keras: -20155702.0000\n",
      "Epoch 28/500\n",
      "643/650 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0885 - r2_keras: 0.3732\n",
      "Epoch 00028: loss did not improve from 0.03225\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0323 - mae: 0.0884 - r2_keras: 0.3730 - val_loss: 0.0709 - val_mae: 0.1237 - val_r2_keras: -16310013.0000\n",
      "Epoch 29/500\n",
      "635/650 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0883 - r2_keras: 0.3745\n",
      "Epoch 00029: loss did not improve from 0.03225\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0323 - mae: 0.0883 - r2_keras: 0.3747 - val_loss: 0.0710 - val_mae: 0.1257 - val_r2_keras: -17190322.0000\n",
      "Epoch 30/500\n",
      "637/650 [============================>.] - ETA: 0s - loss: 0.0322 - mae: 0.0884 - r2_keras: 0.3772\n",
      "Epoch 00030: loss improved from 0.03225 to 0.03215, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0321 - mae: 0.0882 - r2_keras: 0.3775 - val_loss: 0.0710 - val_mae: 0.1270 - val_r2_keras: -18497964.0000\n",
      "Epoch 31/500\n",
      "631/650 [============================>.] - ETA: 0s - loss: 0.0322 - mae: 0.0880 - r2_keras: 0.3764\n",
      "Epoch 00031: loss did not improve from 0.03215\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0322 - mae: 0.0880 - r2_keras: 0.3766 - val_loss: 0.0713 - val_mae: 0.1261 - val_r2_keras: -18054352.0000\n",
      "Epoch 32/500\n",
      "636/650 [============================>.] - ETA: 0s - loss: 0.0321 - mae: 0.0878 - r2_keras: 0.3780\n",
      "Epoch 00032: loss improved from 0.03215 to 0.03209, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0321 - mae: 0.0878 - r2_keras: 0.3785 - val_loss: 0.0722 - val_mae: 0.1264 - val_r2_keras: -18801126.0000\n",
      "Epoch 33/500\n",
      "636/650 [============================>.] - ETA: 0s - loss: 0.0325 - mae: 0.0887 - r2_keras: 0.3710\n",
      "Epoch 00033: loss did not improve from 0.03209\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0325 - mae: 0.0887 - r2_keras: 0.3706 - val_loss: 0.0721 - val_mae: 0.1264 - val_r2_keras: -19460278.0000\n",
      "Epoch 34/500\n",
      "629/650 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0884 - r2_keras: 0.3742\n",
      "Epoch 00034: loss did not improve from 0.03209\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0323 - mae: 0.0885 - r2_keras: 0.3743 - val_loss: 0.0718 - val_mae: 0.1237 - val_r2_keras: -16869702.0000\n",
      "Epoch 35/500\n",
      "643/650 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0887 - r2_keras: 0.3730\n",
      "Epoch 00035: loss did not improve from 0.03209\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0324 - mae: 0.0888 - r2_keras: 0.3728 - val_loss: 0.0711 - val_mae: 0.1199 - val_r2_keras: -14659853.0000\n",
      "Epoch 36/500\n",
      "637/650 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0887 - r2_keras: 0.3763\n",
      "Epoch 00036: loss did not improve from 0.03209\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0323 - mae: 0.0886 - r2_keras: 0.3762 - val_loss: 0.0726 - val_mae: 0.1298 - val_r2_keras: -20741070.0000\n",
      "Epoch 37/500\n",
      "638/650 [============================>.] - ETA: 0s - loss: 0.0321 - mae: 0.0884 - r2_keras: 0.3757\n",
      "Epoch 00037: loss did not improve from 0.03209\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0322 - mae: 0.0886 - r2_keras: 0.3757 - val_loss: 0.0725 - val_mae: 0.1305 - val_r2_keras: -20714142.0000\n",
      "Epoch 38/500\n",
      "625/650 [===========================>..] - ETA: 0s - loss: 0.0319 - mae: 0.0881 - r2_keras: 0.3809\n",
      "Epoch 00038: loss improved from 0.03209 to 0.03196, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0320 - mae: 0.0881 - r2_keras: 0.3806 - val_loss: 0.0738 - val_mae: 0.1287 - val_r2_keras: -21859782.0000\n",
      "Epoch 39/500\n",
      "650/650 [==============================] - ETA: 0s - loss: 0.0319 - mae: 0.0875 - r2_keras: 0.3823\n",
      "Epoch 00039: loss improved from 0.03196 to 0.03190, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0319 - mae: 0.0875 - r2_keras: 0.3823 - val_loss: 0.0717 - val_mae: 0.1220 - val_r2_keras: -15682400.0000\n",
      "Epoch 40/500\n",
      "642/650 [============================>.] - ETA: 0s - loss: 0.0320 - mae: 0.0878 - r2_keras: 0.3810\n",
      "Epoch 00040: loss did not improve from 0.03190\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0320 - mae: 0.0878 - r2_keras: 0.3806 - val_loss: 0.0724 - val_mae: 0.1266 - val_r2_keras: -19928046.0000\n",
      "Epoch 41/500\n",
      "643/650 [============================>.] - ETA: 0s - loss: 0.0320 - mae: 0.0879 - r2_keras: 0.3807\n",
      "Epoch 00041: loss did not improve from 0.03190\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0320 - mae: 0.0878 - r2_keras: 0.3806 - val_loss: 0.0716 - val_mae: 0.1243 - val_r2_keras: -16875096.0000\n",
      "Epoch 42/500\n",
      "634/650 [============================>.] - ETA: 0s - loss: 0.0322 - mae: 0.0884 - r2_keras: 0.3761\n",
      "Epoch 00042: loss did not improve from 0.03190\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0322 - mae: 0.0885 - r2_keras: 0.3762 - val_loss: 0.0719 - val_mae: 0.1264 - val_r2_keras: -18480412.0000\n",
      "Epoch 43/500\n",
      "645/650 [============================>.] - ETA: 0s - loss: 0.0319 - mae: 0.0880 - r2_keras: 0.3809\n",
      "Epoch 00043: loss did not improve from 0.03190\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0319 - mae: 0.0881 - r2_keras: 0.3810 - val_loss: 0.0723 - val_mae: 0.1323 - val_r2_keras: -22142968.0000\n",
      "Epoch 44/500\n",
      "644/650 [============================>.] - ETA: 0s - loss: 0.0320 - mae: 0.0879 - r2_keras: 0.3810\n",
      "Epoch 00044: loss did not improve from 0.03190\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0320 - mae: 0.0879 - r2_keras: 0.3812 - val_loss: 0.0711 - val_mae: 0.1221 - val_r2_keras: -15836118.0000\n",
      "Epoch 45/500\n",
      "636/650 [============================>.] - ETA: 0s - loss: 0.0320 - mae: 0.0881 - r2_keras: 0.3818\n",
      "Epoch 00045: loss did not improve from 0.03190\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0319 - mae: 0.0880 - r2_keras: 0.3820 - val_loss: 0.0713 - val_mae: 0.1239 - val_r2_keras: -17684038.0000\n",
      "Epoch 46/500\n",
      "636/650 [============================>.] - ETA: 0s - loss: 0.0319 - mae: 0.0877 - r2_keras: 0.3825\n",
      "Epoch 00046: loss did not improve from 0.03190\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0319 - mae: 0.0878 - r2_keras: 0.3819 - val_loss: 0.0710 - val_mae: 0.1218 - val_r2_keras: -15884213.0000\n",
      "Epoch 47/500\n",
      "628/650 [===========================>..] - ETA: 0s - loss: 0.0319 - mae: 0.0879 - r2_keras: 0.3804\n",
      "Epoch 00047: loss did not improve from 0.03190\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0319 - mae: 0.0880 - r2_keras: 0.3810 - val_loss: 0.0721 - val_mae: 0.1280 - val_r2_keras: -19977630.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "628/650 [===========================>..] - ETA: 0s - loss: 0.0318 - mae: 0.0879 - r2_keras: 0.3844\n",
      "Epoch 00048: loss improved from 0.03190 to 0.03180, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0318 - mae: 0.0879 - r2_keras: 0.3838 - val_loss: 0.0718 - val_mae: 0.1276 - val_r2_keras: -19420602.0000\n",
      "Epoch 49/500\n",
      "643/650 [============================>.] - ETA: 0s - loss: 0.0319 - mae: 0.0880 - r2_keras: 0.3817\n",
      "Epoch 00049: loss did not improve from 0.03180\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0319 - mae: 0.0881 - r2_keras: 0.3815 - val_loss: 0.0722 - val_mae: 0.1298 - val_r2_keras: -20433634.0000\n",
      "Epoch 50/500\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0317 - mae: 0.0876 - r2_keras: 0.3866\n",
      "Epoch 00050: loss improved from 0.03180 to 0.03169, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0317 - mae: 0.0877 - r2_keras: 0.3866 - val_loss: 0.0715 - val_mae: 0.1230 - val_r2_keras: -16858400.0000\n",
      "Epoch 51/500\n",
      "646/650 [============================>.] - ETA: 0s - loss: 0.0318 - mae: 0.0880 - r2_keras: 0.3833\n",
      "Epoch 00051: loss did not improve from 0.03169\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0318 - mae: 0.0880 - r2_keras: 0.3831 - val_loss: 0.0709 - val_mae: 0.1214 - val_r2_keras: -15530293.0000\n",
      "Epoch 52/500\n",
      "631/650 [============================>.] - ETA: 0s - loss: 0.0318 - mae: 0.0877 - r2_keras: 0.3857\n",
      "Epoch 00052: loss did not improve from 0.03169\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0317 - mae: 0.0877 - r2_keras: 0.3853 - val_loss: 0.0712 - val_mae: 0.1248 - val_r2_keras: -17303326.0000\n",
      "Epoch 53/500\n",
      "637/650 [============================>.] - ETA: 0s - loss: 0.0316 - mae: 0.0871 - r2_keras: 0.3872\n",
      "Epoch 00053: loss improved from 0.03169 to 0.03162, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0316 - mae: 0.0872 - r2_keras: 0.3874 - val_loss: 0.0713 - val_mae: 0.1230 - val_r2_keras: -16765109.0000\n",
      "Epoch 54/500\n",
      "639/650 [============================>.] - ETA: 0s - loss: 0.0316 - mae: 0.0874 - r2_keras: 0.3893\n",
      "Epoch 00054: loss improved from 0.03162 to 0.03154, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0315 - mae: 0.0873 - r2_keras: 0.3886 - val_loss: 0.0712 - val_mae: 0.1222 - val_r2_keras: -15970255.0000\n",
      "Epoch 55/500\n",
      "635/650 [============================>.] - ETA: 0s - loss: 0.0317 - mae: 0.0874 - r2_keras: 0.3858\n",
      "Epoch 00055: loss did not improve from 0.03154\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0317 - mae: 0.0874 - r2_keras: 0.3861 - val_loss: 0.0723 - val_mae: 0.1286 - val_r2_keras: -21000980.0000\n",
      "Epoch 56/500\n",
      "639/650 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0870 - r2_keras: 0.3902\n",
      "Epoch 00056: loss improved from 0.03154 to 0.03151, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0315 - mae: 0.0870 - r2_keras: 0.3902 - val_loss: 0.0718 - val_mae: 0.1268 - val_r2_keras: -18380190.0000\n",
      "Epoch 57/500\n",
      "631/650 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0870 - r2_keras: 0.3898\n",
      "Epoch 00057: loss improved from 0.03151 to 0.03148, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0315 - mae: 0.0870 - r2_keras: 0.3895 - val_loss: 0.0725 - val_mae: 0.1273 - val_r2_keras: -20171002.0000\n",
      "Epoch 58/500\n",
      "625/650 [===========================>..] - ETA: 0s - loss: 0.0314 - mae: 0.0869 - r2_keras: 0.3915\n",
      "Epoch 00058: loss improved from 0.03148 to 0.03145, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0315 - mae: 0.0869 - r2_keras: 0.3912 - val_loss: 0.0728 - val_mae: 0.1292 - val_r2_keras: -20864596.0000\n",
      "Epoch 59/500\n",
      "633/650 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0868 - r2_keras: 0.3904\n",
      "Epoch 00059: loss did not improve from 0.03145\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0315 - mae: 0.0869 - r2_keras: 0.3900 - val_loss: 0.0717 - val_mae: 0.1225 - val_r2_keras: -16799078.0000\n",
      "Epoch 60/500\n",
      "633/650 [============================>.] - ETA: 0s - loss: 0.0316 - mae: 0.0871 - r2_keras: 0.3883\n",
      "Epoch 00060: loss did not improve from 0.03145\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0315 - mae: 0.0870 - r2_keras: 0.3884 - val_loss: 0.0718 - val_mae: 0.1241 - val_r2_keras: -17236966.0000\n",
      "Epoch 61/500\n",
      "638/650 [============================>.] - ETA: 0s - loss: 0.0314 - mae: 0.0863 - r2_keras: 0.3910\n",
      "Epoch 00061: loss improved from 0.03145 to 0.03143, saving model to best_atten_model.h5\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0314 - mae: 0.0863 - r2_keras: 0.3912 - val_loss: 0.0720 - val_mae: 0.1249 - val_r2_keras: -19120912.0000\n",
      "Epoch 62/500\n",
      "629/650 [============================>.] - ETA: 0s - loss: 0.0314 - mae: 0.0863 - r2_keras: 0.3902\n",
      "Epoch 00062: loss did not improve from 0.03143\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0315 - mae: 0.0863 - r2_keras: 0.3902 - val_loss: 0.0726 - val_mae: 0.1275 - val_r2_keras: -20689942.0000\n",
      "Epoch 63/500\n",
      "640/650 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0864 - r2_keras: 0.3897\n",
      "Epoch 00063: loss did not improve from 0.03143\n",
      "650/650 [==============================] - 2s 3ms/step - loss: 0.0315 - mae: 0.0864 - r2_keras: 0.3898 - val_loss: 0.0717 - val_mae: 0.1210 - val_r2_keras: -15893209.0000\n",
      "Epoch 64/500\n",
      "390/650 [=================>............] - ETA: 0s - loss: 0.0314 - mae: 0.0865 - r2_keras: 0.3912"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-fac4a8df4f36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history_atten = atten.fit(X_train[:,np.newaxis,:],y_train,batch_size= 128, epochs= 500,\n\u001b[0m\u001b[0;32m      2\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0matten_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m          )\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_atten = atten.fit(X_train[:,np.newaxis,:],y_train,batch_size= 128, epochs= 500,\n",
    "                  callbacks= atten_callbacks,\n",
    "                  validation_data= (X_val[:, np.newaxis, :], y_val)\n",
    "         )\n",
    "\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0728 - mae: 0.1285 - r2_keras: -21757084.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0727914348244667, 0.1284676343202591, -21757084.0]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten.evaluate(X_test[:,np.newaxis,:], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_96\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_152 (InputLayer)          [(None, 1, 6)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_781 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_782 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_786 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_787 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_657 (ReLU)                (None, 1, 6)         0           dense_781[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_658 (ReLU)                (None, 1, 6)         0           dense_782[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_661 (ReLU)                (None, 1, 6)         0           dense_786[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_662 (ReLU)                (None, 1, 6)         0           dense_787[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_791 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_792 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_180 ( [(None, 6, 6)]       0           re_lu_657[0][0]                  \n",
      "                                                                 re_lu_658[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_182 ( [(None, 6, 6)]       0           re_lu_661[0][0]                  \n",
      "                                                                 re_lu_662[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_665 (ReLU)                (None, 1, 6)         0           dense_791[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_666 (ReLU)                (None, 1, 6)         0           dense_792[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_796 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_797 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_783 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_124 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_180[0][\n",
      "__________________________________________________________________________________________________\n",
      "dense_788 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_125 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_182[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_184 ( [(None, 6, 6)]       0           re_lu_665[0][0]                  \n",
      "                                                                 re_lu_666[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_669 (ReLU)                (None, 1, 6)         0           dense_796[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_670 (ReLU)                (None, 1, 6)         0           dense_797[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_801 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_802 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_659 (ReLU)                (None, 1, 6)         0           dense_783[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_124 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_663 (ReLU)                (None, 1, 6)         0           dense_788[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_125 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_793 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_126 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_184[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_186 ( [(None, 6, 6)]       0           re_lu_669[0][0]                  \n",
      "                                                                 re_lu_670[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_673 (ReLU)                (None, 1, 6)         0           dense_801[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_674 (ReLU)                (None, 1, 6)         0           dense_802[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_806 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_807 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_181 ( [(None, 1, 6)]       0           re_lu_659[0][0]                  \n",
      "                                                                 softmax_124[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_183 ( [(None, 1, 6)]       0           re_lu_663[0][0]                  \n",
      "                                                                 softmax_125[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_667 (ReLU)                (None, 1, 6)         0           dense_793[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_126 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_798 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_127 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_186[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_188 ( [(None, 6, 6)]       0           re_lu_673[0][0]                  \n",
      "                                                                 re_lu_674[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_677 (ReLU)                (None, 1, 6)         0           dense_806[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_678 (ReLU)                (None, 1, 6)         0           dense_807[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_811 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_812 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_784 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_181[0][\n",
      "__________________________________________________________________________________________________\n",
      "dense_789 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_183[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_185 ( [(None, 1, 6)]       0           re_lu_667[0][0]                  \n",
      "                                                                 softmax_126[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_671 (ReLU)                (None, 1, 6)         0           dense_798[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_127 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_803 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_128 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_188[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_190 ( [(None, 6, 6)]       0           re_lu_677[0][0]                  \n",
      "                                                                 re_lu_678[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_681 (ReLU)                (None, 1, 6)         0           dense_811[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_682 (ReLU)                (None, 1, 6)         0           dense_812[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_816 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_817 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_660 (ReLU)                (None, 1, 6)         0           dense_784[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_664 (ReLU)                (None, 1, 6)         0           dense_789[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_794 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_185[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_187 ( [(None, 1, 6)]       0           re_lu_671[0][0]                  \n",
      "                                                                 softmax_127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_675 (ReLU)                (None, 1, 6)         0           dense_803[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_128 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_808 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_129 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_190[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_192 ( [(None, 6, 6)]       0           re_lu_681[0][0]                  \n",
      "                                                                 re_lu_682[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_685 (ReLU)                (None, 1, 6)         0           dense_816[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_686 (ReLU)                (None, 1, 6)         0           dense_817[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_821 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_822 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_785 (Dense)               (None, 1, 6)         42          re_lu_660[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_790 (Dense)               (None, 1, 6)         42          re_lu_664[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_668 (ReLU)                (None, 1, 6)         0           dense_794[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_799 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_187[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_189 ( [(None, 1, 6)]       0           re_lu_675[0][0]                  \n",
      "                                                                 softmax_128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_679 (ReLU)                (None, 1, 6)         0           dense_808[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_129 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_813 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_130 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_192[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_194 ( [(None, 6, 6)]       0           re_lu_685[0][0]                  \n",
      "                                                                 re_lu_686[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_689 (ReLU)                (None, 1, 6)         0           dense_821[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_690 (ReLU)                (None, 1, 6)         0           dense_822[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_826 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_827 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_112 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_785[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_113 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_790[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_795 (Dense)               (None, 1, 6)         42          re_lu_668[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_672 (ReLU)                (None, 1, 6)         0           dense_799[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_804 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_189[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_191 ( [(None, 1, 6)]       0           re_lu_679[0][0]                  \n",
      "                                                                 softmax_129[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_683 (ReLU)                (None, 1, 6)         0           dense_813[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_130 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_818 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_131 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_194[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_196 ( [(None, 6, 6)]       0           re_lu_689[0][0]                  \n",
      "                                                                 re_lu_690[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_693 (ReLU)                (None, 1, 6)         0           dense_826[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_694 (ReLU)                (None, 1, 6)         0           dense_827[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_831 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_832 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 1, 6)         24          add_112[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 1, 6)         24          add_113[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_114 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_795[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_800 (Dense)               (None, 1, 6)         42          re_lu_672[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_676 (ReLU)                (None, 1, 6)         0           dense_804[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_809 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_191[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_193 ( [(None, 1, 6)]       0           re_lu_683[0][0]                  \n",
      "                                                                 softmax_130[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_687 (ReLU)                (None, 1, 6)         0           dense_818[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_131 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_823 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_132 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_196[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_198 ( [(None, 6, 6)]       0           re_lu_693[0][0]                  \n",
      "                                                                 re_lu_694[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_697 (ReLU)                (None, 1, 6)         0           dense_831[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_698 (ReLU)                (None, 1, 6)         0           dense_832[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_836 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_837 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 1, 12)        0           batch_normalization_106[0][0]    \n",
      "                                                                 batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 1, 6)         24          add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_115 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_800[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_805 (Dense)               (None, 1, 6)         42          re_lu_676[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_680 (ReLU)                (None, 1, 6)         0           dense_809[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_814 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_193[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_195 ( [(None, 1, 6)]       0           re_lu_687[0][0]                  \n",
      "                                                                 softmax_131[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_691 (ReLU)                (None, 1, 6)         0           dense_823[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_132 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_828 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_133 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_198[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_200 ( [(None, 6, 6)]       0           re_lu_697[0][0]                  \n",
      "                                                                 re_lu_698[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_701 (ReLU)                (None, 1, 6)         0           dense_836[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_702 (ReLU)                (None, 1, 6)         0           dense_837[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 1, 18)        0           concatenate_83[0][0]             \n",
      "                                                                 batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 1, 6)         24          add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_116 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_805[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_810 (Dense)               (None, 1, 6)         42          re_lu_680[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_684 (ReLU)                (None, 1, 6)         0           dense_814[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_819 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_195[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_197 ( [(None, 1, 6)]       0           re_lu_691[0][0]                  \n",
      "                                                                 softmax_132[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_695 (ReLU)                (None, 1, 6)         0           dense_828[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_133 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_833 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_134 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_200[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_202 ( [(None, 6, 6)]       0           re_lu_701[0][0]                  \n",
      "                                                                 re_lu_702[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 1, 24)        0           concatenate_84[0][0]             \n",
      "                                                                 batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 1, 6)         24          add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_117 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_810[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_815 (Dense)               (None, 1, 6)         42          re_lu_684[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_688 (ReLU)                (None, 1, 6)         0           dense_819[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_824 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_197[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_199 ( [(None, 1, 6)]       0           re_lu_695[0][0]                  \n",
      "                                                                 softmax_133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_699 (ReLU)                (None, 1, 6)         0           dense_833[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_134 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_838 (Dense)               (None, 1, 6)         42          input_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_135 (Tensor [(None, 6, 6)]       0           tf_op_layer_BatchMatMulV2_202[0][\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 1, 30)        0           concatenate_85[0][0]             \n",
      "                                                                 batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 1, 6)         24          add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_118 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_815[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_820 (Dense)               (None, 1, 6)         42          re_lu_688[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_692 (ReLU)                (None, 1, 6)         0           dense_824[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_829 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_199[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_201 ( [(None, 1, 6)]       0           re_lu_699[0][0]                  \n",
      "                                                                 softmax_134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_703 (ReLU)                (None, 1, 6)         0           dense_838[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_135 (Softmax)           (None, 6, 6)         0           tf_op_layer_RealDiv_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 1, 36)        0           concatenate_86[0][0]             \n",
      "                                                                 batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 1, 6)         24          add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_119 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_820[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_825 (Dense)               (None, 1, 6)         42          re_lu_692[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_696 (ReLU)                (None, 1, 6)         0           dense_829[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_834 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_201[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_203 ( [(None, 1, 6)]       0           re_lu_703[0][0]                  \n",
      "                                                                 softmax_135[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 1, 42)        0           concatenate_87[0][0]             \n",
      "                                                                 batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 1, 6)         24          add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_120 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_825[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_830 (Dense)               (None, 1, 6)         42          re_lu_696[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_700 (ReLU)                (None, 1, 6)         0           dense_834[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_839 (Dense)               (None, 1, 6)         42          tf_op_layer_BatchMatMulV2_203[0][\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 1, 48)        0           concatenate_88[0][0]             \n",
      "                                                                 batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 1, 6)         24          add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_121 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_830[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_835 (Dense)               (None, 1, 6)         42          re_lu_700[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_704 (ReLU)                (None, 1, 6)         0           dense_839[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_90 (Concatenate)    (None, 1, 54)        0           concatenate_89[0][0]             \n",
      "                                                                 batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 1, 6)         24          add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_122 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_835[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_840 (Dense)               (None, 1, 6)         42          re_lu_704[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_91 (Concatenate)    (None, 1, 60)        0           concatenate_90[0][0]             \n",
      "                                                                 batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 1, 6)         24          add_122[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_123 (Add)                   (None, 1, 6)         0           input_152[0][0]                  \n",
      "                                                                 dense_840[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 1, 66)        0           concatenate_91[0][0]             \n",
      "                                                                 batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 1, 6)         24          add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 1, 72)        0           concatenate_92[0][0]             \n",
      "                                                                 batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 72)           0           concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_841 (Dense)               (None, 50)           3650        flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_705 (ReLU)                (None, 50)           0           dense_841[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_842 (Dense)               (None, 50)           2550        re_lu_705[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_706 (ReLU)                (None, 50)           0           dense_842[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_843 (Dense)               (None, 2)            102         re_lu_706[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,110\n",
      "Trainable params: 8,966\n",
      "Non-trainable params: 144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "atten_2 = make_attenModel(n_heads=12)\n",
    "atten_2.compile(optimizer = SGD(), loss = 'mse', metrics= ['mae', r2_keras])\n",
    "atten_2_callbacks = [\n",
    "    ModelCheckpoint('best_atten2_model.h5', monitor='loss', verbose=1, save_best_only= True),\n",
    "    ReduceLROnPlateau(patience= 10, monitor = 'loss'),\n",
    "    History()\n",
    "    \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 2.1303 - mae: 0.8744 - r2_keras: 0.1097\n",
      "Epoch 00001: loss improved from inf to 2.13077, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 4s 5ms/step - loss: 2.1308 - mae: 0.8746 - r2_keras: 0.1099\n",
      "Epoch 2/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 1.9145 - mae: 0.8422 - r2_keras: 0.1984\n",
      "Epoch 00002: loss improved from 2.13077 to 1.91385, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 4s 5ms/step - loss: 1.9139 - mae: 0.8421 - r2_keras: 0.1987\n",
      "Epoch 3/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 1.7866 - mae: 0.8188 - r2_keras: 0.2506\n",
      "Epoch 00003: loss improved from 1.91385 to 1.78572, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 1.7857 - mae: 0.8186 - r2_keras: 0.2507\n",
      "Epoch 4/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 1.6999 - mae: 0.8042 - r2_keras: 0.2862\n",
      "Epoch 00004: loss improved from 1.78572 to 1.70054, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 1.7005 - mae: 0.8043 - r2_keras: 0.2861\n",
      "Epoch 5/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.6122 - mae: 0.7849 - r2_keras: 0.3223\n",
      "Epoch 00005: loss improved from 1.70054 to 1.61225, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 1.6123 - mae: 0.7850 - r2_keras: 0.3223\n",
      "Epoch 6/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 1.5345 - mae: 0.7605 - r2_keras: 0.3529\n",
      "Epoch 00006: loss improved from 1.61225 to 1.53569, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 1.5357 - mae: 0.7608 - r2_keras: 0.3530\n",
      "Epoch 7/500\n",
      "803/813 [============================>.] - ETA: 0s - loss: 1.4916 - mae: 0.7454 - r2_keras: 0.3714\n",
      "Epoch 00007: loss improved from 1.53569 to 1.49067, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 1.4907 - mae: 0.7451 - r2_keras: 0.3718\n",
      "Epoch 8/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 1.4556 - mae: 0.7303 - r2_keras: 0.3863 E\n",
      "Epoch 00008: loss improved from 1.49067 to 1.45524, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 1.4552 - mae: 0.7303 - r2_keras: 0.3863\n",
      "Epoch 9/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 1.4156 - mae: 0.7152 - r2_keras: 0.4034\n",
      "Epoch 00009: loss improved from 1.45524 to 1.41526, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 1.4153 - mae: 0.7151 - r2_keras: 0.4034\n",
      "Epoch 10/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.3823 - mae: 0.7018 - r2_keras: 0.4178\n",
      "Epoch 00010: loss improved from 1.41526 to 1.38224, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.3822 - mae: 0.7018 - r2_keras: 0.4177\n",
      "Epoch 11/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 1.3643 - mae: 0.6932 - r2_keras: 0.4254\n",
      "Epoch 00011: loss improved from 1.38224 to 1.36415, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.3642 - mae: 0.6931 - r2_keras: 0.4252\n",
      "Epoch 12/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.3372 - mae: 0.6835 - r2_keras: 0.4362\n",
      "Epoch 00012: loss improved from 1.36415 to 1.33767, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.3377 - mae: 0.6838 - r2_keras: 0.4363\n",
      "Epoch 13/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 1.3357 - mae: 0.6845 - r2_keras: 0.4379\n",
      "Epoch 00013: loss improved from 1.33767 to 1.33539, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.3354 - mae: 0.6844 - r2_keras: 0.4378\n",
      "Epoch 14/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 1.3058 - mae: 0.6742 - r2_keras: 0.4494\n",
      "Epoch 00014: loss improved from 1.33539 to 1.30561, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.3056 - mae: 0.6741 - r2_keras: 0.4495\n",
      "Epoch 15/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 1.2922 - mae: 0.6696 - r2_keras: 0.4551\n",
      "Epoch 00015: loss improved from 1.30561 to 1.29218, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2922 - mae: 0.6696 - r2_keras: 0.4549\n",
      "Epoch 16/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 1.2817 - mae: 0.6659 - r2_keras: 0.4585\n",
      "Epoch 00016: loss improved from 1.29218 to 1.28176, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2818 - mae: 0.6659 - r2_keras: 0.4585\n",
      "Epoch 17/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 1.2609 - mae: 0.6591 - r2_keras: 0.4667\n",
      "Epoch 00017: loss improved from 1.28176 to 1.26102, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2610 - mae: 0.6591 - r2_keras: 0.4670\n",
      "Epoch 18/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 1.2479 - mae: 0.6536 - r2_keras: 0.4734\n",
      "Epoch 00018: loss improved from 1.26102 to 1.24845, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2484 - mae: 0.6536 - r2_keras: 0.4733\n",
      "Epoch 19/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 1.2408 - mae: 0.6508 - r2_keras: 0.4765\n",
      "Epoch 00019: loss improved from 1.24845 to 1.24075, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2407 - mae: 0.6507 - r2_keras: 0.4765\n",
      "Epoch 20/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 1.2370 - mae: 0.6498 - r2_keras: 0.4767\n",
      "Epoch 00020: loss improved from 1.24075 to 1.23750, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2375 - mae: 0.6500 - r2_keras: 0.4772\n",
      "Epoch 21/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 1.2352 - mae: 0.6472 - r2_keras: 0.4789\n",
      "Epoch 00021: loss improved from 1.23750 to 1.23488, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2349 - mae: 0.6471 - r2_keras: 0.4789\n",
      "Epoch 22/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.2146 - mae: 0.6387 - r2_keras: 0.4869\n",
      "Epoch 00022: loss improved from 1.23488 to 1.21514, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2151 - mae: 0.6388 - r2_keras: 0.4866\n",
      "Epoch 23/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 1.2058 - mae: 0.6361 - r2_keras: 0.4898\n",
      "Epoch 00023: loss improved from 1.21514 to 1.20632, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2063 - mae: 0.6361 - r2_keras: 0.4896\n",
      "Epoch 24/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 1.2030 - mae: 0.6351 - r2_keras: 0.4918\n",
      "Epoch 00024: loss improved from 1.20632 to 1.20317, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2032 - mae: 0.6353 - r2_keras: 0.4918\n",
      "Epoch 25/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.2012 - mae: 0.6352 - r2_keras: 0.4926\n",
      "Epoch 00025: loss improved from 1.20317 to 1.20129, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2013 - mae: 0.6352 - r2_keras: 0.4924\n",
      "Epoch 26/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 1.1918 - mae: 0.6318 - r2_keras: 0.4967\n",
      "Epoch 00026: loss improved from 1.20129 to 1.19121, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1912 - mae: 0.6316 - r2_keras: 0.4969\n",
      "Epoch 27/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 1.1767 - mae: 0.6247 - r2_keras: 0.5031\n",
      "Epoch 00027: loss improved from 1.19121 to 1.17727, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1773 - mae: 0.6249 - r2_keras: 0.5029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.1634 - mae: 0.6206 - r2_keras: 0.5087\n",
      "Epoch 00028: loss improved from 1.17727 to 1.16359, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1636 - mae: 0.6207 - r2_keras: 0.5087\n",
      "Epoch 29/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 1.1578 - mae: 0.6169 - r2_keras: 0.5106\n",
      "Epoch 00029: loss improved from 1.16359 to 1.15776, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1578 - mae: 0.6169 - r2_keras: 0.5106\n",
      "Epoch 30/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 1.1489 - mae: 0.6136 - r2_keras: 0.5138\n",
      "Epoch 00030: loss improved from 1.15776 to 1.14949, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1495 - mae: 0.6137 - r2_keras: 0.5142\n",
      "Epoch 31/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 1.1395 - mae: 0.6107 - r2_keras: 0.5183\n",
      "Epoch 00031: loss improved from 1.14949 to 1.14150, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1415 - mae: 0.6113 - r2_keras: 0.5179\n",
      "Epoch 32/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 1.1466 - mae: 0.6131 - r2_keras: 0.5164\n",
      "Epoch 00032: loss did not improve from 1.14150\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.1462 - mae: 0.6131 - r2_keras: 0.5163\n",
      "Epoch 33/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 1.1309 - mae: 0.6079 - r2_keras: 0.5212\n",
      "Epoch 00033: loss improved from 1.14150 to 1.13184, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1318 - mae: 0.6080 - r2_keras: 0.5210\n",
      "Epoch 34/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 1.1260 - mae: 0.6048 - r2_keras: 0.5235\n",
      "Epoch 00034: loss improved from 1.13184 to 1.12719, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1272 - mae: 0.6050 - r2_keras: 0.5232\n",
      "Epoch 35/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 1.1197 - mae: 0.6025 - r2_keras: 0.5273\n",
      "Epoch 00035: loss improved from 1.12719 to 1.11968, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1197 - mae: 0.6025 - r2_keras: 0.5273\n",
      "Epoch 36/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.1200 - mae: 0.6031 - r2_keras: 0.5275\n",
      "Epoch 00036: loss did not improve from 1.11968\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.1200 - mae: 0.6031 - r2_keras: 0.5273\n",
      "Epoch 37/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 1.1112 - mae: 0.6002 - r2_keras: 0.5319\n",
      "Epoch 00037: loss improved from 1.11968 to 1.11117, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1112 - mae: 0.6002 - r2_keras: 0.5319\n",
      "Epoch 38/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 1.1102 - mae: 0.6009 - r2_keras: 0.5305\n",
      "Epoch 00038: loss improved from 1.11117 to 1.11001, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1100 - mae: 0.6007 - r2_keras: 0.5304\n",
      "Epoch 39/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 1.1026 - mae: 0.5968 - r2_keras: 0.5334\n",
      "Epoch 00039: loss improved from 1.11001 to 1.10276, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1028 - mae: 0.5968 - r2_keras: 0.5332\n",
      "Epoch 40/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 1.1055 - mae: 0.5971 - r2_keras: 0.5326\n",
      "Epoch 00040: loss did not improve from 1.10276\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.1061 - mae: 0.5972 - r2_keras: 0.5326\n",
      "Epoch 41/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 1.0978 - mae: 0.5936 - r2_keras: 0.5360\n",
      "Epoch 00041: loss improved from 1.10276 to 1.09748, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0975 - mae: 0.5936 - r2_keras: 0.5361\n",
      "Epoch 42/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 1.0841 - mae: 0.5884 - r2_keras: 0.5414\n",
      "Epoch 00042: loss improved from 1.09748 to 1.08491, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 1.0849 - mae: 0.5887 - r2_keras: 0.5414\n",
      "Epoch 43/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 1.0861 - mae: 0.5873 - r2_keras: 0.5405\n",
      "Epoch 00043: loss did not improve from 1.08491\n",
      "813/813 [==============================] - 7s 8ms/step - loss: 1.0860 - mae: 0.5873 - r2_keras: 0.5402\n",
      "Epoch 44/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 1.0829 - mae: 0.5854 - r2_keras: 0.5426\n",
      "Epoch 00044: loss improved from 1.08491 to 1.08194, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 1.0819 - mae: 0.5851 - r2_keras: 0.5428\n",
      "Epoch 45/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 1.0731 - mae: 0.5823 - r2_keras: 0.5473\n",
      "Epoch 00045: loss improved from 1.08194 to 1.07339, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0734 - mae: 0.5824 - r2_keras: 0.5472\n",
      "Epoch 46/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 1.0703 - mae: 0.5798 - r2_keras: 0.5482\n",
      "Epoch 00046: loss improved from 1.07339 to 1.06999, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 1.0700 - mae: 0.5798 - r2_keras: 0.5483\n",
      "Epoch 47/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 1.0660 - mae: 0.5779 - r2_keras: 0.5484 ETA: 0s - loss: 1.0670 - mae: 0.5\n",
      "Epoch 00047: loss improved from 1.06999 to 1.06566, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0657 - mae: 0.5778 - r2_keras: 0.5481\n",
      "Epoch 48/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 1.0677 - mae: 0.5789 - r2_keras: 0.5486\n",
      "Epoch 00048: loss did not improve from 1.06566\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.0670 - mae: 0.5787 - r2_keras: 0.5489\n",
      "Epoch 49/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 1.0633 - mae: 0.5772 - r2_keras: 0.5508\n",
      "Epoch 00049: loss improved from 1.06566 to 1.06360, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0636 - mae: 0.5773 - r2_keras: 0.5507\n",
      "Epoch 50/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.0626 - mae: 0.5761 - r2_keras: 0.5506\n",
      "Epoch 00050: loss improved from 1.06360 to 1.06213, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0621 - mae: 0.5761 - r2_keras: 0.5507\n",
      "Epoch 51/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 1.0589 - mae: 0.5758 - r2_keras: 0.5512\n",
      "Epoch 00051: loss improved from 1.06213 to 1.05963, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 7s 8ms/step - loss: 1.0596 - mae: 0.5761 - r2_keras: 0.5512\n",
      "Epoch 52/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.0497 - mae: 0.5726 - r2_keras: 0.5578\n",
      "Epoch 00052: loss improved from 1.05963 to 1.04948, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 9s 11ms/step - loss: 1.0495 - mae: 0.5725 - r2_keras: 0.5579\n",
      "Epoch 53/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.0368 - mae: 0.5680 - r2_keras: 0.5611\n",
      "Epoch 00053: loss improved from 1.04948 to 1.03685, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 7s 9ms/step - loss: 1.0369 - mae: 0.5680 - r2_keras: 0.5608\n",
      "Epoch 54/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 1.0636 - mae: 0.5771 - r2_keras: 0.5499\n",
      "Epoch 00054: loss did not improve from 1.03685\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 1.0637 - mae: 0.5771 - r2_keras: 0.5500\n",
      "Epoch 55/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 1.0499 - mae: 0.5731 - r2_keras: 0.5559 ETA: 1s - loss: 1.0\n",
      "Epoch 00055: loss did not improve from 1.03685\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 1.0503 - mae: 0.5732 - r2_keras: 0.5559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 1.0483 - mae: 0.5727 - r2_keras: 0.5565\n",
      "Epoch 00056: loss did not improve from 1.03685\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.0485 - mae: 0.5727 - r2_keras: 0.5564\n",
      "Epoch 57/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 1.0346 - mae: 0.5665 - r2_keras: 0.5622\n",
      "Epoch 00057: loss improved from 1.03685 to 1.03455, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0345 - mae: 0.5665 - r2_keras: 0.5624\n",
      "Epoch 58/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.0291 - mae: 0.5648 - r2_keras: 0.5662\n",
      "Epoch 00058: loss improved from 1.03455 to 1.02918, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0292 - mae: 0.5649 - r2_keras: 0.5661\n",
      "Epoch 59/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 1.0327 - mae: 0.5667 - r2_keras: 0.5650\n",
      "Epoch 00059: loss did not improve from 1.02918\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.0315 - mae: 0.5664 - r2_keras: 0.5650\n",
      "Epoch 60/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 1.0321 - mae: 0.5671 - r2_keras: 0.5640\n",
      "Epoch 00060: loss did not improve from 1.02918\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.0326 - mae: 0.5672 - r2_keras: 0.5637\n",
      "Epoch 61/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.0299 - mae: 0.5653 - r2_keras: 0.5639\n",
      "Epoch 00061: loss did not improve from 1.02918\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0306 - mae: 0.5654 - r2_keras: 0.5638\n",
      "Epoch 62/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 1.0244 - mae: 0.5644 - r2_keras: 0.5660\n",
      "Epoch 00062: loss improved from 1.02918 to 1.02414, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0241 - mae: 0.5643 - r2_keras: 0.5663\n",
      "Epoch 63/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 1.0160 - mae: 0.5617 - r2_keras: 0.5705\n",
      "Epoch 00063: loss improved from 1.02414 to 1.01590, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0159 - mae: 0.5616 - r2_keras: 0.5703\n",
      "Epoch 64/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 1.0183 - mae: 0.5606 - r2_keras: 0.5690\n",
      "Epoch 00064: loss did not improve from 1.01590\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0183 - mae: 0.5606 - r2_keras: 0.5690\n",
      "Epoch 65/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 1.0114 - mae: 0.5606 - r2_keras: 0.5722\n",
      "Epoch 00065: loss improved from 1.01590 to 1.01198, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0120 - mae: 0.5608 - r2_keras: 0.5716\n",
      "Epoch 66/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.0073 - mae: 0.5580 - r2_keras: 0.5740\n",
      "Epoch 00066: loss improved from 1.01198 to 1.00736, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0074 - mae: 0.5580 - r2_keras: 0.5742\n",
      "Epoch 67/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 1.0037 - mae: 0.5556 - r2_keras: 0.5758\n",
      "Epoch 00067: loss improved from 1.00736 to 1.00400, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.0040 - mae: 0.5556 - r2_keras: 0.5757\n",
      "Epoch 68/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 1.0092 - mae: 0.5560 - r2_keras: 0.5735\n",
      "Epoch 00068: loss did not improve from 1.00400\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.0092 - mae: 0.5560 - r2_keras: 0.5735\n",
      "Epoch 69/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 1.0130 - mae: 0.5562 - r2_keras: 0.5711\n",
      "Epoch 00069: loss did not improve from 1.00400\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.0122 - mae: 0.5560 - r2_keras: 0.5715\n",
      "Epoch 70/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 1.0158 - mae: 0.5585 - r2_keras: 0.5707 E\n",
      "Epoch 00070: loss did not improve from 1.00400\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.0158 - mae: 0.5585 - r2_keras: 0.5709\n",
      "Epoch 71/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 1.0077 - mae: 0.5571 - r2_keras: 0.5738\n",
      "Epoch 00071: loss did not improve from 1.00400\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.0076 - mae: 0.5571 - r2_keras: 0.5735\n",
      "Epoch 72/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 1.0035 - mae: 0.5538 - r2_keras: 0.5754\n",
      "Epoch 00072: loss did not improve from 1.00400\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 1.0049 - mae: 0.5542 - r2_keras: 0.5747\n",
      "Epoch 73/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9951 - mae: 0.5529 - r2_keras: 0.5784\n",
      "Epoch 00073: loss improved from 1.00400 to 0.99604, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9960 - mae: 0.5531 - r2_keras: 0.5782\n",
      "Epoch 74/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.9890 - mae: 0.5494 - r2_keras: 0.5833\n",
      "Epoch 00074: loss improved from 0.99604 to 0.98930, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9893 - mae: 0.5496 - r2_keras: 0.5831\n",
      "Epoch 75/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9856 - mae: 0.5469 - r2_keras: 0.5834\n",
      "Epoch 00075: loss improved from 0.98930 to 0.98577, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9858 - mae: 0.5470 - r2_keras: 0.5834\n",
      "Epoch 76/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.9798 - mae: 0.5446 - r2_keras: 0.5849 ETA: 0s - loss: 0.9827 - mae: 0.545\n",
      "Epoch 00076: loss improved from 0.98577 to 0.97998, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9800 - mae: 0.5447 - r2_keras: 0.5851\n",
      "Epoch 77/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.9802 - mae: 0.5454 - r2_keras: 0.5845\n",
      "Epoch 00077: loss improved from 0.97998 to 0.97983, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9798 - mae: 0.5454 - r2_keras: 0.5850\n",
      "Epoch 78/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9803 - mae: 0.5450 - r2_keras: 0.5851\n",
      "Epoch 00078: loss did not improve from 0.97983\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9807 - mae: 0.5450 - r2_keras: 0.5848\n",
      "Epoch 79/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9837 - mae: 0.5456 - r2_keras: 0.5843\n",
      "Epoch 00079: loss did not improve from 0.97983\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9831 - mae: 0.5455 - r2_keras: 0.5842\n",
      "Epoch 80/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.9771 - mae: 0.5441 - r2_keras: 0.5869\n",
      "Epoch 00080: loss improved from 0.97983 to 0.97711, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9771 - mae: 0.5441 - r2_keras: 0.5869\n",
      "Epoch 81/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.9702 - mae: 0.5410 - r2_keras: 0.5899\n",
      "Epoch 00081: loss improved from 0.97711 to 0.97051, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9705 - mae: 0.5410 - r2_keras: 0.5898\n",
      "Epoch 82/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.9712 - mae: 0.5419 - r2_keras: 0.5887\n",
      "Epoch 00082: loss did not improve from 0.97051\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9711 - mae: 0.5419 - r2_keras: 0.5888\n",
      "Epoch 83/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.9702 - mae: 0.5401 - r2_keras: 0.5892\n",
      "Epoch 00083: loss improved from 0.97051 to 0.96970, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9697 - mae: 0.5400 - r2_keras: 0.5895\n",
      "Epoch 84/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.9756 - mae: 0.5416 - r2_keras: 0.5880\n",
      "Epoch 00084: loss did not improve from 0.96970\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9755 - mae: 0.5416 - r2_keras: 0.5882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9686 - mae: 0.5392 - r2_keras: 0.5907\n",
      "Epoch 00085: loss improved from 0.96970 to 0.96839, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9684 - mae: 0.5392 - r2_keras: 0.5907\n",
      "Epoch 86/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.9677 - mae: 0.5397 - r2_keras: 0.5916\n",
      "Epoch 00086: loss improved from 0.96839 to 0.96728, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9673 - mae: 0.5397 - r2_keras: 0.5917\n",
      "Epoch 87/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.9658 - mae: 0.5392 - r2_keras: 0.5914\n",
      "Epoch 00087: loss improved from 0.96728 to 0.96577, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9658 - mae: 0.5392 - r2_keras: 0.5914\n",
      "Epoch 88/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9636 - mae: 0.5369 - r2_keras: 0.5928\n",
      "Epoch 00088: loss improved from 0.96577 to 0.96432, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9643 - mae: 0.5371 - r2_keras: 0.5927\n",
      "Epoch 89/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9640 - mae: 0.5381 - r2_keras: 0.5913\n",
      "Epoch 00089: loss did not improve from 0.96432\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9644 - mae: 0.5382 - r2_keras: 0.5909\n",
      "Epoch 90/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9682 - mae: 0.5394 - r2_keras: 0.5902\n",
      "Epoch 00090: loss did not improve from 0.96432\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9681 - mae: 0.5394 - r2_keras: 0.5904\n",
      "Epoch 91/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.9596 - mae: 0.5362 - r2_keras: 0.5936\n",
      "Epoch 00091: loss improved from 0.96432 to 0.95960, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9596 - mae: 0.5362 - r2_keras: 0.5936\n",
      "Epoch 92/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9535 - mae: 0.5341 - r2_keras: 0.5966\n",
      "Epoch 00092: loss improved from 0.95960 to 0.95371, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9537 - mae: 0.5342 - r2_keras: 0.5965\n",
      "Epoch 93/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9587 - mae: 0.5350 - r2_keras: 0.5953\n",
      "Epoch 00093: loss did not improve from 0.95371\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9589 - mae: 0.5350 - r2_keras: 0.5954\n",
      "Epoch 94/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.9517 - mae: 0.5333 - r2_keras: 0.5970\n",
      "Epoch 00094: loss improved from 0.95371 to 0.95043, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9504 - mae: 0.5329 - r2_keras: 0.5975\n",
      "Epoch 95/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.9530 - mae: 0.5330 - r2_keras: 0.5973\n",
      "Epoch 00095: loss did not improve from 0.95043\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9531 - mae: 0.5330 - r2_keras: 0.5972\n",
      "Epoch 96/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.9540 - mae: 0.5318 - r2_keras: 0.5961\n",
      "Epoch 00096: loss did not improve from 0.95043\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9531 - mae: 0.5317 - r2_keras: 0.5963\n",
      "Epoch 97/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.9460 - mae: 0.5306 - r2_keras: 0.6002\n",
      "Epoch 00097: loss improved from 0.95043 to 0.94592, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9459 - mae: 0.5306 - r2_keras: 0.6003\n",
      "Epoch 98/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9442 - mae: 0.5307 - r2_keras: 0.6001\n",
      "Epoch 00098: loss improved from 0.94592 to 0.94461, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9446 - mae: 0.5308 - r2_keras: 0.5999\n",
      "Epoch 99/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.9466 - mae: 0.5303 - r2_keras: 0.5992\n",
      "Epoch 00099: loss did not improve from 0.94461\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9470 - mae: 0.5304 - r2_keras: 0.5992\n",
      "Epoch 100/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.9479 - mae: 0.5302 - r2_keras: 0.5974\n",
      "Epoch 00100: loss did not improve from 0.94461\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9479 - mae: 0.5302 - r2_keras: 0.5974\n",
      "Epoch 101/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.9389 - mae: 0.5265 - r2_keras: 0.6036\n",
      "Epoch 00101: loss improved from 0.94461 to 0.93910, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.9391 - mae: 0.5265 - r2_keras: 0.6030\n",
      "Epoch 102/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.9357 - mae: 0.5246 - r2_keras: 0.6038\n",
      "Epoch 00102: loss improved from 0.93910 to 0.93595, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9359 - mae: 0.5247 - r2_keras: 0.6035\n",
      "Epoch 103/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.9370 - mae: 0.5239 - r2_keras: 0.6023\n",
      "Epoch 00103: loss did not improve from 0.93595\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9369 - mae: 0.5240 - r2_keras: 0.6024\n",
      "Epoch 104/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.9367 - mae: 0.5260 - r2_keras: 0.6037\n",
      "Epoch 00104: loss did not improve from 0.93595\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9362 - mae: 0.5259 - r2_keras: 0.6037\n",
      "Epoch 105/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9330 - mae: 0.5245 - r2_keras: 0.6059\n",
      "Epoch 00105: loss improved from 0.93595 to 0.93288, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9329 - mae: 0.5244 - r2_keras: 0.6057\n",
      "Epoch 106/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.9245 - mae: 0.5208 - r2_keras: 0.6088\n",
      "Epoch 00106: loss improved from 0.93288 to 0.92478, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9248 - mae: 0.5208 - r2_keras: 0.6085\n",
      "Epoch 107/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9330 - mae: 0.5230 - r2_keras: 0.6047\n",
      "Epoch 00107: loss did not improve from 0.92478\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9332 - mae: 0.5230 - r2_keras: 0.6046\n",
      "Epoch 108/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.9339 - mae: 0.5238 - r2_keras: 0.6061\n",
      "Epoch 00108: loss did not improve from 0.92478\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9339 - mae: 0.5238 - r2_keras: 0.6062\n",
      "Epoch 109/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.9267 - mae: 0.5213 - r2_keras: 0.6082\n",
      "Epoch 00109: loss did not improve from 0.92478\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9267 - mae: 0.5213 - r2_keras: 0.6082\n",
      "Epoch 110/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.9192 - mae: 0.5196 - r2_keras: 0.6111\n",
      "Epoch 00110: loss improved from 0.92478 to 0.91901, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9190 - mae: 0.5196 - r2_keras: 0.6112\n",
      "Epoch 111/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9258 - mae: 0.5200 - r2_keras: 0.6090\n",
      "Epoch 00111: loss did not improve from 0.91901\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9256 - mae: 0.5199 - r2_keras: 0.6089\n",
      "Epoch 112/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9188 - mae: 0.5179 - r2_keras: 0.6105 ETA: \n",
      "Epoch 00112: loss improved from 0.91901 to 0.91886, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9189 - mae: 0.5179 - r2_keras: 0.6106\n",
      "Epoch 113/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9276 - mae: 0.5194 - r2_keras: 0.6068\n",
      "Epoch 00113: loss did not improve from 0.91886\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9272 - mae: 0.5192 - r2_keras: 0.6069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.9281 - mae: 0.5207 - r2_keras: 0.6069\n",
      "Epoch 00114: loss did not improve from 0.91886\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9280 - mae: 0.5207 - r2_keras: 0.6071\n",
      "Epoch 115/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9288 - mae: 0.5209 - r2_keras: 0.6069\n",
      "Epoch 00115: loss did not improve from 0.91886\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9284 - mae: 0.5209 - r2_keras: 0.6070\n",
      "Epoch 116/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.9247 - mae: 0.5195 - r2_keras: 0.6088\n",
      "Epoch 00116: loss did not improve from 0.91886\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9243 - mae: 0.5194 - r2_keras: 0.6087\n",
      "Epoch 117/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.9251 - mae: 0.5190 - r2_keras: 0.6085\n",
      "Epoch 00117: loss did not improve from 0.91886\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9253 - mae: 0.5191 - r2_keras: 0.6081\n",
      "Epoch 118/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.9234 - mae: 0.5199 - r2_keras: 0.6092\n",
      "Epoch 00118: loss did not improve from 0.91886\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9235 - mae: 0.5199 - r2_keras: 0.6093\n",
      "Epoch 119/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.9240 - mae: 0.5190 - r2_keras: 0.6094\n",
      "Epoch 00119: loss did not improve from 0.91886\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9230 - mae: 0.5188 - r2_keras: 0.6095\n",
      "Epoch 120/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.9158 - mae: 0.5179 - r2_keras: 0.6121\n",
      "Epoch 00120: loss improved from 0.91886 to 0.91623, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9162 - mae: 0.5180 - r2_keras: 0.6121\n",
      "Epoch 121/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.9185 - mae: 0.5171 - r2_keras: 0.6118\n",
      "Epoch 00121: loss did not improve from 0.91623\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9188 - mae: 0.5172 - r2_keras: 0.6116\n",
      "Epoch 122/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.9207 - mae: 0.5176 - r2_keras: 0.6102\n",
      "Epoch 00122: loss did not improve from 0.91623\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9207 - mae: 0.5176 - r2_keras: 0.6102\n",
      "Epoch 123/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.9215 - mae: 0.5182 - r2_keras: 0.6095\n",
      "Epoch 00123: loss did not improve from 0.91623\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9215 - mae: 0.5182 - r2_keras: 0.6095\n",
      "Epoch 124/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.9167 - mae: 0.5171 - r2_keras: 0.6119\n",
      "Epoch 00124: loss did not improve from 0.91623\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9167 - mae: 0.5170 - r2_keras: 0.6115\n",
      "Epoch 125/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9115 - mae: 0.5139 - r2_keras: 0.6150\n",
      "Epoch 00125: loss improved from 0.91623 to 0.91092, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9109 - mae: 0.5136 - r2_keras: 0.6150\n",
      "Epoch 126/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.9149 - mae: 0.5141 - r2_keras: 0.6130\n",
      "Epoch 00126: loss did not improve from 0.91092\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9152 - mae: 0.5142 - r2_keras: 0.6129\n",
      "Epoch 127/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.9139 - mae: 0.5140 - r2_keras: 0.6129\n",
      "Epoch 00127: loss did not improve from 0.91092\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9141 - mae: 0.5140 - r2_keras: 0.6130\n",
      "Epoch 128/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.9092 - mae: 0.5115 - r2_keras: 0.6157\n",
      "Epoch 00128: loss improved from 0.91092 to 0.90921, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9092 - mae: 0.5115 - r2_keras: 0.6157\n",
      "Epoch 129/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.9067 - mae: 0.5118 - r2_keras: 0.6154\n",
      "Epoch 00129: loss improved from 0.90921 to 0.90724, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9072 - mae: 0.5119 - r2_keras: 0.6159\n",
      "Epoch 130/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.9088 - mae: 0.5125 - r2_keras: 0.6156\n",
      "Epoch 00130: loss did not improve from 0.90724\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9094 - mae: 0.5127 - r2_keras: 0.6156\n",
      "Epoch 131/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.9034 - mae: 0.5108 - r2_keras: 0.6175\n",
      "Epoch 00131: loss improved from 0.90724 to 0.90342, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9034 - mae: 0.5108 - r2_keras: 0.6175\n",
      "Epoch 132/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.9104 - mae: 0.5123 - r2_keras: 0.6152\n",
      "Epoch 00132: loss did not improve from 0.90342\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9093 - mae: 0.5120 - r2_keras: 0.6157\n",
      "Epoch 133/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9102 - mae: 0.5114 - r2_keras: 0.6151\n",
      "Epoch 00133: loss did not improve from 0.90342\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9109 - mae: 0.5115 - r2_keras: 0.6147\n",
      "Epoch 134/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9080 - mae: 0.5105 - r2_keras: 0.6161\n",
      "Epoch 00134: loss did not improve from 0.90342\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9091 - mae: 0.5108 - r2_keras: 0.6158\n",
      "Epoch 135/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.9124 - mae: 0.5116 - r2_keras: 0.6139\n",
      "Epoch 00135: loss did not improve from 0.90342\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9122 - mae: 0.5115 - r2_keras: 0.6141\n",
      "Epoch 136/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.9088 - mae: 0.5093 - r2_keras: 0.6147\n",
      "Epoch 00136: loss did not improve from 0.90342\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9094 - mae: 0.5095 - r2_keras: 0.6149\n",
      "Epoch 137/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.9100 - mae: 0.5098 - r2_keras: 0.6153\n",
      "Epoch 00137: loss did not improve from 0.90342\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9098 - mae: 0.5097 - r2_keras: 0.6154\n",
      "Epoch 138/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9084 - mae: 0.5079 - r2_keras: 0.6159\n",
      "Epoch 00138: loss did not improve from 0.90342\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9083 - mae: 0.5080 - r2_keras: 0.6155\n",
      "Epoch 139/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.9034 - mae: 0.5076 - r2_keras: 0.6180\n",
      "Epoch 00139: loss improved from 0.90342 to 0.90335, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9033 - mae: 0.5075 - r2_keras: 0.6180\n",
      "Epoch 140/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8948 - mae: 0.5053 - r2_keras: 0.6211\n",
      "Epoch 00140: loss improved from 0.90335 to 0.89410, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8941 - mae: 0.5051 - r2_keras: 0.6208\n",
      "Epoch 141/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.9006 - mae: 0.5069 - r2_keras: 0.6180\n",
      "Epoch 00141: loss did not improve from 0.89410\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.9008 - mae: 0.5069 - r2_keras: 0.6180\n",
      "Epoch 142/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.9040 - mae: 0.5078 - r2_keras: 0.6179\n",
      "Epoch 00142: loss did not improve from 0.89410\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.9039 - mae: 0.5078 - r2_keras: 0.6180\n",
      "Epoch 143/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8974 - mae: 0.5065 - r2_keras: 0.6204\n",
      "Epoch 00143: loss did not improve from 0.89410\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8972 - mae: 0.5065 - r2_keras: 0.6204\n",
      "Epoch 144/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811/813 [============================>.] - ETA: 0s - loss: 0.8970 - mae: 0.5048 - r2_keras: 0.6204\n",
      "Epoch 00144: loss did not improve from 0.89410\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8971 - mae: 0.5048 - r2_keras: 0.6202\n",
      "Epoch 145/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8903 - mae: 0.5028 - r2_keras: 0.6223\n",
      "Epoch 00145: loss improved from 0.89410 to 0.89004, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8900 - mae: 0.5027 - r2_keras: 0.6223\n",
      "Epoch 146/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8978 - mae: 0.5036 - r2_keras: 0.6212\n",
      "Epoch 00146: loss did not improve from 0.89004\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8975 - mae: 0.5036 - r2_keras: 0.6213\n",
      "Epoch 147/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8959 - mae: 0.5035 - r2_keras: 0.6216\n",
      "Epoch 00147: loss did not improve from 0.89004\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8956 - mae: 0.5033 - r2_keras: 0.6218\n",
      "Epoch 148/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8892 - mae: 0.5008 - r2_keras: 0.6236\n",
      "Epoch 00148: loss improved from 0.89004 to 0.88928, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8893 - mae: 0.5008 - r2_keras: 0.6235\n",
      "Epoch 149/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8927 - mae: 0.5020 - r2_keras: 0.6219\n",
      "Epoch 00149: loss did not improve from 0.88928\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8929 - mae: 0.5020 - r2_keras: 0.6219\n",
      "Epoch 150/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8887 - mae: 0.5009 - r2_keras: 0.6231\n",
      "Epoch 00150: loss improved from 0.88928 to 0.88919, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.8892 - mae: 0.5011 - r2_keras: 0.6233\n",
      "Epoch 151/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8882 - mae: 0.4999 - r2_keras: 0.6233\n",
      "Epoch 00151: loss improved from 0.88919 to 0.88816, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8882 - mae: 0.5000 - r2_keras: 0.6236\n",
      "Epoch 152/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8890 - mae: 0.5006 - r2_keras: 0.6232\n",
      "Epoch 00152: loss did not improve from 0.88816\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8895 - mae: 0.5007 - r2_keras: 0.6229\n",
      "Epoch 153/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8877 - mae: 0.5018 - r2_keras: 0.6231\n",
      "Epoch 00153: loss improved from 0.88816 to 0.88776, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.8878 - mae: 0.5018 - r2_keras: 0.6229\n",
      "Epoch 154/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8987 - mae: 0.5029 - r2_keras: 0.6195\n",
      "Epoch 00154: loss did not improve from 0.88776\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8993 - mae: 0.5030 - r2_keras: 0.6194\n",
      "Epoch 155/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8860 - mae: 0.5007 - r2_keras: 0.6260\n",
      "Epoch 00155: loss improved from 0.88776 to 0.88577, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8858 - mae: 0.5006 - r2_keras: 0.6261\n",
      "Epoch 156/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8861 - mae: 0.4977 - r2_keras: 0.6252\n",
      "Epoch 00156: loss did not improve from 0.88577\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8867 - mae: 0.4979 - r2_keras: 0.6250\n",
      "Epoch 157/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.8900 - mae: 0.5006 - r2_keras: 0.6227\n",
      "Epoch 00157: loss did not improve from 0.88577\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8900 - mae: 0.5006 - r2_keras: 0.6227\n",
      "Epoch 158/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8837 - mae: 0.4982 - r2_keras: 0.6269\n",
      "Epoch 00158: loss improved from 0.88577 to 0.88416, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8842 - mae: 0.4983 - r2_keras: 0.6269\n",
      "Epoch 159/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8897 - mae: 0.4987 - r2_keras: 0.6238\n",
      "Epoch 00159: loss did not improve from 0.88416\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8903 - mae: 0.4989 - r2_keras: 0.6236\n",
      "Epoch 160/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8810 - mae: 0.4956 - r2_keras: 0.6281\n",
      "Epoch 00160: loss improved from 0.88416 to 0.88107, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8811 - mae: 0.4955 - r2_keras: 0.6280\n",
      "Epoch 161/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8801 - mae: 0.4956 - r2_keras: 0.6282\n",
      "Epoch 00161: loss improved from 0.88107 to 0.87983, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8798 - mae: 0.4954 - r2_keras: 0.6280\n",
      "Epoch 162/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.8813 - mae: 0.4944 - r2_keras: 0.6279\n",
      "Epoch 00162: loss did not improve from 0.87983\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8816 - mae: 0.4947 - r2_keras: 0.6278\n",
      "Epoch 163/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8785 - mae: 0.4944 - r2_keras: 0.6274\n",
      "Epoch 00163: loss improved from 0.87983 to 0.87852, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8785 - mae: 0.4945 - r2_keras: 0.6271\n",
      "Epoch 164/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8775 - mae: 0.4933 - r2_keras: 0.6288\n",
      "Epoch 00164: loss improved from 0.87852 to 0.87772, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8777 - mae: 0.4932 - r2_keras: 0.6286\n",
      "Epoch 165/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8799 - mae: 0.4936 - r2_keras: 0.6271\n",
      "Epoch 00165: loss did not improve from 0.87772\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8797 - mae: 0.4935 - r2_keras: 0.6272\n",
      "Epoch 166/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8859 - mae: 0.4955 - r2_keras: 0.6262\n",
      "Epoch 00166: loss did not improve from 0.87772\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8855 - mae: 0.4954 - r2_keras: 0.6265\n",
      "Epoch 167/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8796 - mae: 0.4947 - r2_keras: 0.6272\n",
      "Epoch 00167: loss did not improve from 0.87772\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8807 - mae: 0.4950 - r2_keras: 0.6269\n",
      "Epoch 168/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8834 - mae: 0.4958 - r2_keras: 0.6255\n",
      "Epoch 00168: loss did not improve from 0.87772\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8837 - mae: 0.4960 - r2_keras: 0.6253\n",
      "Epoch 169/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8863 - mae: 0.4948 - r2_keras: 0.6244\n",
      "Epoch 00169: loss did not improve from 0.87772\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8867 - mae: 0.4949 - r2_keras: 0.6241\n",
      "Epoch 170/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8779 - mae: 0.4925 - r2_keras: 0.6280\n",
      "Epoch 00170: loss improved from 0.87772 to 0.87765, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8777 - mae: 0.4925 - r2_keras: 0.6281\n",
      "Epoch 171/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8822 - mae: 0.4948 - r2_keras: 0.6260\n",
      "Epoch 00171: loss did not improve from 0.87765\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8829 - mae: 0.4950 - r2_keras: 0.6258\n",
      "Epoch 172/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.8752 - mae: 0.4925 - r2_keras: 0.6295\n",
      "Epoch 00172: loss improved from 0.87765 to 0.87522, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8752 - mae: 0.4925 - r2_keras: 0.6295\n",
      "Epoch 173/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8766 - mae: 0.4918 - r2_keras: 0.6290\n",
      "Epoch 00173: loss did not improve from 0.87522\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8766 - mae: 0.4918 - r2_keras: 0.6289\n",
      "Epoch 174/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8775 - mae: 0.4927 - r2_keras: 0.6289\n",
      "Epoch 00174: loss did not improve from 0.87522\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8778 - mae: 0.4927 - r2_keras: 0.6286\n",
      "Epoch 175/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8754 - mae: 0.4920 - r2_keras: 0.6306\n",
      "Epoch 00175: loss did not improve from 0.87522\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8754 - mae: 0.4920 - r2_keras: 0.6302\n",
      "Epoch 176/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8746 - mae: 0.4912 - r2_keras: 0.6300\n",
      "Epoch 00176: loss did not improve from 0.87522\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8752 - mae: 0.4914 - r2_keras: 0.6298\n",
      "Epoch 177/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.8757 - mae: 0.4921 - r2_keras: 0.6296\n",
      "Epoch 00177: loss did not improve from 0.87522\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8764 - mae: 0.4923 - r2_keras: 0.6292\n",
      "Epoch 178/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8783 - mae: 0.4929 - r2_keras: 0.6278\n",
      "Epoch 00178: loss did not improve from 0.87522\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8783 - mae: 0.4928 - r2_keras: 0.6278\n",
      "Epoch 179/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8817 - mae: 0.4955 - r2_keras: 0.6271\n",
      "Epoch 00179: loss did not improve from 0.87522\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8819 - mae: 0.4956 - r2_keras: 0.6268\n",
      "Epoch 180/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8824 - mae: 0.4930 - r2_keras: 0.6262\n",
      "Epoch 00180: loss did not improve from 0.87522\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8820 - mae: 0.4929 - r2_keras: 0.6265\n",
      "Epoch 181/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8760 - mae: 0.4913 - r2_keras: 0.6289\n",
      "Epoch 00181: loss did not improve from 0.87522\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8758 - mae: 0.4912 - r2_keras: 0.6291\n",
      "Epoch 182/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8736 - mae: 0.4907 - r2_keras: 0.6306\n",
      "Epoch 00182: loss improved from 0.87522 to 0.87415, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8741 - mae: 0.4909 - r2_keras: 0.6306\n",
      "Epoch 183/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8718 - mae: 0.4902 - r2_keras: 0.6322\n",
      "Epoch 00183: loss improved from 0.87415 to 0.87180, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8718 - mae: 0.4902 - r2_keras: 0.6322\n",
      "Epoch 184/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8684 - mae: 0.4894 - r2_keras: 0.6328\n",
      "Epoch 00184: loss improved from 0.87180 to 0.86923, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8692 - mae: 0.4896 - r2_keras: 0.6329\n",
      "Epoch 185/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8731 - mae: 0.4910 - r2_keras: 0.6312\n",
      "Epoch 00185: loss did not improve from 0.86923\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8728 - mae: 0.4909 - r2_keras: 0.6311\n",
      "Epoch 186/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8644 - mae: 0.4875 - r2_keras: 0.6340\n",
      "Epoch 00186: loss improved from 0.86923 to 0.86397, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8640 - mae: 0.4875 - r2_keras: 0.6343\n",
      "Epoch 187/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8662 - mae: 0.4874 - r2_keras: 0.6344\n",
      "Epoch 00187: loss did not improve from 0.86397\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8662 - mae: 0.4875 - r2_keras: 0.6344\n",
      "Epoch 188/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8624 - mae: 0.4873 - r2_keras: 0.6347\n",
      "Epoch 00188: loss improved from 0.86397 to 0.86299, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.8630 - mae: 0.4875 - r2_keras: 0.6348\n",
      "Epoch 189/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8617 - mae: 0.4855 - r2_keras: 0.6354\n",
      "Epoch 00189: loss improved from 0.86299 to 0.86223, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8622 - mae: 0.4856 - r2_keras: 0.6354\n",
      "Epoch 190/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8646 - mae: 0.4860 - r2_keras: 0.6346\n",
      "Epoch 00190: loss did not improve from 0.86223\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8639 - mae: 0.4858 - r2_keras: 0.6344\n",
      "Epoch 191/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8628 - mae: 0.4868 - r2_keras: 0.6364\n",
      "Epoch 00191: loss did not improve from 0.86223\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8629 - mae: 0.4868 - r2_keras: 0.6365\n",
      "Epoch 192/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8560 - mae: 0.4841 - r2_keras: 0.6378 ETA: 0s - loss: 0.8540 - mae: 0.4\n",
      "Epoch 00192: loss improved from 0.86223 to 0.85586, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8559 - mae: 0.4841 - r2_keras: 0.6379\n",
      "Epoch 193/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8607 - mae: 0.4858 - r2_keras: 0.6362\n",
      "Epoch 00193: loss did not improve from 0.85586\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8606 - mae: 0.4857 - r2_keras: 0.6362\n",
      "Epoch 194/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8585 - mae: 0.4846 - r2_keras: 0.6355\n",
      "Epoch 00194: loss did not improve from 0.85586\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8578 - mae: 0.4843 - r2_keras: 0.6358\n",
      "Epoch 195/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.8606 - mae: 0.4850 - r2_keras: 0.6360\n",
      "Epoch 00195: loss did not improve from 0.85586\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8606 - mae: 0.4850 - r2_keras: 0.6360\n",
      "Epoch 196/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8623 - mae: 0.4846 - r2_keras: 0.6345\n",
      "Epoch 00196: loss did not improve from 0.85586\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8622 - mae: 0.4846 - r2_keras: 0.6345\n",
      "Epoch 197/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8592 - mae: 0.4851 - r2_keras: 0.6364\n",
      "Epoch 00197: loss did not improve from 0.85586\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8599 - mae: 0.4853 - r2_keras: 0.6364\n",
      "Epoch 198/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8582 - mae: 0.4849 - r2_keras: 0.6371\n",
      "Epoch 00198: loss did not improve from 0.85586\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8584 - mae: 0.4849 - r2_keras: 0.6372\n",
      "Epoch 199/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.8540 - mae: 0.4833 - r2_keras: 0.6390\n",
      "Epoch 00199: loss improved from 0.85586 to 0.85397, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8540 - mae: 0.4833 - r2_keras: 0.6390\n",
      "Epoch 200/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.8555 - mae: 0.4832 - r2_keras: 0.6393\n",
      "Epoch 00200: loss did not improve from 0.85397\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8564 - mae: 0.4835 - r2_keras: 0.6387\n",
      "Epoch 201/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8589 - mae: 0.4849 - r2_keras: 0.6348\n",
      "Epoch 00201: loss did not improve from 0.85397\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8582 - mae: 0.4847 - r2_keras: 0.6355\n",
      "Epoch 202/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8586 - mae: 0.4836 - r2_keras: 0.6375\n",
      "Epoch 00202: loss did not improve from 0.85397\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8577 - mae: 0.4834 - r2_keras: 0.6379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8534 - mae: 0.4817 - r2_keras: 0.6387\n",
      "Epoch 00203: loss improved from 0.85397 to 0.85345, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8534 - mae: 0.4818 - r2_keras: 0.6386\n",
      "Epoch 204/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8472 - mae: 0.4808 - r2_keras: 0.6417\n",
      "Epoch 00204: loss improved from 0.85345 to 0.84709, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8471 - mae: 0.4807 - r2_keras: 0.6418\n",
      "Epoch 205/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8450 - mae: 0.4801 - r2_keras: 0.6427\n",
      "Epoch 00205: loss improved from 0.84709 to 0.84426, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8443 - mae: 0.4799 - r2_keras: 0.6428\n",
      "Epoch 206/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8507 - mae: 0.4819 - r2_keras: 0.6405\n",
      "Epoch 00206: loss did not improve from 0.84426\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8519 - mae: 0.4822 - r2_keras: 0.6398\n",
      "Epoch 207/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8523 - mae: 0.4824 - r2_keras: 0.6389\n",
      "Epoch 00207: loss did not improve from 0.84426\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8520 - mae: 0.4823 - r2_keras: 0.6392\n",
      "Epoch 208/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8488 - mae: 0.4807 - r2_keras: 0.6412\n",
      "Epoch 00208: loss did not improve from 0.84426\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8490 - mae: 0.4808 - r2_keras: 0.6411\n",
      "Epoch 209/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8480 - mae: 0.4808 - r2_keras: 0.6398\n",
      "Epoch 00209: loss did not improve from 0.84426\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8478 - mae: 0.4808 - r2_keras: 0.6401\n",
      "Epoch 210/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8454 - mae: 0.4801 - r2_keras: 0.6414\n",
      "Epoch 00210: loss did not improve from 0.84426\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8457 - mae: 0.4801 - r2_keras: 0.6410\n",
      "Epoch 211/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8451 - mae: 0.4798 - r2_keras: 0.6434\n",
      "Epoch 00211: loss did not improve from 0.84426\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8464 - mae: 0.4801 - r2_keras: 0.6428\n",
      "Epoch 212/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8449 - mae: 0.4792 - r2_keras: 0.6427\n",
      "Epoch 00212: loss did not improve from 0.84426\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8454 - mae: 0.4792 - r2_keras: 0.6423\n",
      "Epoch 213/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8441 - mae: 0.4785 - r2_keras: 0.6429\n",
      "Epoch 00213: loss did not improve from 0.84426\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8443 - mae: 0.4785 - r2_keras: 0.6429\n",
      "Epoch 214/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8409 - mae: 0.4783 - r2_keras: 0.6443\n",
      "Epoch 00214: loss improved from 0.84426 to 0.84158, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.8416 - mae: 0.4785 - r2_keras: 0.6441\n",
      "Epoch 215/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8367 - mae: 0.4771 - r2_keras: 0.6453\n",
      "Epoch 00215: loss improved from 0.84158 to 0.83698, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8370 - mae: 0.4772 - r2_keras: 0.6452\n",
      "Epoch 216/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8431 - mae: 0.4788 - r2_keras: 0.6435\n",
      "Epoch 00216: loss did not improve from 0.83698\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8423 - mae: 0.4787 - r2_keras: 0.6439\n",
      "Epoch 217/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8444 - mae: 0.4797 - r2_keras: 0.6418\n",
      "Epoch 00217: loss did not improve from 0.83698\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8445 - mae: 0.4798 - r2_keras: 0.6415\n",
      "Epoch 218/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8433 - mae: 0.4785 - r2_keras: 0.6423\n",
      "Epoch 00218: loss did not improve from 0.83698\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8436 - mae: 0.4785 - r2_keras: 0.6424\n",
      "Epoch 219/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8379 - mae: 0.4779 - r2_keras: 0.6448\n",
      "Epoch 00219: loss did not improve from 0.83698\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8375 - mae: 0.4778 - r2_keras: 0.6451\n",
      "Epoch 220/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8394 - mae: 0.4771 - r2_keras: 0.6445\n",
      "Epoch 00220: loss did not improve from 0.83698\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8392 - mae: 0.4770 - r2_keras: 0.6444\n",
      "Epoch 221/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8464 - mae: 0.4791 - r2_keras: 0.6410\n",
      "Epoch 00221: loss did not improve from 0.83698\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8467 - mae: 0.4792 - r2_keras: 0.6410\n",
      "Epoch 222/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8364 - mae: 0.4765 - r2_keras: 0.6458\n",
      "Epoch 00222: loss improved from 0.83698 to 0.83691, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8369 - mae: 0.4766 - r2_keras: 0.6455\n",
      "Epoch 223/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8339 - mae: 0.4755 - r2_keras: 0.6465\n",
      "Epoch 00223: loss improved from 0.83691 to 0.83398, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8340 - mae: 0.4755 - r2_keras: 0.6465\n",
      "Epoch 224/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8376 - mae: 0.4773 - r2_keras: 0.6458\n",
      "Epoch 00224: loss did not improve from 0.83398\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8374 - mae: 0.4773 - r2_keras: 0.6458\n",
      "Epoch 225/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8410 - mae: 0.4769 - r2_keras: 0.6440\n",
      "Epoch 00225: loss did not improve from 0.83398\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8412 - mae: 0.4769 - r2_keras: 0.6439\n",
      "Epoch 226/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.8374 - mae: 0.4763 - r2_keras: 0.6454\n",
      "Epoch 00226: loss did not improve from 0.83398\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8374 - mae: 0.4763 - r2_keras: 0.6454\n",
      "Epoch 227/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8409 - mae: 0.4756 - r2_keras: 0.6440\n",
      "Epoch 00227: loss did not improve from 0.83398\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8403 - mae: 0.4754 - r2_keras: 0.6442\n",
      "Epoch 228/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8340 - mae: 0.4741 - r2_keras: 0.6477\n",
      "Epoch 00228: loss did not improve from 0.83398\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8342 - mae: 0.4742 - r2_keras: 0.6476\n",
      "Epoch 229/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8343 - mae: 0.4742 - r2_keras: 0.6463\n",
      "Epoch 00229: loss did not improve from 0.83398\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8344 - mae: 0.4742 - r2_keras: 0.6464\n",
      "Epoch 230/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8378 - mae: 0.4752 - r2_keras: 0.6454\n",
      "Epoch 00230: loss did not improve from 0.83398\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8373 - mae: 0.4751 - r2_keras: 0.6458\n",
      "Epoch 231/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8327 - mae: 0.4732 - r2_keras: 0.6477\n",
      "Epoch 00231: loss improved from 0.83398 to 0.83286, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8329 - mae: 0.4732 - r2_keras: 0.6474\n",
      "Epoch 232/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8332 - mae: 0.4733 - r2_keras: 0.6478\n",
      "Epoch 00232: loss did not improve from 0.83286\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8331 - mae: 0.4732 - r2_keras: 0.6479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8341 - mae: 0.4739 - r2_keras: 0.6461\n",
      "Epoch 00233: loss did not improve from 0.83286\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8337 - mae: 0.4739 - r2_keras: 0.6462\n",
      "Epoch 234/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8323 - mae: 0.4725 - r2_keras: 0.6486\n",
      "Epoch 00234: loss improved from 0.83286 to 0.83196, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8320 - mae: 0.4724 - r2_keras: 0.6486\n",
      "Epoch 235/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8510 - mae: 0.4798 - r2_keras: 0.6398\n",
      "Epoch 00235: loss did not improve from 0.83196\n",
      "813/813 [==============================] - 7s 8ms/step - loss: 0.8517 - mae: 0.4799 - r2_keras: 0.6397\n",
      "Epoch 236/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8432 - mae: 0.4773 - r2_keras: 0.6427\n",
      "Epoch 00236: loss did not improve from 0.83196\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8439 - mae: 0.4774 - r2_keras: 0.6425\n",
      "Epoch 237/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8347 - mae: 0.4750 - r2_keras: 0.6451\n",
      "Epoch 00237: loss did not improve from 0.83196\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8345 - mae: 0.4747 - r2_keras: 0.6449\n",
      "Epoch 238/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.8356 - mae: 0.4744 - r2_keras: 0.6455\n",
      "Epoch 00238: loss did not improve from 0.83196\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8356 - mae: 0.4744 - r2_keras: 0.6455\n",
      "Epoch 239/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8377 - mae: 0.4761 - r2_keras: 0.6436\n",
      "Epoch 00239: loss did not improve from 0.83196\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8378 - mae: 0.4762 - r2_keras: 0.6436\n",
      "Epoch 240/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8342 - mae: 0.4744 - r2_keras: 0.6471\n",
      "Epoch 00240: loss did not improve from 0.83196\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8345 - mae: 0.4745 - r2_keras: 0.6469\n",
      "Epoch 241/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8329 - mae: 0.4748 - r2_keras: 0.6476\n",
      "Epoch 00241: loss did not improve from 0.83196\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8322 - mae: 0.4746 - r2_keras: 0.6478\n",
      "Epoch 242/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8250 - mae: 0.4711 - r2_keras: 0.6501 ETA: 0s - loss: 0.8256 - mae: 0.4710 - r2_keras\n",
      "Epoch 00242: loss improved from 0.83196 to 0.82495, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8249 - mae: 0.4711 - r2_keras: 0.6502\n",
      "Epoch 243/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8335 - mae: 0.4737 - r2_keras: 0.6471\n",
      "Epoch 00243: loss did not improve from 0.82495\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8336 - mae: 0.4738 - r2_keras: 0.6473\n",
      "Epoch 244/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8265 - mae: 0.4719 - r2_keras: 0.6499\n",
      "Epoch 00244: loss did not improve from 0.82495\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8258 - mae: 0.4718 - r2_keras: 0.6501\n",
      "Epoch 245/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8279 - mae: 0.4716 - r2_keras: 0.6499\n",
      "Epoch 00245: loss did not improve from 0.82495\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8272 - mae: 0.4715 - r2_keras: 0.6503\n",
      "Epoch 246/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8287 - mae: 0.4711 - r2_keras: 0.6498\n",
      "Epoch 00246: loss did not improve from 0.82495\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8284 - mae: 0.4711 - r2_keras: 0.6499\n",
      "Epoch 247/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8312 - mae: 0.4715 - r2_keras: 0.6486\n",
      "Epoch 00247: loss did not improve from 0.82495\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8311 - mae: 0.4714 - r2_keras: 0.6484\n",
      "Epoch 248/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8270 - mae: 0.4704 - r2_keras: 0.6503\n",
      "Epoch 00248: loss did not improve from 0.82495\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8266 - mae: 0.4703 - r2_keras: 0.6502\n",
      "Epoch 249/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8283 - mae: 0.4711 - r2_keras: 0.6486\n",
      "Epoch 00249: loss did not improve from 0.82495\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8281 - mae: 0.4710 - r2_keras: 0.6489\n",
      "Epoch 250/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8270 - mae: 0.4714 - r2_keras: 0.6509 ETA: 0s - loss: 0.8273 - mae: 0.4715 - r2_keras: 0.65\n",
      "Epoch 00250: loss did not improve from 0.82495\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8268 - mae: 0.4713 - r2_keras: 0.6511\n",
      "Epoch 251/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8197 - mae: 0.4680 - r2_keras: 0.6520\n",
      "Epoch 00251: loss improved from 0.82495 to 0.81988, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8199 - mae: 0.4680 - r2_keras: 0.6521\n",
      "Epoch 252/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8188 - mae: 0.4690 - r2_keras: 0.6523\n",
      "Epoch 00252: loss improved from 0.81988 to 0.81889, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8189 - mae: 0.4690 - r2_keras: 0.6523\n",
      "Epoch 253/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.8199 - mae: 0.4681 - r2_keras: 0.6529\n",
      "Epoch 00253: loss did not improve from 0.81889\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8199 - mae: 0.4681 - r2_keras: 0.6527\n",
      "Epoch 254/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8192 - mae: 0.4682 - r2_keras: 0.6524\n",
      "Epoch 00254: loss did not improve from 0.81889\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8193 - mae: 0.4682 - r2_keras: 0.6523\n",
      "Epoch 255/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8161 - mae: 0.4666 - r2_keras: 0.6546\n",
      "Epoch 00255: loss improved from 0.81889 to 0.81588, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8159 - mae: 0.4666 - r2_keras: 0.6550\n",
      "Epoch 256/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8165 - mae: 0.4666 - r2_keras: 0.6532 ETA: 1s - loss:\n",
      "Epoch 00256: loss did not improve from 0.81588\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8165 - mae: 0.4667 - r2_keras: 0.6534\n",
      "Epoch 257/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8170 - mae: 0.4663 - r2_keras: 0.6540\n",
      "Epoch 00257: loss did not improve from 0.81588\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8169 - mae: 0.4662 - r2_keras: 0.6540\n",
      "Epoch 258/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8132 - mae: 0.4651 - r2_keras: 0.6560\n",
      "Epoch 00258: loss improved from 0.81588 to 0.81368, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8137 - mae: 0.4653 - r2_keras: 0.6554\n",
      "Epoch 259/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8190 - mae: 0.4676 - r2_keras: 0.6540\n",
      "Epoch 00259: loss did not improve from 0.81368\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8196 - mae: 0.4677 - r2_keras: 0.6538\n",
      "Epoch 260/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8263 - mae: 0.4701 - r2_keras: 0.6500\n",
      "Epoch 00260: loss did not improve from 0.81368\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8267 - mae: 0.4702 - r2_keras: 0.6499\n",
      "Epoch 261/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8187 - mae: 0.4665 - r2_keras: 0.6545\n",
      "Epoch 00261: loss did not improve from 0.81368\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8191 - mae: 0.4667 - r2_keras: 0.6547\n",
      "Epoch 262/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8151 - mae: 0.4661 - r2_keras: 0.6549\n",
      "Epoch 00262: loss did not improve from 0.81368\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8157 - mae: 0.4664 - r2_keras: 0.6548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8248 - mae: 0.4682 - r2_keras: 0.6505\n",
      "Epoch 00263: loss did not improve from 0.81368\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8250 - mae: 0.4683 - r2_keras: 0.6505\n",
      "Epoch 264/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.8225 - mae: 0.4680 - r2_keras: 0.6511\n",
      "Epoch 00264: loss did not improve from 0.81368\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8226 - mae: 0.4680 - r2_keras: 0.6512\n",
      "Epoch 265/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.8180 - mae: 0.4666 - r2_keras: 0.6534\n",
      "Epoch 00265: loss did not improve from 0.81368\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8180 - mae: 0.4666 - r2_keras: 0.6534\n",
      "Epoch 266/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.8124 - mae: 0.4637 - r2_keras: 0.6554\n",
      "Epoch 00266: loss improved from 0.81368 to 0.81278, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8128 - mae: 0.4637 - r2_keras: 0.6554\n",
      "Epoch 267/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.8119 - mae: 0.4640 - r2_keras: 0.6565\n",
      "Epoch 00267: loss improved from 0.81278 to 0.81102, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8110 - mae: 0.4638 - r2_keras: 0.6565\n",
      "Epoch 268/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.8110 - mae: 0.4649 - r2_keras: 0.6573\n",
      "Epoch 00268: loss improved from 0.81102 to 0.81096, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8110 - mae: 0.4649 - r2_keras: 0.6573\n",
      "Epoch 269/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8158 - mae: 0.4664 - r2_keras: 0.6549\n",
      "Epoch 00269: loss did not improve from 0.81096\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8158 - mae: 0.4664 - r2_keras: 0.6549\n",
      "Epoch 270/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.8144 - mae: 0.4654 - r2_keras: 0.6549\n",
      "Epoch 00270: loss did not improve from 0.81096\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8151 - mae: 0.4655 - r2_keras: 0.6549\n",
      "Epoch 271/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8162 - mae: 0.4669 - r2_keras: 0.6554\n",
      "Epoch 00271: loss did not improve from 0.81096\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8165 - mae: 0.4670 - r2_keras: 0.6553\n",
      "Epoch 272/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.8115 - mae: 0.4650 - r2_keras: 0.6568\n",
      "Epoch 00272: loss did not improve from 0.81096\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8117 - mae: 0.4650 - r2_keras: 0.6565\n",
      "Epoch 273/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.8117 - mae: 0.4647 - r2_keras: 0.6572\n",
      "Epoch 00273: loss did not improve from 0.81096\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8122 - mae: 0.4648 - r2_keras: 0.6571\n",
      "Epoch 274/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.8136 - mae: 0.4650 - r2_keras: 0.6561\n",
      "Epoch 00274: loss did not improve from 0.81096\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8132 - mae: 0.4650 - r2_keras: 0.6563\n",
      "Epoch 275/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8151 - mae: 0.4650 - r2_keras: 0.6559\n",
      "Epoch 00275: loss did not improve from 0.81096\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8148 - mae: 0.4649 - r2_keras: 0.6560\n",
      "Epoch 276/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.8173 - mae: 0.4664 - r2_keras: 0.6540\n",
      "Epoch 00276: loss did not improve from 0.81096\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.8174 - mae: 0.4665 - r2_keras: 0.6539\n",
      "Epoch 277/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.8105 - mae: 0.4639 - r2_keras: 0.6569\n",
      "Epoch 00277: loss did not improve from 0.81096\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.8114 - mae: 0.4643 - r2_keras: 0.6563\n",
      "Epoch 278/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7794 - mae: 0.4508 - r2_keras: 0.6697\n",
      "Epoch 00278: loss improved from 0.81096 to 0.77932, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7793 - mae: 0.4507 - r2_keras: 0.6697\n",
      "Epoch 279/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7771 - mae: 0.4496 - r2_keras: 0.6697\n",
      "Epoch 00279: loss improved from 0.77932 to 0.77627, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7763 - mae: 0.4494 - r2_keras: 0.6700\n",
      "Epoch 280/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7740 - mae: 0.4489 - r2_keras: 0.6729\n",
      "Epoch 00280: loss improved from 0.77627 to 0.77432, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7743 - mae: 0.4490 - r2_keras: 0.6725\n",
      "Epoch 281/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7736 - mae: 0.4494 - r2_keras: 0.6721\n",
      "Epoch 00281: loss improved from 0.77432 to 0.77405, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7741 - mae: 0.4495 - r2_keras: 0.6717\n",
      "Epoch 282/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7745 - mae: 0.4494 - r2_keras: 0.6712\n",
      "Epoch 00282: loss did not improve from 0.77405\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7743 - mae: 0.4493 - r2_keras: 0.6711\n",
      "Epoch 283/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7706 - mae: 0.4480 - r2_keras: 0.6735\n",
      "Epoch 00283: loss improved from 0.77405 to 0.77097, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7710 - mae: 0.4482 - r2_keras: 0.6736\n",
      "Epoch 284/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7697 - mae: 0.4477 - r2_keras: 0.6735\n",
      "Epoch 00284: loss improved from 0.77097 to 0.76961, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7696 - mae: 0.4477 - r2_keras: 0.6737\n",
      "Epoch 285/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7688 - mae: 0.4468 - r2_keras: 0.6744\n",
      "Epoch 00285: loss improved from 0.76961 to 0.76874, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7687 - mae: 0.4469 - r2_keras: 0.6743\n",
      "Epoch 286/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7694 - mae: 0.4479 - r2_keras: 0.6731\n",
      "Epoch 00286: loss did not improve from 0.76874\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7698 - mae: 0.4480 - r2_keras: 0.6728\n",
      "Epoch 287/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7683 - mae: 0.4468 - r2_keras: 0.6741 ETA: 0s - loss: 0.7682 - mae: 0.4\n",
      "Epoch 00287: loss improved from 0.76874 to 0.76842, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7684 - mae: 0.4469 - r2_keras: 0.6741\n",
      "Epoch 288/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7704 - mae: 0.4477 - r2_keras: 0.6735\n",
      "Epoch 00288: loss did not improve from 0.76842\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7706 - mae: 0.4477 - r2_keras: 0.6736\n",
      "Epoch 289/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7709 - mae: 0.4480 - r2_keras: 0.6741\n",
      "Epoch 00289: loss did not improve from 0.76842\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7706 - mae: 0.4479 - r2_keras: 0.6741\n",
      "Epoch 290/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7700 - mae: 0.4472 - r2_keras: 0.6744\n",
      "Epoch 00290: loss did not improve from 0.76842\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7702 - mae: 0.4472 - r2_keras: 0.6744\n",
      "Epoch 291/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7721 - mae: 0.4490 - r2_keras: 0.6737\n",
      "Epoch 00291: loss did not improve from 0.76842\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7725 - mae: 0.4490 - r2_keras: 0.6734\n",
      "Epoch 292/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7690 - mae: 0.4471 - r2_keras: 0.6739\n",
      "Epoch 00292: loss did not improve from 0.76842\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7692 - mae: 0.4471 - r2_keras: 0.6739\n",
      "Epoch 293/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7621 - mae: 0.4458 - r2_keras: 0.6775\n",
      "Epoch 00293: loss improved from 0.76842 to 0.76329, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7633 - mae: 0.4461 - r2_keras: 0.6763\n",
      "Epoch 294/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7691 - mae: 0.4477 - r2_keras: 0.6751\n",
      "Epoch 00294: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7692 - mae: 0.4478 - r2_keras: 0.6749\n",
      "Epoch 295/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7706 - mae: 0.4481 - r2_keras: 0.6742\n",
      "Epoch 00295: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7702 - mae: 0.4480 - r2_keras: 0.6738\n",
      "Epoch 296/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7688 - mae: 0.4472 - r2_keras: 0.6750\n",
      "Epoch 00296: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7685 - mae: 0.4471 - r2_keras: 0.6751\n",
      "Epoch 297/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7724 - mae: 0.4479 - r2_keras: 0.6731\n",
      "Epoch 00297: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7727 - mae: 0.4481 - r2_keras: 0.6730\n",
      "Epoch 298/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7725 - mae: 0.4482 - r2_keras: 0.6730\n",
      "Epoch 00298: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7728 - mae: 0.4483 - r2_keras: 0.6728\n",
      "Epoch 299/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7663 - mae: 0.4473 - r2_keras: 0.6748\n",
      "Epoch 00299: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7658 - mae: 0.4472 - r2_keras: 0.6749\n",
      "Epoch 300/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7694 - mae: 0.4475 - r2_keras: 0.6742\n",
      "Epoch 00300: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7694 - mae: 0.4475 - r2_keras: 0.6742\n",
      "Epoch 301/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7717 - mae: 0.4482 - r2_keras: 0.6730\n",
      "Epoch 00301: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7719 - mae: 0.4482 - r2_keras: 0.6729\n",
      "Epoch 302/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.7674 - mae: 0.4470 - r2_keras: 0.6750\n",
      "Epoch 00302: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7676 - mae: 0.4471 - r2_keras: 0.6750\n",
      "Epoch 303/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7707 - mae: 0.4488 - r2_keras: 0.6735\n",
      "Epoch 00303: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7706 - mae: 0.4488 - r2_keras: 0.6737\n",
      "Epoch 304/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7667 - mae: 0.4479 - r2_keras: 0.6744\n",
      "Epoch 00304: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7666 - mae: 0.4479 - r2_keras: 0.6741\n",
      "Epoch 305/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7663 - mae: 0.4472 - r2_keras: 0.6754\n",
      "Epoch 00305: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7663 - mae: 0.4472 - r2_keras: 0.6756\n",
      "Epoch 306/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7656 - mae: 0.4462 - r2_keras: 0.6759\n",
      "Epoch 00306: loss did not improve from 0.76329\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7656 - mae: 0.4462 - r2_keras: 0.6760\n",
      "Epoch 307/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7636 - mae: 0.4468 - r2_keras: 0.6773\n",
      "Epoch 00307: loss improved from 0.76329 to 0.76309, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.7631 - mae: 0.4467 - r2_keras: 0.6768\n",
      "Epoch 308/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7631 - mae: 0.4461 - r2_keras: 0.6771\n",
      "Epoch 00308: loss did not improve from 0.76309\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7632 - mae: 0.4462 - r2_keras: 0.6772\n",
      "Epoch 309/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7628 - mae: 0.4460 - r2_keras: 0.6776\n",
      "Epoch 00309: loss did not improve from 0.76309\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7632 - mae: 0.4462 - r2_keras: 0.6773\n",
      "Epoch 310/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7647 - mae: 0.4462 - r2_keras: 0.6767\n",
      "Epoch 00310: loss did not improve from 0.76309\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7646 - mae: 0.4461 - r2_keras: 0.6768\n",
      "Epoch 311/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7679 - mae: 0.4477 - r2_keras: 0.6744\n",
      "Epoch 00311: loss did not improve from 0.76309\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7680 - mae: 0.4476 - r2_keras: 0.6743\n",
      "Epoch 312/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.7609 - mae: 0.4451 - r2_keras: 0.6769\n",
      "Epoch 00312: loss improved from 0.76309 to 0.76122, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7612 - mae: 0.4452 - r2_keras: 0.6766\n",
      "Epoch 313/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7635 - mae: 0.4464 - r2_keras: 0.6772\n",
      "Epoch 00313: loss did not improve from 0.76122\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7633 - mae: 0.4464 - r2_keras: 0.6774\n",
      "Epoch 314/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7638 - mae: 0.4460 - r2_keras: 0.6769\n",
      "Epoch 00314: loss did not improve from 0.76122\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7634 - mae: 0.4459 - r2_keras: 0.6770\n",
      "Epoch 315/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7615 - mae: 0.4453 - r2_keras: 0.6772\n",
      "Epoch 00315: loss did not improve from 0.76122\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7618 - mae: 0.4454 - r2_keras: 0.6770\n",
      "Epoch 316/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7611 - mae: 0.4454 - r2_keras: 0.6775\n",
      "Epoch 00316: loss improved from 0.76122 to 0.76096, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7610 - mae: 0.4453 - r2_keras: 0.6776\n",
      "Epoch 317/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7586 - mae: 0.4454 - r2_keras: 0.6792\n",
      "Epoch 00317: loss improved from 0.76096 to 0.75891, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7589 - mae: 0.4455 - r2_keras: 0.6792\n",
      "Epoch 318/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7651 - mae: 0.4456 - r2_keras: 0.6759\n",
      "Epoch 00318: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7653 - mae: 0.4457 - r2_keras: 0.6759\n",
      "Epoch 319/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7702 - mae: 0.4470 - r2_keras: 0.6732\n",
      "Epoch 00319: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7703 - mae: 0.4471 - r2_keras: 0.6730\n",
      "Epoch 320/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7666 - mae: 0.4460 - r2_keras: 0.6761\n",
      "Epoch 00320: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7669 - mae: 0.4461 - r2_keras: 0.6759\n",
      "Epoch 321/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7674 - mae: 0.4466 - r2_keras: 0.6747\n",
      "Epoch 00321: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7671 - mae: 0.4466 - r2_keras: 0.6751\n",
      "Epoch 322/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807/813 [============================>.] - ETA: 0s - loss: 0.7638 - mae: 0.4463 - r2_keras: 0.6761\n",
      "Epoch 00322: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7636 - mae: 0.4462 - r2_keras: 0.6761\n",
      "Epoch 323/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7588 - mae: 0.4446 - r2_keras: 0.6788\n",
      "Epoch 00323: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7592 - mae: 0.4446 - r2_keras: 0.6785\n",
      "Epoch 324/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7636 - mae: 0.4465 - r2_keras: 0.6774\n",
      "Epoch 00324: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7630 - mae: 0.4463 - r2_keras: 0.6770\n",
      "Epoch 325/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7630 - mae: 0.4463 - r2_keras: 0.6768\n",
      "Epoch 00325: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7630 - mae: 0.4463 - r2_keras: 0.6768\n",
      "Epoch 326/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7692 - mae: 0.4470 - r2_keras: 0.6739\n",
      "Epoch 00326: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7688 - mae: 0.4469 - r2_keras: 0.6740\n",
      "Epoch 327/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7607 - mae: 0.4459 - r2_keras: 0.6778\n",
      "Epoch 00327: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7609 - mae: 0.4460 - r2_keras: 0.6778\n",
      "Epoch 328/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7609 - mae: 0.4449 - r2_keras: 0.6775\n",
      "Epoch 00328: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7607 - mae: 0.4449 - r2_keras: 0.6777\n",
      "Epoch 329/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7617 - mae: 0.4447 - r2_keras: 0.6766\n",
      "Epoch 00329: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7617 - mae: 0.4447 - r2_keras: 0.6766\n",
      "Epoch 330/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7674 - mae: 0.4462 - r2_keras: 0.6744\n",
      "Epoch 00330: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7673 - mae: 0.4462 - r2_keras: 0.6744\n",
      "Epoch 331/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7642 - mae: 0.4458 - r2_keras: 0.6755\n",
      "Epoch 00331: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7637 - mae: 0.4456 - r2_keras: 0.6757\n",
      "Epoch 332/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7671 - mae: 0.4466 - r2_keras: 0.6740\n",
      "Epoch 00332: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7675 - mae: 0.4467 - r2_keras: 0.6738\n",
      "Epoch 333/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7618 - mae: 0.4452 - r2_keras: 0.6774\n",
      "Epoch 00333: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7618 - mae: 0.4452 - r2_keras: 0.6775\n",
      "Epoch 334/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7649 - mae: 0.4464 - r2_keras: 0.6764\n",
      "Epoch 00334: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7648 - mae: 0.4464 - r2_keras: 0.6765\n",
      "Epoch 335/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7605 - mae: 0.4447 - r2_keras: 0.6792\n",
      "Epoch 00335: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7603 - mae: 0.4446 - r2_keras: 0.6792\n",
      "Epoch 336/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7646 - mae: 0.4461 - r2_keras: 0.6767\n",
      "Epoch 00336: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7649 - mae: 0.4462 - r2_keras: 0.6763\n",
      "Epoch 337/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7614 - mae: 0.4449 - r2_keras: 0.6767\n",
      "Epoch 00337: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7615 - mae: 0.4449 - r2_keras: 0.6768\n",
      "Epoch 338/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7643 - mae: 0.4455 - r2_keras: 0.6760\n",
      "Epoch 00338: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7644 - mae: 0.4454 - r2_keras: 0.6757\n",
      "Epoch 339/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7586 - mae: 0.4446 - r2_keras: 0.6793\n",
      "Epoch 00339: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7594 - mae: 0.4448 - r2_keras: 0.6787\n",
      "Epoch 340/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7676 - mae: 0.4470 - r2_keras: 0.6751\n",
      "Epoch 00340: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7672 - mae: 0.4468 - r2_keras: 0.6753\n",
      "Epoch 341/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7646 - mae: 0.4461 - r2_keras: 0.6754\n",
      "Epoch 00341: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7647 - mae: 0.4461 - r2_keras: 0.6756\n",
      "Epoch 342/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7654 - mae: 0.4455 - r2_keras: 0.6751\n",
      "Epoch 00342: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7652 - mae: 0.4454 - r2_keras: 0.6751\n",
      "Epoch 343/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7607 - mae: 0.4449 - r2_keras: 0.6782\n",
      "Epoch 00343: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7610 - mae: 0.4450 - r2_keras: 0.6780\n",
      "Epoch 344/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7645 - mae: 0.4465 - r2_keras: 0.6762\n",
      "Epoch 00344: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7646 - mae: 0.4465 - r2_keras: 0.6763\n",
      "Epoch 345/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7618 - mae: 0.4452 - r2_keras: 0.6771\n",
      "Epoch 00345: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7618 - mae: 0.4452 - r2_keras: 0.6769\n",
      "Epoch 346/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7660 - mae: 0.4466 - r2_keras: 0.6758\n",
      "Epoch 00346: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7660 - mae: 0.4466 - r2_keras: 0.6758\n",
      "Epoch 347/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7635 - mae: 0.4461 - r2_keras: 0.6755\n",
      "Epoch 00347: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7635 - mae: 0.4461 - r2_keras: 0.6756\n",
      "Epoch 348/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7622 - mae: 0.4457 - r2_keras: 0.6768\n",
      "Epoch 00348: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7623 - mae: 0.4457 - r2_keras: 0.6768\n",
      "Epoch 349/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.7655 - mae: 0.4465 - r2_keras: 0.6765\n",
      "Epoch 00349: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7655 - mae: 0.4466 - r2_keras: 0.6765\n",
      "Epoch 350/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7632 - mae: 0.4459 - r2_keras: 0.6764\n",
      "Epoch 00350: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7644 - mae: 0.4461 - r2_keras: 0.6761\n",
      "Epoch 351/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7628 - mae: 0.4458 - r2_keras: 0.6767\n",
      "Epoch 00351: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7625 - mae: 0.4456 - r2_keras: 0.6764\n",
      "Epoch 352/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.7611 - mae: 0.4456 - r2_keras: 0.6767\n",
      "Epoch 00352: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7622 - mae: 0.4460 - r2_keras: 0.6764\n",
      "Epoch 353/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7630 - mae: 0.4456 - r2_keras: 0.6764\n",
      "Epoch 00353: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7628 - mae: 0.4456 - r2_keras: 0.6762\n",
      "Epoch 354/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7657 - mae: 0.4464 - r2_keras: 0.6752\n",
      "Epoch 00354: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7655 - mae: 0.4464 - r2_keras: 0.6749\n",
      "Epoch 355/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7602 - mae: 0.4454 - r2_keras: 0.6780\n",
      "Epoch 00355: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7602 - mae: 0.4454 - r2_keras: 0.6781\n",
      "Epoch 356/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7656 - mae: 0.4464 - r2_keras: 0.6759\n",
      "Epoch 00356: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7656 - mae: 0.4464 - r2_keras: 0.6760\n",
      "Epoch 357/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7589 - mae: 0.4446 - r2_keras: 0.6773\n",
      "Epoch 00357: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7590 - mae: 0.4446 - r2_keras: 0.6773\n",
      "Epoch 358/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7641 - mae: 0.4459 - r2_keras: 0.6768\n",
      "Epoch 00358: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7650 - mae: 0.4461 - r2_keras: 0.6765\n",
      "Epoch 359/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7633 - mae: 0.4458 - r2_keras: 0.6762\n",
      "Epoch 00359: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7635 - mae: 0.4458 - r2_keras: 0.6763\n",
      "Epoch 360/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7597 - mae: 0.4450 - r2_keras: 0.6785\n",
      "Epoch 00360: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7596 - mae: 0.4450 - r2_keras: 0.6785\n",
      "Epoch 361/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7615 - mae: 0.4454 - r2_keras: 0.6778\n",
      "Epoch 00361: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7610 - mae: 0.4453 - r2_keras: 0.6779\n",
      "Epoch 362/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7591 - mae: 0.4453 - r2_keras: 0.6788\n",
      "Epoch 00362: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7599 - mae: 0.4455 - r2_keras: 0.6787\n",
      "Epoch 363/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7638 - mae: 0.4466 - r2_keras: 0.6748\n",
      "Epoch 00363: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7640 - mae: 0.4467 - r2_keras: 0.6748\n",
      "Epoch 364/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7603 - mae: 0.4450 - r2_keras: 0.6786 ETA: 0s - loss: 0.7615 - \n",
      "Epoch 00364: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7612 - mae: 0.4453 - r2_keras: 0.6781\n",
      "Epoch 365/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7615 - mae: 0.4449 - r2_keras: 0.6775\n",
      "Epoch 00365: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7618 - mae: 0.4450 - r2_keras: 0.6774\n",
      "Epoch 366/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7615 - mae: 0.4455 - r2_keras: 0.6780\n",
      "Epoch 00366: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7612 - mae: 0.4454 - r2_keras: 0.6780\n",
      "Epoch 367/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7612 - mae: 0.4455 - r2_keras: 0.6771\n",
      "Epoch 00367: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7610 - mae: 0.4454 - r2_keras: 0.6771\n",
      "Epoch 368/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7634 - mae: 0.4457 - r2_keras: 0.6770\n",
      "Epoch 00368: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7633 - mae: 0.4457 - r2_keras: 0.6768\n",
      "Epoch 369/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7626 - mae: 0.4457 - r2_keras: 0.6779\n",
      "Epoch 00369: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7621 - mae: 0.4457 - r2_keras: 0.6781\n",
      "Epoch 370/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7657 - mae: 0.4465 - r2_keras: 0.6756\n",
      "Epoch 00370: loss did not improve from 0.75891\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7657 - mae: 0.4465 - r2_keras: 0.6756\n",
      "Epoch 371/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7580 - mae: 0.4442 - r2_keras: 0.6784\n",
      "Epoch 00371: loss improved from 0.75891 to 0.75820, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.7582 - mae: 0.4442 - r2_keras: 0.6786\n",
      "Epoch 372/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7630 - mae: 0.4454 - r2_keras: 0.6768\n",
      "Epoch 00372: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7630 - mae: 0.4454 - r2_keras: 0.6768\n",
      "Epoch 373/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7616 - mae: 0.4452 - r2_keras: 0.6776\n",
      "Epoch 00373: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7610 - mae: 0.4451 - r2_keras: 0.6779\n",
      "Epoch 374/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7631 - mae: 0.4458 - r2_keras: 0.6764\n",
      "Epoch 00374: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7629 - mae: 0.4457 - r2_keras: 0.6765\n",
      "Epoch 375/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7673 - mae: 0.4467 - r2_keras: 0.6743\n",
      "Epoch 00375: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7681 - mae: 0.4469 - r2_keras: 0.6742\n",
      "Epoch 376/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7632 - mae: 0.4463 - r2_keras: 0.6769\n",
      "Epoch 00376: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7633 - mae: 0.4463 - r2_keras: 0.6762\n",
      "Epoch 377/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7668 - mae: 0.4467 - r2_keras: 0.6767\n",
      "Epoch 00377: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7657 - mae: 0.4464 - r2_keras: 0.6766\n",
      "Epoch 378/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7606 - mae: 0.4451 - r2_keras: 0.6782\n",
      "Epoch 00378: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7601 - mae: 0.4449 - r2_keras: 0.6781\n",
      "Epoch 379/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7624 - mae: 0.4458 - r2_keras: 0.6770 ETA: 1s - loss: 0\n",
      "Epoch 00379: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7625 - mae: 0.4458 - r2_keras: 0.6768\n",
      "Epoch 380/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7621 - mae: 0.4457 - r2_keras: 0.6765\n",
      "Epoch 00380: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7625 - mae: 0.4459 - r2_keras: 0.6766\n",
      "Epoch 381/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7612 - mae: 0.4453 - r2_keras: 0.6769\n",
      "Epoch 00381: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7604 - mae: 0.4451 - r2_keras: 0.6771\n",
      "Epoch 382/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7668 - mae: 0.4465 - r2_keras: 0.6754\n",
      "Epoch 00382: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7664 - mae: 0.4464 - r2_keras: 0.6754\n",
      "Epoch 383/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7589 - mae: 0.4449 - r2_keras: 0.6778\n",
      "Epoch 00383: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7587 - mae: 0.4449 - r2_keras: 0.6780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7632 - mae: 0.4465 - r2_keras: 0.6767\n",
      "Epoch 00384: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7630 - mae: 0.4464 - r2_keras: 0.6768\n",
      "Epoch 385/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7647 - mae: 0.4464 - r2_keras: 0.6756\n",
      "Epoch 00385: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7645 - mae: 0.4463 - r2_keras: 0.6759\n",
      "Epoch 386/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7619 - mae: 0.4450 - r2_keras: 0.6775\n",
      "Epoch 00386: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7614 - mae: 0.4448 - r2_keras: 0.6776\n",
      "Epoch 387/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7682 - mae: 0.4470 - r2_keras: 0.6743\n",
      "Epoch 00387: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7685 - mae: 0.4471 - r2_keras: 0.6743\n",
      "Epoch 388/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7658 - mae: 0.4469 - r2_keras: 0.6755\n",
      "Epoch 00388: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7662 - mae: 0.4470 - r2_keras: 0.6752\n",
      "Epoch 389/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7651 - mae: 0.4464 - r2_keras: 0.6760\n",
      "Epoch 00389: loss did not improve from 0.75820\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7650 - mae: 0.4464 - r2_keras: 0.6758\n",
      "Epoch 390/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7574 - mae: 0.4441 - r2_keras: 0.6794\n",
      "Epoch 00390: loss improved from 0.75820 to 0.75769, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7577 - mae: 0.4442 - r2_keras: 0.6794\n",
      "Epoch 391/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7613 - mae: 0.4450 - r2_keras: 0.6776\n",
      "Epoch 00391: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7613 - mae: 0.4450 - r2_keras: 0.6776\n",
      "Epoch 392/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7610 - mae: 0.4449 - r2_keras: 0.6786\n",
      "Epoch 00392: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7608 - mae: 0.4449 - r2_keras: 0.6786\n",
      "Epoch 393/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7622 - mae: 0.4450 - r2_keras: 0.6764\n",
      "Epoch 00393: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7628 - mae: 0.4452 - r2_keras: 0.6763\n",
      "Epoch 394/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7611 - mae: 0.4453 - r2_keras: 0.6783 ETA: 0s - loss: 0.762\n",
      "Epoch 00394: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7610 - mae: 0.4453 - r2_keras: 0.6783\n",
      "Epoch 395/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7603 - mae: 0.4447 - r2_keras: 0.6778\n",
      "Epoch 00395: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7600 - mae: 0.4446 - r2_keras: 0.6780\n",
      "Epoch 396/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7594 - mae: 0.4447 - r2_keras: 0.6788\n",
      "Epoch 00396: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7595 - mae: 0.4447 - r2_keras: 0.6786\n",
      "Epoch 397/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.7589 - mae: 0.4451 - r2_keras: 0.6788 ETA: 0s - loss: 0.7576 - mae: 0.4449 - r2_keras\n",
      "Epoch 00397: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7598 - mae: 0.4454 - r2_keras: 0.6783\n",
      "Epoch 398/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7624 - mae: 0.4456 - r2_keras: 0.6762\n",
      "Epoch 00398: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7624 - mae: 0.4456 - r2_keras: 0.6764\n",
      "Epoch 399/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7650 - mae: 0.4461 - r2_keras: 0.6753\n",
      "Epoch 00399: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7651 - mae: 0.4462 - r2_keras: 0.6753\n",
      "Epoch 400/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7651 - mae: 0.4464 - r2_keras: 0.6763\n",
      "Epoch 00400: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7651 - mae: 0.4464 - r2_keras: 0.6760\n",
      "Epoch 401/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7619 - mae: 0.4449 - r2_keras: 0.6765\n",
      "Epoch 00401: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7621 - mae: 0.4449 - r2_keras: 0.6764\n",
      "Epoch 402/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7612 - mae: 0.4452 - r2_keras: 0.6775\n",
      "Epoch 00402: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7611 - mae: 0.4452 - r2_keras: 0.6775\n",
      "Epoch 403/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7629 - mae: 0.4456 - r2_keras: 0.6774\n",
      "Epoch 00403: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7631 - mae: 0.4456 - r2_keras: 0.6772\n",
      "Epoch 404/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7640 - mae: 0.4460 - r2_keras: 0.6767\n",
      "Epoch 00404: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7639 - mae: 0.4459 - r2_keras: 0.6767\n",
      "Epoch 405/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7641 - mae: 0.4460 - r2_keras: 0.6762\n",
      "Epoch 00405: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7641 - mae: 0.4460 - r2_keras: 0.6762\n",
      "Epoch 406/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7609 - mae: 0.4455 - r2_keras: 0.6772\n",
      "Epoch 00406: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7610 - mae: 0.4454 - r2_keras: 0.6771\n",
      "Epoch 407/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7629 - mae: 0.4463 - r2_keras: 0.6768\n",
      "Epoch 00407: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7623 - mae: 0.4461 - r2_keras: 0.6767\n",
      "Epoch 408/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7624 - mae: 0.4452 - r2_keras: 0.6776\n",
      "Epoch 00408: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7624 - mae: 0.4452 - r2_keras: 0.6776\n",
      "Epoch 409/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7650 - mae: 0.4465 - r2_keras: 0.6755\n",
      "Epoch 00409: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7649 - mae: 0.4465 - r2_keras: 0.6756\n",
      "Epoch 410/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7600 - mae: 0.4456 - r2_keras: 0.6786\n",
      "Epoch 00410: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7600 - mae: 0.4455 - r2_keras: 0.6785\n",
      "Epoch 411/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7624 - mae: 0.4459 - r2_keras: 0.6767\n",
      "Epoch 00411: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7624 - mae: 0.4459 - r2_keras: 0.6767\n",
      "Epoch 412/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7647 - mae: 0.4467 - r2_keras: 0.6759\n",
      "Epoch 00412: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7637 - mae: 0.4465 - r2_keras: 0.6763\n",
      "Epoch 413/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7658 - mae: 0.4461 - r2_keras: 0.6764\n",
      "Epoch 00413: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7652 - mae: 0.4459 - r2_keras: 0.6765\n",
      "Epoch 414/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7617 - mae: 0.4457 - r2_keras: 0.6769\n",
      "Epoch 00414: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7615 - mae: 0.4457 - r2_keras: 0.6769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 415/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7645 - mae: 0.4460 - r2_keras: 0.6754\n",
      "Epoch 00415: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7640 - mae: 0.4459 - r2_keras: 0.6757\n",
      "Epoch 416/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.7626 - mae: 0.4458 - r2_keras: 0.6777\n",
      "Epoch 00416: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7634 - mae: 0.4460 - r2_keras: 0.6771\n",
      "Epoch 417/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7604 - mae: 0.4451 - r2_keras: 0.6771\n",
      "Epoch 00417: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7608 - mae: 0.4452 - r2_keras: 0.6770\n",
      "Epoch 418/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7691 - mae: 0.4473 - r2_keras: 0.6745\n",
      "Epoch 00418: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7691 - mae: 0.4473 - r2_keras: 0.6746\n",
      "Epoch 419/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7620 - mae: 0.4453 - r2_keras: 0.6770\n",
      "Epoch 00419: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7612 - mae: 0.4451 - r2_keras: 0.6774\n",
      "Epoch 420/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7620 - mae: 0.4458 - r2_keras: 0.6779\n",
      "Epoch 00420: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7617 - mae: 0.4457 - r2_keras: 0.6776\n",
      "Epoch 421/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7627 - mae: 0.4454 - r2_keras: 0.6767\n",
      "Epoch 00421: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7631 - mae: 0.4455 - r2_keras: 0.6766\n",
      "Epoch 422/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7666 - mae: 0.4467 - r2_keras: 0.6756\n",
      "Epoch 00422: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7669 - mae: 0.4467 - r2_keras: 0.6756\n",
      "Epoch 423/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7623 - mae: 0.4457 - r2_keras: 0.6774\n",
      "Epoch 00423: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7625 - mae: 0.4457 - r2_keras: 0.6771\n",
      "Epoch 424/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7586 - mae: 0.4444 - r2_keras: 0.6786\n",
      "Epoch 00424: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7586 - mae: 0.4444 - r2_keras: 0.6787\n",
      "Epoch 425/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7595 - mae: 0.4446 - r2_keras: 0.6786\n",
      "Epoch 00425: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7595 - mae: 0.4446 - r2_keras: 0.6786\n",
      "Epoch 426/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7611 - mae: 0.4453 - r2_keras: 0.6783\n",
      "Epoch 00426: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7610 - mae: 0.4453 - r2_keras: 0.6783\n",
      "Epoch 427/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7601 - mae: 0.4451 - r2_keras: 0.6795\n",
      "Epoch 00427: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7605 - mae: 0.4452 - r2_keras: 0.6792\n",
      "Epoch 428/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7611 - mae: 0.4456 - r2_keras: 0.6780\n",
      "Epoch 00428: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7615 - mae: 0.4457 - r2_keras: 0.6777\n",
      "Epoch 429/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7648 - mae: 0.4457 - r2_keras: 0.6767\n",
      "Epoch 00429: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7646 - mae: 0.4457 - r2_keras: 0.6765\n",
      "Epoch 430/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7612 - mae: 0.4456 - r2_keras: 0.6776\n",
      "Epoch 00430: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7616 - mae: 0.4458 - r2_keras: 0.6774\n",
      "Epoch 431/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7620 - mae: 0.4450 - r2_keras: 0.6781\n",
      "Epoch 00431: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7614 - mae: 0.4450 - r2_keras: 0.6783\n",
      "Epoch 432/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7635 - mae: 0.4460 - r2_keras: 0.6764\n",
      "Epoch 00432: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7640 - mae: 0.4462 - r2_keras: 0.6764\n",
      "Epoch 433/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7620 - mae: 0.4458 - r2_keras: 0.6777\n",
      "Epoch 00433: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7616 - mae: 0.4456 - r2_keras: 0.6776\n",
      "Epoch 434/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7608 - mae: 0.4450 - r2_keras: 0.6779\n",
      "Epoch 00434: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7608 - mae: 0.4450 - r2_keras: 0.6779\n",
      "Epoch 435/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7626 - mae: 0.4456 - r2_keras: 0.6771\n",
      "Epoch 00435: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7626 - mae: 0.4456 - r2_keras: 0.6770\n",
      "Epoch 436/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7664 - mae: 0.4465 - r2_keras: 0.6759\n",
      "Epoch 00436: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7665 - mae: 0.4465 - r2_keras: 0.6758\n",
      "Epoch 437/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7637 - mae: 0.4459 - r2_keras: 0.6758\n",
      "Epoch 00437: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7635 - mae: 0.4459 - r2_keras: 0.6761\n",
      "Epoch 438/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7649 - mae: 0.4466 - r2_keras: 0.6764\n",
      "Epoch 00438: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7642 - mae: 0.4465 - r2_keras: 0.6766\n",
      "Epoch 439/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7631 - mae: 0.4459 - r2_keras: 0.6773\n",
      "Epoch 00439: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7626 - mae: 0.4457 - r2_keras: 0.6773\n",
      "Epoch 440/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7646 - mae: 0.4460 - r2_keras: 0.6773\n",
      "Epoch 00440: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7643 - mae: 0.4460 - r2_keras: 0.6775\n",
      "Epoch 441/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7657 - mae: 0.4462 - r2_keras: 0.6758\n",
      "Epoch 00441: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7656 - mae: 0.4463 - r2_keras: 0.6759\n",
      "Epoch 442/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7628 - mae: 0.4453 - r2_keras: 0.6778\n",
      "Epoch 00442: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7626 - mae: 0.4453 - r2_keras: 0.6777\n",
      "Epoch 443/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7612 - mae: 0.4453 - r2_keras: 0.6769\n",
      "Epoch 00443: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7612 - mae: 0.4453 - r2_keras: 0.6769\n",
      "Epoch 444/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7639 - mae: 0.4459 - r2_keras: 0.6770\n",
      "Epoch 00444: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7639 - mae: 0.4459 - r2_keras: 0.6770\n",
      "Epoch 445/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7613 - mae: 0.4447 - r2_keras: 0.6773\n",
      "Epoch 00445: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7612 - mae: 0.4447 - r2_keras: 0.6771\n",
      "Epoch 446/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7601 - mae: 0.4447 - r2_keras: 0.6785\n",
      "Epoch 00446: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7601 - mae: 0.4447 - r2_keras: 0.6784\n",
      "Epoch 447/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7663 - mae: 0.4470 - r2_keras: 0.6755\n",
      "Epoch 00447: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7661 - mae: 0.4470 - r2_keras: 0.6755\n",
      "Epoch 448/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7648 - mae: 0.4459 - r2_keras: 0.6766\n",
      "Epoch 00448: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7648 - mae: 0.4459 - r2_keras: 0.6767\n",
      "Epoch 449/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7624 - mae: 0.4451 - r2_keras: 0.6775\n",
      "Epoch 00449: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7622 - mae: 0.4451 - r2_keras: 0.6775\n",
      "Epoch 450/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7623 - mae: 0.4459 - r2_keras: 0.6771\n",
      "Epoch 00450: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7626 - mae: 0.4460 - r2_keras: 0.6767\n",
      "Epoch 451/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7642 - mae: 0.4466 - r2_keras: 0.6755\n",
      "Epoch 00451: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7643 - mae: 0.4467 - r2_keras: 0.6755\n",
      "Epoch 452/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7665 - mae: 0.4460 - r2_keras: 0.6755\n",
      "Epoch 00452: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7662 - mae: 0.4459 - r2_keras: 0.6757\n",
      "Epoch 453/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7604 - mae: 0.4448 - r2_keras: 0.6779\n",
      "Epoch 00453: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7609 - mae: 0.4450 - r2_keras: 0.6777\n",
      "Epoch 454/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7614 - mae: 0.4446 - r2_keras: 0.6773\n",
      "Epoch 00454: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7615 - mae: 0.4446 - r2_keras: 0.6772\n",
      "Epoch 455/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7632 - mae: 0.4455 - r2_keras: 0.6770\n",
      "Epoch 00455: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7636 - mae: 0.4456 - r2_keras: 0.6765\n",
      "Epoch 456/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7646 - mae: 0.4463 - r2_keras: 0.6759\n",
      "Epoch 00456: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7648 - mae: 0.4463 - r2_keras: 0.6757\n",
      "Epoch 457/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7645 - mae: 0.4462 - r2_keras: 0.6761\n",
      "Epoch 00457: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7645 - mae: 0.4460 - r2_keras: 0.6761\n",
      "Epoch 458/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7656 - mae: 0.4468 - r2_keras: 0.6758\n",
      "Epoch 00458: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7659 - mae: 0.4469 - r2_keras: 0.6755\n",
      "Epoch 459/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7619 - mae: 0.4460 - r2_keras: 0.6774\n",
      "Epoch 00459: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7618 - mae: 0.4459 - r2_keras: 0.6773\n",
      "Epoch 460/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7616 - mae: 0.4455 - r2_keras: 0.6779\n",
      "Epoch 00460: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7616 - mae: 0.4455 - r2_keras: 0.6780\n",
      "Epoch 461/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7633 - mae: 0.4455 - r2_keras: 0.6759\n",
      "Epoch 00461: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7627 - mae: 0.4452 - r2_keras: 0.6763\n",
      "Epoch 462/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7607 - mae: 0.4455 - r2_keras: 0.6782 ETA: 1s\n",
      "Epoch 00462: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7606 - mae: 0.4455 - r2_keras: 0.6781\n",
      "Epoch 463/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7613 - mae: 0.4455 - r2_keras: 0.6782\n",
      "Epoch 00463: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7614 - mae: 0.4456 - r2_keras: 0.6782\n",
      "Epoch 464/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.7657 - mae: 0.4465 - r2_keras: 0.6759\n",
      "Epoch 00464: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7662 - mae: 0.4467 - r2_keras: 0.6758\n",
      "Epoch 465/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7634 - mae: 0.4455 - r2_keras: 0.6770\n",
      "Epoch 00465: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7649 - mae: 0.4460 - r2_keras: 0.6768\n",
      "Epoch 466/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7642 - mae: 0.4458 - r2_keras: 0.6775\n",
      "Epoch 00466: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7640 - mae: 0.4457 - r2_keras: 0.6774\n",
      "Epoch 467/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7631 - mae: 0.4460 - r2_keras: 0.6769\n",
      "Epoch 00467: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7627 - mae: 0.4459 - r2_keras: 0.6771\n",
      "Epoch 468/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7642 - mae: 0.4458 - r2_keras: 0.6762\n",
      "Epoch 00468: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7652 - mae: 0.4460 - r2_keras: 0.6758\n",
      "Epoch 469/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7636 - mae: 0.4460 - r2_keras: 0.6764\n",
      "Epoch 00469: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.7636 - mae: 0.4460 - r2_keras: 0.6764\n",
      "Epoch 470/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7631 - mae: 0.4455 - r2_keras: 0.6774\n",
      "Epoch 00470: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7624 - mae: 0.4453 - r2_keras: 0.6777\n",
      "Epoch 471/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7628 - mae: 0.4454 - r2_keras: 0.6759\n",
      "Epoch 00471: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7628 - mae: 0.4454 - r2_keras: 0.6759\n",
      "Epoch 472/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7667 - mae: 0.4467 - r2_keras: 0.6766\n",
      "Epoch 00472: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7663 - mae: 0.4466 - r2_keras: 0.6766\n",
      "Epoch 473/500\n",
      "808/813 [============================>.] - ETA: 0s - loss: 0.7638 - mae: 0.4460 - r2_keras: 0.6775\n",
      "Epoch 00473: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7630 - mae: 0.4459 - r2_keras: 0.6777\n",
      "Epoch 474/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7595 - mae: 0.4445 - r2_keras: 0.6776 ETA: 1s - loss: 0\n",
      "Epoch 00474: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7595 - mae: 0.4445 - r2_keras: 0.6776\n",
      "Epoch 475/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7645 - mae: 0.4458 - r2_keras: 0.6762\n",
      "Epoch 00475: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7645 - mae: 0.4458 - r2_keras: 0.6761\n",
      "Epoch 476/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7607 - mae: 0.4449 - r2_keras: 0.6789\n",
      "Epoch 00476: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7609 - mae: 0.4449 - r2_keras: 0.6791\n",
      "Epoch 477/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809/813 [============================>.] - ETA: 0s - loss: 0.7618 - mae: 0.4453 - r2_keras: 0.6779\n",
      "Epoch 00477: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7615 - mae: 0.4453 - r2_keras: 0.6782\n",
      "Epoch 478/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7601 - mae: 0.4454 - r2_keras: 0.6781\n",
      "Epoch 00478: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7604 - mae: 0.4455 - r2_keras: 0.6779\n",
      "Epoch 479/500\n",
      "813/813 [==============================] - ETA: 0s - loss: 0.7625 - mae: 0.4460 - r2_keras: 0.6777\n",
      "Epoch 00479: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7625 - mae: 0.4460 - r2_keras: 0.6777\n",
      "Epoch 480/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7617 - mae: 0.4460 - r2_keras: 0.6779\n",
      "Epoch 00480: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7617 - mae: 0.4459 - r2_keras: 0.6775\n",
      "Epoch 481/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7600 - mae: 0.4449 - r2_keras: 0.6779\n",
      "Epoch 00481: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7600 - mae: 0.4450 - r2_keras: 0.6778\n",
      "Epoch 482/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7623 - mae: 0.4454 - r2_keras: 0.6780\n",
      "Epoch 00482: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.7624 - mae: 0.4455 - r2_keras: 0.6776\n",
      "Epoch 483/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7596 - mae: 0.4445 - r2_keras: 0.6792\n",
      "Epoch 00483: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7597 - mae: 0.4445 - r2_keras: 0.6791\n",
      "Epoch 484/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7650 - mae: 0.4472 - r2_keras: 0.6757\n",
      "Epoch 00484: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7646 - mae: 0.4471 - r2_keras: 0.6759\n",
      "Epoch 485/500\n",
      "806/813 [============================>.] - ETA: 0s - loss: 0.7624 - mae: 0.4454 - r2_keras: 0.6778\n",
      "Epoch 00485: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7618 - mae: 0.4453 - r2_keras: 0.6776\n",
      "Epoch 486/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7620 - mae: 0.4458 - r2_keras: 0.6773\n",
      "Epoch 00486: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7623 - mae: 0.4459 - r2_keras: 0.6774\n",
      "Epoch 487/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7659 - mae: 0.4463 - r2_keras: 0.6756\n",
      "Epoch 00487: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7658 - mae: 0.4464 - r2_keras: 0.6755\n",
      "Epoch 488/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7649 - mae: 0.4461 - r2_keras: 0.6754\n",
      "Epoch 00488: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7646 - mae: 0.4461 - r2_keras: 0.6753\n",
      "Epoch 489/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7658 - mae: 0.4460 - r2_keras: 0.6757\n",
      "Epoch 00489: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7660 - mae: 0.4461 - r2_keras: 0.6754\n",
      "Epoch 490/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7612 - mae: 0.4454 - r2_keras: 0.6772\n",
      "Epoch 00490: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 7s 8ms/step - loss: 0.7612 - mae: 0.4454 - r2_keras: 0.6773\n",
      "Epoch 491/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7654 - mae: 0.4456 - r2_keras: 0.6758\n",
      "Epoch 00491: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7649 - mae: 0.4455 - r2_keras: 0.6760\n",
      "Epoch 492/500\n",
      "805/813 [============================>.] - ETA: 0s - loss: 0.7660 - mae: 0.4465 - r2_keras: 0.6757\n",
      "Epoch 00492: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7655 - mae: 0.4462 - r2_keras: 0.6756\n",
      "Epoch 493/500\n",
      "812/813 [============================>.] - ETA: 0s - loss: 0.7617 - mae: 0.4451 - r2_keras: 0.6777\n",
      "Epoch 00493: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7618 - mae: 0.4451 - r2_keras: 0.6774\n",
      "Epoch 494/500\n",
      "807/813 [============================>.] - ETA: 0s - loss: 0.7608 - mae: 0.4452 - r2_keras: 0.6780\n",
      "Epoch 00494: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7615 - mae: 0.4455 - r2_keras: 0.6778\n",
      "Epoch 495/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7579 - mae: 0.4443 - r2_keras: 0.6789\n",
      "Epoch 00495: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7581 - mae: 0.4444 - r2_keras: 0.6790\n",
      "Epoch 496/500\n",
      "809/813 [============================>.] - ETA: 0s - loss: 0.7590 - mae: 0.4445 - r2_keras: 0.6791\n",
      "Epoch 00496: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7592 - mae: 0.4447 - r2_keras: 0.6793\n",
      "Epoch 497/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7632 - mae: 0.4458 - r2_keras: 0.6766\n",
      "Epoch 00497: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7630 - mae: 0.4458 - r2_keras: 0.6768\n",
      "Epoch 498/500\n",
      "811/813 [============================>.] - ETA: 0s - loss: 0.7625 - mae: 0.4456 - r2_keras: 0.6760\n",
      "Epoch 00498: loss did not improve from 0.75769\n",
      "813/813 [==============================] - 5s 7ms/step - loss: 0.7627 - mae: 0.4457 - r2_keras: 0.6759\n",
      "Epoch 499/500\n",
      "804/813 [============================>.] - ETA: 0s - loss: 0.7568 - mae: 0.4444 - r2_keras: 0.6807\n",
      "Epoch 00499: loss improved from 0.75769 to 0.75757, saving model to best_atten2_model.h5\n",
      "813/813 [==============================] - 6s 8ms/step - loss: 0.7576 - mae: 0.4445 - r2_keras: 0.6805\n",
      "Epoch 500/500\n",
      "810/813 [============================>.] - ETA: 0s - loss: 0.7637 - mae: 0.4458 - r2_keras: 0.6762\n",
      "Epoch 00500: loss did not improve from 0.75757\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 0.7644 - mae: 0.4459 - r2_keras: 0.6758\n"
     ]
    }
   ],
   "source": [
    "history_atten2 = atten_2.fit(X_train[:,np.newaxis,:],y_train,batch_size= 128, epochs= 500,\n",
    "                  callbacks= atten_2_callbacks,\n",
    "#                   validation_data= (X_val[:, np.newaxis, :], y_val)\n",
    "         )\n",
    "\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 2s 2ms/step - loss: 0.7256 - mae: 0.4246 - r2_keras: 0.6575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7255892157554626, 0.42456498742103577, 0.6574675440788269]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_2.evaluate(X_test[:,np.newaxis,:], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24425848550>"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# atten2_predictions = atten_2.predict(X_test[:,np.newaxis,:], verbose= 1)\n",
    "plt.plot(atten2_predictions[:100,0], alpha = 0.7, label = 'preds')\n",
    "plt.plot(X_test[:100,0], alpha = 0.7, label = 'groud_truth')\n",
    "# plt.ylim(0,7)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
