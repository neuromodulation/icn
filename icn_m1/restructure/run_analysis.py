import numpy as np

import features
import preprocessing


def run(gen, settings, df_M1, fs, line_noise, filter_fun, methods='bandpass', use_mean=True, normalize=True,
        include_label=False):
    """Run "real-time" analysis of neurophysiological data generated by "gen".

    Parameters
    ----------
    gen : generator object
        generator that yields segments of data.
    settings : dict
        dictionary of settings such as "seglengths" or "frequencyranges"
    df_M1 : data frame
        data frame with the channel configurations and rereferencing settings.
    fs : int/float, optional
        length of the lower transition band. The default is 4.
    line_noise : int/float
        line frequency from given recording. Used to notch-filter data.
    filter_fun : array
        output of calc_band_filters.
    methods : str or iterable (list, tuple) of str
        features to be extracted. Includes "bandpass", "mobility", "complexity", "wavelet1"
    use_mean : boolean, optional
        if True, mean is used for normalization, else median is used. default is True
    normalize : boolean, optional
        if True, data is normalized over time (in sec) stored in settings["normalization_time"]
    include_label : boolean, optional
        set to True if label is included in data. default is False

    Returns
    -------
    filter_fun : array
        filter coefficients stored in array of shape (n_franges, filter_len (in samples))
    """

    if isinstance(methods, str):
        methods = [methods]
    num_fbands = len(settings['frequencyranges'])
    num_features = num_fbands * len(methods)  # later important to be distinguishable for different features
    num_channels = df_M1[(df_M1["used"] == 1) & (df_M1["target"] == 0)].index.shape[0]
    feature_arr = np.zeros([1, num_channels, num_features])
    fs_new = int(settings["resamplingrate"])
    normalize_time = int(settings["normalization_time"])

    cnt_samples = 0

    if include_label:
        label = np.zeros(1)

    if normalize:
        normalize_samples = int(normalize_time * fs_new)
        feature_arr_norm = np.zeros(feature_arr.shape)
    while True:
        ieeg_batch = next(gen, None)
        if ieeg_batch is None:
            print(f"{str(np.round(cnt_samples/fs_new, 2))} seconds of data processed.")
            if normalize:
                if include_label:
                    return feature_arr_norm, label
                else:
                    return feature_arr_norm
            else:
                if include_label:
                    return feature_arr, label
                else:
                    return feature_arr

        # separate label if included in data
        if include_label:
            current_label = ieeg_batch[-1, -1]
            ieeg_batch = ieeg_batch[0:-1, :]

        # call rereference
        ieeg_batch = preprocessing.rereference(ieeg_batch, df_M1)

        # notch filter
        ieeg_batch = preprocessing.notch_filt(ieeg_batch, fs, line_noise)

        # calculate features
        feature_sample = features.get_features(ieeg_batch, settings, methods, fs, filter_fun)

        if cnt_samples == 0:
            feature_arr = np.expand_dims(feature_sample, axis=0)
            if include_label:
                label = np.reshape(current_label, (1,))
        else:
            feature_arr = np.concatenate((feature_arr, np.expand_dims(feature_sample, axis=0)), axis=0)
            if include_label:
                label = np.concatenate((label, np.reshape(current_label, (1,))))

        if normalize is True:
            if cnt_samples < normalize_samples:
                if cnt_samples == 0:
                    n_idx = 0
                else:
                    n_idx = np.arange(0, cnt_samples, 1)
            else:
                n_idx = np.arange(cnt_samples - normalize_samples, cnt_samples+1, 1)

            if cnt_samples == 0:
                feature_arr_norm = np.zeros(feature_arr.shape)
                feature_arr_norm[n_idx, :, :] = np.clip(feature_arr[n_idx, :, :], settings["clip_low"],
                                                        settings["clip_high"])
            else:
                if use_mean is True:
                    norm_previous = np.mean(feature_arr[n_idx, :, :], axis=0)
                else:
                    norm_previous = np.median(feature_arr[n_idx, :, :], axis=0)

                feature_norm = (feature_arr[cnt_samples, :, :] - norm_previous) / norm_previous

                # Artifact rejection
                feature_norm = np.clip(feature_norm, settings["clip_low"], settings["clip_high"])
                feature_arr_norm = np.concatenate((feature_arr_norm, np.expand_dims(feature_norm, axis=0)), axis=0)
        cnt_samples += 1
